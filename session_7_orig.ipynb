{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 **AI Resilience Masterclass: Building Unbreakable LLM Systems**\n",
    "## **6-Hour Deep Dive into Error Handling, Rate Limiting & Self-Healing AI**\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Welcome to Your Journey Towards Bulletproof AI!**\n",
    "\n",
    "```ascii\n",
    "    🤖 Your AI System Journey\n",
    "    ┌─────────────────────┐\n",
    "    │   Fragile System    │ ──► Error Fundamentals\n",
    "    └─────────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "    ┌─────────────────────┐\n",
    "    │  Resilient System   │ ──► Graceful Degradation\n",
    "    └─────────────────────┘\n",
    "              │\n",
    "              ▼\n",
    "    ┌─────────────────────┐\n",
    "    │ Self-Healing System │ ──► Autonomous Recovery\n",
    "    └─────────────────────┘\n",
    "```\n",
    "\n",
    "### 🎓 **What You'll Master Today**\n",
    "\n",
    "| Module | Time | What You'll Build | Real-World Impact |\n",
    "|--------|------|-------------------|-------------------|\n",
    "| **1. Error Olympics** | 45min | Custom error handlers with retry logic | 99.9% uptime |\n",
    "| **2. Graceful Degradation** | 60min | Multi-tier fallback system | Zero downtime |\n",
    "| **3. Rate Limit Ninja** | 45min | Smart quota management | 50% cost reduction |\n",
    "| **4. Circuit Breakers** | 60min | Self-protecting systems | Auto-recovery |\n",
    "| **5. Observability Hub** | 45min | Real-time monitoring | Predictive maintenance |\n",
    "| **6. Self-Healing AI** | 75min | Autonomous recovery system | 24/7 reliability |\n",
    "\n",
    "### 🛠️ **Your Toolkit for Today**\n",
    "- 🔑 OpenAI API (the only external service we need!)\n",
    "- 📊 Real-time dashboards\n",
    "- 🎮 Interactive experiments\n",
    "- 💾 Production-ready patterns\n",
    "- 🚨 Live monitoring & alerts\n",
    "\n",
    "### ⚡ **Why This Matters**\n",
    "- **Netflix**: 99.99% availability = resilient systems\n",
    "- **OpenAI**: Handles millions of requests = smart rate limiting\n",
    "- **Google**: Self-healing infrastructure = autonomous recovery\n",
    "\n",
    "**Today, you'll build systems that match these standards!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 **Pre-Flight Checklist**\n",
    "### Let's ensure your environment is ready for takeoff! 🚁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 AI Resilience Workshop - System Check\n",
      "==================================================\n",
      "⏰ Workshop Start Time: 2025-09-19 19:43:32\n",
      "==================================================\n",
      "\n",
      "📦 Installing required packages...\n",
      "  ✅ openai installed\n",
      "  ✅ tenacity installed\n",
      "  ✅ ratelimit installed\n",
      "  ✅ prometheus-client installed\n",
      "  ✅ plotly installed\n",
      "  ✅ pandas installed\n",
      "  ✅ numpy installed\n",
      "  ✅ ipywidgets installed\n",
      "  ✅ rich installed\n",
      "  ✅ httpx installed\n",
      "  ✅ asyncio installed\n",
      "  ✅ aiohttp installed\n",
      "  ✅ python-dotenv installed\n",
      "\n",
      "🔑 OpenAI API Key Setup\n",
      "----------------------------------------\n",
      "✅ API key loaded from environment\n",
      "\n",
      "🔍 Testing OpenAI API connection...\n",
      "✅ Successfully connected to OpenAI API!\n",
      "   Available models: 88 models found\n",
      "\n",
      "✨ Environment ready! Let's build resilient AI systems!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 1: Environment Setup & Validation\n",
    "# This cell sets up EVERYTHING you need for the workshop!\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "print(\"🚀 AI Resilience Workshop - System Check\")\n",
    "print(\"=\"*50)\n",
    "print(f\"⏰ Workshop Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Install required packages\n",
    "packages = [\n",
    "    'openai>=1.0.0',\n",
    "    'tenacity',  # For retry logic\n",
    "    'ratelimit',  # For rate limiting\n",
    "    'prometheus-client',  # For metrics\n",
    "    'plotly',  # For interactive visualizations\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'ipywidgets',\n",
    "    'rich',  # For beautiful terminal output\n",
    "    'httpx',  # For advanced HTTP handling\n",
    "    'asyncio',\n",
    "    'aiohttp',\n",
    "    'python-dotenv'\n",
    "]\n",
    "\n",
    "print(\"\\n📦 Installing required packages...\")\n",
    "for package in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "    print(f\"  ✅ {package.split('>')[0]} installed\")\n",
    "\n",
    "# Import everything we need\n",
    "import openai\n",
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "from typing import Optional, Dict, Any, List, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from collections import deque, defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import traceback\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data science imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Resilience imports - FIXED VERSION\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_exponential,\n",
    "    retry_if_exception_type,\n",
    "    before,  # Changed from before_retry\n",
    "    after    # Changed from after_retry\n",
    ")\n",
    "\n",
    "print(\"\\n🔑 OpenAI API Key Setup\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Interactive API key setup\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    print(\"⚠️  No API key found in environment\")\n",
    "    print(\"Please enter your OpenAI API key:\")\n",
    "    api_key = input(\"API Key: \").strip()\n",
    "    os.environ['OPENAI_API_KEY'] = api_key\n",
    "    openai.api_key = api_key\n",
    "else:\n",
    "    openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "    print(\"✅ API key loaded from environment\")\n",
    "\n",
    "# Test API connection\n",
    "print(\"\\n🔍 Testing OpenAI API connection...\")\n",
    "try:\n",
    "    client = openai.OpenAI()\n",
    "    models = client.models.list()\n",
    "    print(\"✅ Successfully connected to OpenAI API!\")\n",
    "    print(f\"   Available models: {len(list(models))} models found\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Connection failed: {e}\")\n",
    "    print(\"   Please check your API key and try again\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger('AIResilience')\n",
    "\n",
    "print(\"\\n✨ Environment ready! Let's build resilient AI systems!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🏋️ **Module 1: The Error Olympics**\n",
    "## Master Every Type of Failure Your AI Will Face!\n",
    "\n",
    "---\n",
    "\n",
    "## 🎪 **Welcome to the Failure Zoo!**\n",
    "### Where Every Error Has a Story (and a Solution)\n",
    "\n",
    "In the wild world of AI systems, errors aren't just bugs – they're opportunities to build resilience! Let's meet the entire cast of characters that will test your system's mettle.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎭 **Act 1: The Main Cast of Error Characters**\n",
    "\n",
    "### 🐢 **The Timeout Turtle** - *Chronicus Slowpokeus*\n",
    "**Natural Habitat:** Network edges and overloaded servers  \n",
    "**Favorite Food:** Your patience  \n",
    "**Warning Signs:** \n",
    "- Requests taking >30 seconds\n",
    "- Users refreshing repeatedly\n",
    "- That spinning wheel of doom\n",
    "\n",
    "**Theory Behind the Beast:**\n",
    "Timeouts occur when network latency exceeds acceptable thresholds. They follow the **Long Tail Distribution** – most requests are fast, but a few are devastatingly slow.\n",
    "\n",
    "```python\n",
    "# The Timeout Lifecycle\n",
    "Stage 1: Request sent         (0s)    😊\n",
    "Stage 2: Waiting...           (10s)   😐\n",
    "Stage 3: Still waiting...     (30s)   😰\n",
    "Stage 4: Timeout!            (60s)   💀\n",
    "```\n",
    "\n",
    "**Survival Strategy:**\n",
    "- Set aggressive timeouts (fail fast!)\n",
    "- Implement exponential backoff\n",
    "- Use circuit breakers to detect chronic slowness\n",
    "\n",
    "---\n",
    "\n",
    "### 🚦 **The Rate Limit Riot** - *Velocitus Restrictus*\n",
    "**Natural Habitat:** API gateways and billing departments  \n",
    "**Favorite Saying:** \"Whoa there, speedster!\"  \n",
    "**Damage Type:** 429 Too Many Requests\n",
    "\n",
    "**The Mathematics of Rate Limiting:**\n",
    "```\n",
    "Tokens Available = Bucket Capacity - Tokens Used\n",
    "Refill Rate = Tokens per Second\n",
    "Wait Time = (Tokens Needed - Tokens Available) / Refill Rate\n",
    "```\n",
    "\n",
    "**Visual Representation:**\n",
    "```\n",
    "Bucket at start:    [🪙🪙🪙🪙🪙] Full (5/5 tokens)\n",
    "After 3 requests:   [🪙🪙⚫⚫⚫] Partial (2/5 tokens)\n",
    "After rate limit:   [⚫⚫⚫⚫⚫] Empty (0/5 tokens)\n",
    "                    ↓ Time passes...\n",
    "After refill:       [🪙🪙🪙⚫⚫] Refilling (3/5 tokens)\n",
    "```\n",
    "\n",
    "**Pro Tips:**\n",
    "- Implement request queuing\n",
    "- Use multiple API keys for load distribution\n",
    "- Cache aggressively to reduce API calls\n",
    "\n",
    "---\n",
    "\n",
    "### 🌊 **The Token Tsunami** - *Contextus Overloadicus*\n",
    "**Natural Habitat:** Long conversation threads  \n",
    "**Special Attack:** Context window overflow  \n",
    "**Weakness:** Summarization magic\n",
    "\n",
    "**Token Economics 101:**\n",
    "| Model | Max Tokens | Cost Impact | Use Case |\n",
    "|-------|------------|-------------|----------|\n",
    "| GPT-4 | 8,192 | 💰💰💰 | Complex reasoning |\n",
    "| GPT-4-32k | 32,768 | 💰💰💰💰💰 | Document analysis |\n",
    "| GPT-3.5 | 4,096 | 💰 | Quick responses |\n",
    "\n",
    "**The Token Overflow Equation:**\n",
    "```\n",
    "Total Tokens = Prompt Tokens + Completion Tokens + System Tokens\n",
    "If Total Tokens > Max Context Window → 💥 ERROR\n",
    "```\n",
    "\n",
    "**Defensive Strategies:**\n",
    "1. **The Sliding Window:** Keep only recent context\n",
    "2. **The Summarizer:** Compress older messages\n",
    "3. **The Chunker:** Split into smaller requests\n",
    "\n",
    "---\n",
    "\n",
    "### 😴 **The Service Slumber** - *API Hibernatus*\n",
    "**Natural Habitat:** Cloud regions during maintenance  \n",
    "**Symptoms:** 503 Service Unavailable  \n",
    "**Duration:** 30 seconds to ∞\n",
    "\n",
    "**The Anatomy of Downtime:**\n",
    "```\n",
    "99.9% Uptime = 8.76 hours downtime/year\n",
    "99.99% Uptime = 52.56 minutes downtime/year\n",
    "99.999% Uptime = 5.26 minutes downtime/year\n",
    "\n",
    "Your Reality: Murphy's Law applies 100% of the time\n",
    "```\n",
    "\n",
    "**Recovery Patterns:**\n",
    "```python\n",
    "# The Resilience Ladder\n",
    "Level 1: Retry ────────→ \"Try, try again\"\n",
    "Level 2: Fallback ─────→ \"Use backup service\"\n",
    "Level 3: Degrade ──────→ \"Reduce functionality\"\n",
    "Level 4: Cache ────────→ \"Use stale data\"\n",
    "Level 5: Queue ────────→ \"Process later\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🎭 **The Format Fiasco** - *Parseus Impossibilus*\n",
    "**Natural Habitat:** JSON responses and XML nightmares  \n",
    "**Favorite Trick:** Returning HTML when you expect JSON  \n",
    "**Battle Cry:** \"undefined is not a function!\"\n",
    "\n",
    "**Common Format Failures:**\n",
    "```javascript\n",
    "Expected: {\"result\": \"success\", \"data\": {...}}\n",
    "Reality:  \"<html><body>503 Service Unavailable</body></html>\"\n",
    "Result:   JSON.parse() → 💥\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 **Act 2: The Science of Failure**\n",
    "\n",
    "### **The Error Probability Matrix**\n",
    "```\n",
    "                High Frequency │ Low Frequency\n",
    "    High Impact ┌──────────────┼──────────────┐\n",
    "                │   CRITICAL   │   PREPARE    │\n",
    "                │ Rate Limits  │ Outages      │\n",
    "                ├──────────────┼──────────────┤\n",
    "    Low Impact  │   OPTIMIZE   │   ACCEPT     │\n",
    "                │ Timeouts     │ Format Errors│\n",
    "                └──────────────┴──────────────┘\n",
    "```\n",
    "\n",
    "### **The Resilience Pyramid**\n",
    "```\n",
    "         ╱╲          Prevent\n",
    "        ╱  ╲         (Input validation, rate limiting)\n",
    "       ╱────╲        \n",
    "      ╱      ╲       Detect\n",
    "     ╱ DETECT ╲      (Health checks, monitoring)\n",
    "    ╱──────────╲     \n",
    "   ╱            ╲    Respond\n",
    "  ╱   RESPOND    ╲   (Retry, fallback, circuit break)\n",
    " ╱────────────────╲  \n",
    "╱     RECOVER      ╲ Recover\n",
    "└──────────────────┘ (Self-heal, scale, alert)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎮 **Act 3: The Training Ground**\n",
    "\n",
    "### **Error Handling Kata** 🥋\n",
    "Practice these patterns until they become muscle memory:\n",
    "\n",
    "**Pattern 1: The Graceful Retry**\n",
    "```python\n",
    "for attempt in range(3):\n",
    "    try:\n",
    "        result = risky_operation()\n",
    "        break\n",
    "    except TemporaryError:\n",
    "        if attempt == 2:\n",
    "            use_fallback()\n",
    "        time.sleep(2 ** attempt)  # Exponential backoff\n",
    "```\n",
    "\n",
    "**Pattern 2: The Circuit Breaker**\n",
    "```python\n",
    "if circuit.is_open():\n",
    "    return cached_response\n",
    "try:\n",
    "    response = make_request()\n",
    "    circuit.record_success()\n",
    "except:\n",
    "    circuit.record_failure()\n",
    "    if circuit.failure_count > threshold:\n",
    "        circuit.open()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 **Act 4: The Championship Round**\n",
    "\n",
    "### **Error Olympics Events:**\n",
    "\n",
    "**🏃 Sprint Recovery** (Fastest error recovery)\n",
    "- Gold: <100ms fallback\n",
    "- Silver: <500ms retry\n",
    "- Bronze: <2s degradation\n",
    "\n",
    "**🏋️ Load Bearing** (Most concurrent errors handled)\n",
    "- Gold: 1000+ errors/second\n",
    "- Silver: 100+ errors/second\n",
    "- Bronze: 10+ errors/second\n",
    "\n",
    "**🤸 Gymnastics** (Most graceful degradation)\n",
    "- Gold: Seamless user experience\n",
    "- Silver: Minor feature reduction\n",
    "- Bronze: Maintenance mode\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Act 5: The Wisdom Wall**\n",
    "\n",
    "### **Ancient Proverbs of Error Handling:**\n",
    "\n",
    "> \"A timeout in time saves nine retries\" - *Ancient DevOps Wisdom*\n",
    "\n",
    "> \"He who fails fast, recovers faster\" - *The Circuit Breaker's Creed*\n",
    "\n",
    "> \"Cache is king when the API is down\" - *The Fallback Manifesto*\n",
    "\n",
    "### **The Error Handler's Oath:**\n",
    "```\n",
    "I solemnly swear to:\n",
    "✓ Never ignore an error silently\n",
    "✓ Always provide meaningful error messages\n",
    "✓ Log everything, panic about nothing\n",
    "✓ Test my error paths as much as happy paths\n",
    "✓ Remember that users don't care about my stack trace\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 **Act 6: The Visual Guide**\n",
    "\n",
    "### **Error Flow Visualization:**\n",
    "```\n",
    "User Request\n",
    "    ↓\n",
    "[Primary Service]\n",
    "    ↓\n",
    "  Error? ──No──→ Success! 🎉\n",
    "    ↓Yes\n",
    "[Retry Logic]\n",
    "    ↓\n",
    "  Works? ──Yes─→ Success! 🎉\n",
    "    ↓No\n",
    "[Fallback Service]\n",
    "    ↓\n",
    "  Works? ──Yes─→ Degraded Success 😊\n",
    "    ↓No\n",
    "[Cache Check]\n",
    "    ↓\n",
    "  Found? ──Yes─→ Stale Success 😐\n",
    "    ↓No\n",
    "[Error Response]\n",
    "    ↓\n",
    "Graceful Failure 😢\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Act 7: The Metrics That Matter**\n",
    "\n",
    "### **Your Error Dashboard:**\n",
    "```\n",
    "┌─────────────────────────────────────┐\n",
    "│ 🚨 ERROR RATE: 0.1%                │\n",
    "│ ⏱️  P99 LATENCY: 2.3s               │\n",
    "│ 🔄 RETRY SUCCESS: 87%               │\n",
    "│ 💾 CACHE HIT: 45%                  │\n",
    "│ 🔌 CIRCUIT STATE: [CLOSED]          │\n",
    "└─────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### **SLA Reality Check:**\n",
    "- **99% uptime** = Your customers notice\n",
    "- **99.9% uptime** = Your boss notices\n",
    "- **99.99% uptime** = Nobody notices (until it breaks)\n",
    "- **99.999% uptime** = You're probably lying\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 **Graduation Ceremony**\n",
    "\n",
    "### **You've Mastered Error Handling When:**\n",
    "- [ ] Errors make you curious, not panicked\n",
    "- [ ] Your logs tell a story, not a mystery\n",
    "- [ ] Users see helpful messages, not stack traces\n",
    "- [ ] Your system degrades gracefully, not catastrophically\n",
    "- [ ] You measure everything and assume nothing\n",
    "\n",
    "\n",
    "\n",
    "## 🚀 **Next Steps: From Theory to Practice**\n",
    "\n",
    "Ready to implement these patterns? Head to the code cells below where we'll build:\n",
    "1. A retry mechanism that actually works\n",
    "2. Circuit breakers that save your bacon\n",
    "3. Fallback chains that never let you down\n",
    "4. Monitoring that tells you what's really happening\n",
    "\n",
    "Remember: **Every error handled well is a future outage prevented!**\n",
    "\n",
    "---\n",
    "\n",
    "*\"In the face of ambiguity, refuse the temptation to guess. In the face of errors, refuse the temptation to ignore.\"* - The Zen of Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resilient AI Client initialized!\n",
      "📊 Error types tracked: 13\n",
      "🔄 Retry logic: Exponential backoff with jitter\n",
      "📈 Metrics collection: Enabled\n",
      "🤖 Default model: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 2: The Complete Error Taxonomy\n",
    "# Let's build a comprehensive error handling system!\n",
    "\n",
    "from enum import Enum, auto\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Any, Callable, List, Dict\n",
    "import time\n",
    "import random\n",
    "import openai\n",
    "import traceback\n",
    "from collections import deque, defaultdict\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class ErrorType(Enum):\n",
    "    \"\"\"Complete taxonomy of AI system errors\"\"\"\n",
    "    # Network errors\n",
    "    TIMEOUT = auto()\n",
    "    CONNECTION_ERROR = auto()\n",
    "    DNS_FAILURE = auto()\n",
    "    \n",
    "    # API errors\n",
    "    RATE_LIMIT = auto()\n",
    "    QUOTA_EXCEEDED = auto()\n",
    "    AUTHENTICATION = auto()\n",
    "    \n",
    "    # Model errors\n",
    "    CONTEXT_LENGTH = auto()\n",
    "    INVALID_INPUT = auto()\n",
    "    CONTENT_FILTER = auto()\n",
    "    \n",
    "    # Service errors\n",
    "    SERVICE_UNAVAILABLE = auto()\n",
    "    INTERNAL_ERROR = auto()\n",
    "    \n",
    "    # Data errors\n",
    "    PARSING_ERROR = auto()\n",
    "    VALIDATION_ERROR = auto()\n",
    "\n",
    "@dataclass\n",
    "class ErrorContext:\n",
    "    \"\"\"Rich context for error handling decisions\"\"\"\n",
    "    error_type: ErrorType\n",
    "    timestamp: float\n",
    "    attempt_number: int\n",
    "    error_message: str\n",
    "    request_data: Optional[Dict[str, Any]] = None\n",
    "    response_data: Optional[Dict[str, Any]] = None\n",
    "    traceback: Optional[str] = None\n",
    "    \n",
    "    @property\n",
    "    def is_retryable(self) -> bool:\n",
    "        \"\"\"Determine if error is worth retrying\"\"\"\n",
    "        non_retryable = {\n",
    "            ErrorType.AUTHENTICATION,\n",
    "            ErrorType.INVALID_INPUT,\n",
    "            ErrorType.CONTENT_FILTER,\n",
    "            ErrorType.CONTEXT_LENGTH\n",
    "        }\n",
    "        return self.error_type not in non_retryable\n",
    "\n",
    "class ResilientAIClient:\n",
    "    \"\"\"Production-grade resilient OpenAI client\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = openai.OpenAI()\n",
    "        self.error_history = deque(maxlen=100)\n",
    "        self.metrics = defaultdict(int)\n",
    "        self.callbacks = {}\n",
    "        \n",
    "    def register_error_callback(self, error_type: ErrorType, callback: Callable):\n",
    "        \"\"\"Register custom error handlers\"\"\"\n",
    "        self.callbacks[error_type] = callback\n",
    "        \n",
    "    def classify_error(self, exception: Exception) -> ErrorType:\n",
    "        \"\"\"Smart error classification\"\"\"\n",
    "        error_str = str(exception).lower()\n",
    "        \n",
    "        # Classification logic\n",
    "        if \"rate limit\" in error_str or \"429\" in error_str:\n",
    "            return ErrorType.RATE_LIMIT\n",
    "        elif \"timeout\" in error_str:\n",
    "            return ErrorType.TIMEOUT\n",
    "        elif \"connection\" in error_str:\n",
    "            return ErrorType.CONNECTION_ERROR\n",
    "        elif \"context length\" in error_str or \"token\" in error_str:\n",
    "            return ErrorType.CONTEXT_LENGTH\n",
    "        elif \"503\" in error_str or \"unavailable\" in error_str:\n",
    "            return ErrorType.SERVICE_UNAVAILABLE\n",
    "        elif \"authentication\" in error_str or \"401\" in error_str:\n",
    "            return ErrorType.AUTHENTICATION\n",
    "        elif \"500\" in error_str:\n",
    "            return ErrorType.INTERNAL_ERROR\n",
    "        else:\n",
    "            return ErrorType.INTERNAL_ERROR\n",
    "    \n",
    "    async def call_with_resilience(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: str = \"gpt-4o\",  # Updated to gpt-4o\n",
    "        max_retries: int = 3,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Make API call with comprehensive error handling\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Record attempt\n",
    "                self.metrics['total_attempts'] += 1\n",
    "                \n",
    "                # Make the actual API call\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    **kwargs\n",
    "                )\n",
    "                \n",
    "                # Success!\n",
    "                self.metrics['successful_calls'] += 1\n",
    "                return response\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Classify the error\n",
    "                error_type = self.classify_error(e)\n",
    "                \n",
    "                # Create error context\n",
    "                context = ErrorContext(\n",
    "                    error_type=error_type,\n",
    "                    timestamp=time.time(),\n",
    "                    attempt_number=attempt + 1,\n",
    "                    error_message=str(e),\n",
    "                    request_data={'messages': messages, 'model': model},\n",
    "                    traceback=traceback.format_exc()\n",
    "                )\n",
    "                \n",
    "                # Record error\n",
    "                self.error_history.append(context)\n",
    "                self.metrics[f'error_{error_type.name}'] += 1\n",
    "                \n",
    "                # Execute callback if registered\n",
    "                if error_type in self.callbacks:\n",
    "                    self.callbacks[error_type](context)\n",
    "                \n",
    "                # Decide on retry\n",
    "                if not context.is_retryable or attempt == max_retries - 1:\n",
    "                    raise\n",
    "                \n",
    "                # Calculate backoff\n",
    "                wait_time = self._calculate_backoff(attempt, error_type)\n",
    "                print(f\"⏳ Retry {attempt + 1}/{max_retries} after {wait_time:.1f}s\")\n",
    "                time.sleep(wait_time)\n",
    "    \n",
    "    def _calculate_backoff(self, attempt: int, error_type: ErrorType) -> float:\n",
    "        \"\"\"Intelligent backoff calculation\"\"\"\n",
    "        base_wait = 1.0\n",
    "        \n",
    "        # Different strategies for different errors\n",
    "        if error_type == ErrorType.RATE_LIMIT:\n",
    "            # Longer wait for rate limits\n",
    "            return base_wait * (2 ** attempt) + random.uniform(0, 1)\n",
    "        elif error_type == ErrorType.SERVICE_UNAVAILABLE:\n",
    "            # Even longer for service issues\n",
    "            return base_wait * (3 ** attempt)\n",
    "        else:\n",
    "            # Standard exponential backoff\n",
    "            return base_wait * (1.5 ** attempt)\n",
    "    \n",
    "    def get_error_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate error analytics report\"\"\"\n",
    "        if not self.error_history:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        data = []\n",
    "        for error in self.error_history:\n",
    "            data.append({\n",
    "                'timestamp': datetime.fromtimestamp(error.timestamp),\n",
    "                'type': error.error_type.name,\n",
    "                'attempt': error.attempt_number,\n",
    "                'retryable': error.is_retryable,\n",
    "                'message': error.error_message[:50]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "# Create our resilient client\n",
    "resilient_client = ResilientAIClient()\n",
    "\n",
    "# Register some custom error handlers\n",
    "def handle_rate_limit(context: ErrorContext):\n",
    "    print(f\"🚦 Rate limit hit! Waiting longer...\")\n",
    "    print(f\"   Error: {context.error_message}\")\n",
    "\n",
    "def handle_timeout(context: ErrorContext):\n",
    "    print(f\"⏱️ Timeout occurred on attempt {context.attempt_number}\")\n",
    "\n",
    "resilient_client.register_error_callback(ErrorType.RATE_LIMIT, handle_rate_limit)\n",
    "resilient_client.register_error_callback(ErrorType.TIMEOUT, handle_timeout)\n",
    "\n",
    "print(\"✅ Resilient AI Client initialized!\")\n",
    "print(f\"📊 Error types tracked: {len(ErrorType)}\")\n",
    "print(f\"🔄 Retry logic: Exponential backoff with jitter\")\n",
    "print(f\"📈 Metrics collection: Enabled\")\n",
    "print(f\"🤖 Default model: gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 **Live Error Simulation Lab**\n",
    "#### Let's trigger real errors and see our handlers in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Interactive Error Testing Console\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21999a7507c146ba8031a30b8c3ed53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🎯 Select an error type and click to simulate:</h3>'), HTML(value=\"<p style='col…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Tips:\n",
      "• Select 'none' to test a real API call to gpt-4o\n",
      "• Try different error types to see recovery strategies\n",
      "• Notice how different errors require different handling approaches\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 3: Interactive Error Simulator\n",
    "# Experience different failure modes in a controlled environment!\n",
    "\n",
    "import openai\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ErrorSimulator:\n",
    "    \"\"\"Simulate various API errors for testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.simulation_mode = True\n",
    "        self.error_probability = 0.5\n",
    "        self.client = openai.OpenAI()\n",
    "        \n",
    "    def simulate_error(self, error_type: str):\n",
    "        \"\"\"Simulate specific error types\"\"\"\n",
    "        \n",
    "        # Create simple exceptions that mimic OpenAI errors\n",
    "        errors = {\n",
    "            'rate_limit': Exception(\"Error code: 429 - Rate limit exceeded. Please retry after 1 second.\"),\n",
    "            'timeout': TimeoutError(\"Request timeout after 30s\"),\n",
    "            'context_length': Exception(\"Error: Maximum context length (8192 tokens) exceeded\"),\n",
    "            'service_unavailable': Exception(\"Error code: 503 - The server is temporarily unavailable\"),\n",
    "            'auth': Exception(\"Error code: 401 - Incorrect API key provided\")\n",
    "        }\n",
    "        \n",
    "        if error_type in errors:\n",
    "            raise errors[error_type]\n",
    "    \n",
    "    def make_call_with_chaos(self, messages, error_type=None):\n",
    "        \"\"\"Make API call with optional chaos engineering\"\"\"\n",
    "        \n",
    "        # Randomly inject errors if in simulation mode\n",
    "        if self.simulation_mode and error_type:\n",
    "            self.simulate_error(error_type)\n",
    "        \n",
    "        # Otherwise make real call\n",
    "        return self.client.chat.completions.create(\n",
    "            model=\"gpt-4o\",  # Updated to gpt-4o\n",
    "            messages=messages,\n",
    "            max_tokens=50\n",
    "        )\n",
    "\n",
    "# Create simulator\n",
    "simulator = ErrorSimulator()\n",
    "\n",
    "# Interactive error testing widget\n",
    "print(\"🎮 Interactive Error Testing Console\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "error_dropdown = widgets.Dropdown(\n",
    "    options=['none', 'rate_limit', 'timeout', 'context_length', 'service_unavailable', 'auth'],\n",
    "    value='none',\n",
    "    description='Error Type:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='🧪 Trigger Error',\n",
    "    button_style='danger',\n",
    "    tooltip='Simulate the selected error',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def test_error_handling(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        error_type = error_dropdown.value\n",
    "        \n",
    "        print(f\"\\n🔬 Testing: {error_type}\")\n",
    "        print(\"-\"*40)\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": \"Hello! Please respond with a brief greeting.\"}]\n",
    "        \n",
    "        try:\n",
    "            if error_type == 'none':\n",
    "                # Make real API call\n",
    "                print(\"📡 Making real API call to gpt-4o...\")\n",
    "                response = simulator.client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",  # Updated to gpt-4o\n",
    "                    messages=messages,\n",
    "                    max_tokens=50\n",
    "                )\n",
    "                print(\"✅ Success! Response received:\")\n",
    "                print(f\"   {response.choices[0].message.content}\")\n",
    "            else:\n",
    "                # Simulate error\n",
    "                print(f\"💥 Simulating {error_type} error...\")\n",
    "                simulator.make_call_with_chaos(messages, error_type)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error caught: {type(e).__name__}\")\n",
    "            print(f\"   Message: {str(e)}\")\n",
    "            \n",
    "            # Now show recovery\n",
    "            print(\"\\n🔄 Attempting recovery...\")\n",
    "            \n",
    "            # Classify and handle\n",
    "            error_class = resilient_client.classify_error(e)\n",
    "            print(f\"   Error classified as: {error_class.name}\")\n",
    "            \n",
    "            if error_class == ErrorType.RATE_LIMIT:\n",
    "                print(\"   Strategy: Exponential backoff with jitter\")\n",
    "                print(\"   Waiting 2 seconds...\")\n",
    "                time.sleep(2)\n",
    "                print(\"   ✅ Ready to retry!\")\n",
    "                \n",
    "            elif error_class == ErrorType.TIMEOUT:\n",
    "                print(\"   Strategy: Reduce payload size and retry\")\n",
    "                print(\"   ✅ Optimized request prepared\")\n",
    "                \n",
    "            elif error_class == ErrorType.CONTEXT_LENGTH:\n",
    "                print(\"   Strategy: Truncate context and retry\")\n",
    "                print(\"   ✅ Context reduced by 50%\")\n",
    "                \n",
    "            elif error_class == ErrorType.AUTHENTICATION:\n",
    "                print(\"   Strategy: Check API key and credentials\")\n",
    "                print(\"   ❌ Cannot retry - authentication required\")\n",
    "                \n",
    "            elif error_class == ErrorType.SERVICE_UNAVAILABLE:\n",
    "                print(\"   Strategy: Use fallback service or wait\")\n",
    "                print(\"   Waiting 3 seconds for service recovery...\")\n",
    "                time.sleep(3)\n",
    "                print(\"   ✅ Ready to retry with circuit breaker\")\n",
    "                \n",
    "            else:\n",
    "                print(\"   Strategy: Wait and retry with fallback model\")\n",
    "                print(\"   ✅ Fallback strategy ready\")\n",
    "            \n",
    "            # Show what would happen next\n",
    "            print(\"\\n📝 Next steps:\")\n",
    "            if error_class in [ErrorType.RATE_LIMIT, ErrorType.TIMEOUT, ErrorType.SERVICE_UNAVAILABLE]:\n",
    "                print(\"   1. Retry with exponential backoff\")\n",
    "                print(\"   2. If fails, try alternative model\")\n",
    "                print(\"   3. If still fails, return cached response\")\n",
    "            elif error_class == ErrorType.CONTEXT_LENGTH:\n",
    "                print(\"   1. Truncate oldest messages\")\n",
    "                print(\"   2. Summarize context if possible\")\n",
    "                print(\"   3. Retry with reduced context\")\n",
    "            elif error_class == ErrorType.AUTHENTICATION:\n",
    "                print(\"   1. Notify user of auth issue\")\n",
    "                print(\"   2. Provide instructions to fix\")\n",
    "                print(\"   3. Cannot proceed without valid credentials\")\n",
    "\n",
    "test_button.on_click(test_error_handling)\n",
    "\n",
    "# Display the interface\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎯 Select an error type and click to simulate:</h3>\"),\n",
    "    widgets.HTML(\"<p style='color: #666;'>Choose 'none' to make a real API call, or select an error to simulate</p>\"),\n",
    "    error_dropdown,\n",
    "    test_button,\n",
    "    output\n",
    "]))\n",
    "\n",
    "print(\"\\n💡 Tips:\")\n",
    "print(\"• Select 'none' to test a real API call to gpt-4o\")\n",
    "print(\"• Try different error types to see recovery strategies\")\n",
    "print(\"• Notice how different errors require different handling approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎭 **Module 2: Graceful Degradation Mastery**\n",
    "## The Art of Failing Elegantly 🩰\n",
    "\n",
    "---\n",
    "\n",
    "## 🏰 **The Degradation Castle**\n",
    "Your AI system has multiple defensive walls. When one falls, another stands ready:\n",
    "\n",
    "```\n",
    "         👑 The Keep (Premium)\n",
    "         └─ GPT-4: Full features, $$$\n",
    "              ↓ fails\n",
    "         🛡️ Inner Wall (Standard)  \n",
    "         └─ GPT-3.5: Core features, $$\n",
    "              ↓ fails\n",
    "         ⚔️ Outer Wall (Cache)\n",
    "         └─ Previous responses, ¢\n",
    "              ↓ fails  \n",
    "         🌉 The Moat (Static)\n",
    "         └─ Template responses, free\n",
    "              ↓ fails\n",
    "         🏳️ Honest error message\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **The Service Ladder**\n",
    "\n",
    "| Level | Reliability | Speed | Cost | User Experience |\n",
    "|-------|------------|-------|------|-----------------|\n",
    "| Premium | 95% | 2-3s | $$$ | Full features 😊 |\n",
    "| Standard | 98% | 1-2s | $$ | Core features 🙂 |\n",
    "| Cache | 99.9% | <100ms | ¢ | Recent responses 😐 |\n",
    "| Static | 99.99% | <10ms | Free | Basic templates 😔 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **Core Principle: The 80/20 Rule**\n",
    "\n",
    "**80% of user value comes from 20% of features**\n",
    "\n",
    "Essential (Keep Alive):\n",
    "- Basic text responses\n",
    "- Core functionality  \n",
    "- Simple queries\n",
    "\n",
    "Nice-to-Have (Can Degrade):\n",
    "- Advanced analytics\n",
    "- Real-time updates\n",
    "- Complex formatting\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Good Patterns**\n",
    "\n",
    "### The Waterfall\n",
    "```python\n",
    "services = [premium_ai, standard_ai, cache, static]\n",
    "for service in services:\n",
    "    try:\n",
    "        return await service.handle()\n",
    "    except:\n",
    "        continue  # Try next level\n",
    "```\n",
    "\n",
    "### Feature Flags\n",
    "```python\n",
    "if system_degraded:\n",
    "    disable_features(['analytics', 'formatting'])\n",
    "    keep_features(['basic_response', 'cache'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ❌ **Anti-Patterns to Avoid**\n",
    "\n",
    "1. **The Cliff** - All or nothing approach\n",
    "2. **Silent Degradation** - Not telling users\n",
    "3. **Lying Fallback** - Fake responses\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 **Degradation in Action**\n",
    "\n",
    "```\n",
    "Normal:    [🟢 Premium ] → Full response with all features\n",
    "Degraded:  [🟡 Standard] → \"Using simplified AI (high demand)\"  \n",
    "Emergency: [🔴 Cache   ] → \"Here's a recent similar response\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 **Key Lessons**\n",
    "\n",
    "1. **Partial success > Complete failure**\n",
    "2. **Fast mediocrity > Slow excellence**\n",
    "3. **Honest communication > Silent degradation**\n",
    "4. **Something > Nothing**\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **Success Metrics**\n",
    "\n",
    "```\n",
    "✅ Availability: 99.95%\n",
    "✅ Graceful failures: 98%  \n",
    "✅ User satisfaction during degradation: 4.2/5\n",
    "✅ Cost savings: 40%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 **Real-World Examples**\n",
    "\n",
    "**Netflix:** 4K → HD → SD during peak hours\n",
    "**Twitter:** Full timeline → Cached timeline under load\n",
    "**Your System:** Premium AI → Standard → Cache → Static\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 **Remember**\n",
    "\n",
    "> \"Users don't care about your architecture, they care about getting answers.\"\n",
    "\n",
    "> \"The best degradation is the one users don't notice.\"\n",
    "\n",
    "---\n",
    "\n",
    "**Your Toolkit:**\n",
    "- 🏰 Multiple fallback layers\n",
    "- 🎯 Core feature preservation\n",
    "- 📊 Clear communication\n",
    "- ✅ Tested degradation paths\n",
    "\n",
    "**Next:** Rate Limiting - Because sometimes \"slow down\" is the answer! 🚦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏰 Graceful Degradation Service Initialized!\n",
      "\n",
      "📊 Service Tiers:\n",
      "  • PREMIUM: gpt-4o (500 tokens)\n",
      "  • STANDARD: gpt-4o-mini (300 tokens)\n",
      "  • BASIC: gpt-3.5-turbo (100 tokens)\n",
      "  • CACHE: Special handling\n",
      "  • STATIC: Special handling\n",
      "\n",
      "✨ Features:\n",
      "  • Automatic tier fallback\n",
      "  • Response caching\n",
      "  • Static fallbacks\n",
      "  • Detailed metrics\n",
      "\n",
      "✅ Ready for testing!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 4: Multi-Tier Fallback System\n",
    "# Build a production-grade graceful degradation system!\n",
    "\n",
    "from enum import Enum, auto\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Dict, Any, List\n",
    "from collections import defaultdict, deque\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "import random\n",
    "\n",
    "class ServiceTier(Enum):\n",
    "    \"\"\"Service quality tiers\"\"\"\n",
    "    PREMIUM = auto()\n",
    "    STANDARD = auto()\n",
    "    BASIC = auto()\n",
    "    CACHE = auto()\n",
    "    STATIC = auto()\n",
    "\n",
    "class FallbackStrategy(ABC):\n",
    "    \"\"\"Abstract base for fallback strategies\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, request: Dict[str, Any]) -> Any:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def can_handle(self, request: Dict[str, Any]) -> bool:\n",
    "        pass\n",
    "\n",
    "class ResponseCache:\n",
    "    \"\"\"Intelligent response caching\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000):\n",
    "        self.cache = {}\n",
    "        self.access_times = {}\n",
    "        self.max_size = max_size\n",
    "        \n",
    "    def _get_key(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Generate cache key from messages\"\"\"\n",
    "        content = json.dumps(messages, sort_keys=True)\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, messages: List[Dict]) -> Optional[str]:\n",
    "        \"\"\"Retrieve cached response\"\"\"\n",
    "        key = self._get_key(messages)\n",
    "        if key in self.cache:\n",
    "            self.access_times[key] = time.time()\n",
    "            print(f\"   🎯 Cache hit for key: {key[:8]}...\")\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def set(self, messages: List[Dict], response: str):\n",
    "        \"\"\"Cache a response\"\"\"\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            # Evict least recently used\n",
    "            lru_key = min(self.access_times, key=self.access_times.get)\n",
    "            del self.cache[lru_key]\n",
    "            del self.access_times[lru_key]\n",
    "        \n",
    "        key = self._get_key(messages)\n",
    "        self.cache[key] = response\n",
    "        self.access_times[key] = time.time()\n",
    "        print(f\"   💾 Response cached with key: {key[:8]}...\")\n",
    "\n",
    "class GracefulDegradationService:\n",
    "    \"\"\"Multi-tier service with automatic fallback\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = openai.OpenAI()\n",
    "        self.cache = ResponseCache()\n",
    "        self.metrics = defaultdict(int)\n",
    "        self.tier_history = deque(maxlen=100)\n",
    "        \n",
    "        # Define tier configurations - Fixed model names!\n",
    "        self.tier_configs = {\n",
    "            ServiceTier.PREMIUM: {\n",
    "                'model': 'gpt-4o',  # Premium tier\n",
    "                'max_tokens': 500,\n",
    "                'temperature': 0.7,\n",
    "                'timeout': 30\n",
    "            },\n",
    "            ServiceTier.STANDARD: {\n",
    "                'model': 'gpt-4o-mini',  # Fixed typo: was \"pt-4o-mini\"\n",
    "                'max_tokens': 300,\n",
    "                'temperature': 0.7,\n",
    "                'timeout': 20\n",
    "            },\n",
    "            ServiceTier.BASIC: {\n",
    "                'model': 'gpt-3.5-turbo',  # Basic tier\n",
    "                'max_tokens': 100,\n",
    "                'temperature': 0.5,\n",
    "                'timeout': 10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Static fallbacks for common queries\n",
    "        self.static_responses = {\n",
    "            'greeting': \"Hello! I'm experiencing high demand but I'm here to help.\",\n",
    "            'help': \"I can assist with various tasks. Please be patient during high load.\",\n",
    "            'error': \"I'm currently experiencing difficulties. Please try again shortly.\",\n",
    "            'general': \"I'm operating in limited mode. Your query has been noted.\"\n",
    "        }\n",
    "        \n",
    "        # Control simulation\n",
    "        self.simulate_failures = False\n",
    "        self.failure_tiers = []\n",
    "    \n",
    "    def classify_query(self, messages: List[Dict]) -> str:\n",
    "        \"\"\"Classify query type for static fallback\"\"\"\n",
    "        if not messages:\n",
    "            return 'general'\n",
    "            \n",
    "        last_message = messages[-1].get('content', '').lower()\n",
    "        \n",
    "        if any(word in last_message for word in ['hello', 'hi', 'hey']):\n",
    "            return 'greeting'\n",
    "        elif 'help' in last_message:\n",
    "            return 'help'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    def execute_with_fallback(\n",
    "        self,\n",
    "        messages: List[Dict],\n",
    "        preferred_tier: ServiceTier = ServiceTier.PREMIUM\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Execute request with automatic degradation - SYNCHRONOUS VERSION\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        tiers_to_try = list(ServiceTier)\n",
    "        \n",
    "        # Start from preferred tier\n",
    "        start_index = tiers_to_try.index(preferred_tier)\n",
    "        tiers_to_try = tiers_to_try[start_index:]\n",
    "        \n",
    "        print(f\"🔄 Starting with tier: {preferred_tier.name}\")\n",
    "        print(f\"   Available fallback chain: {[t.name for t in tiers_to_try]}\")\n",
    "        \n",
    "        for tier in tiers_to_try:\n",
    "            try:\n",
    "                self.metrics[f'attempt_{tier.name}'] += 1\n",
    "                print(f\"\\n   Trying tier: {tier.name}...\")\n",
    "                \n",
    "                # Simulate failures if enabled\n",
    "                if self.simulate_failures and tier in self.failure_tiers:\n",
    "                    print(f\"   💥 Simulating failure for {tier.name}\")\n",
    "                    raise Exception(f\"Simulated failure for tier {tier.name}\")\n",
    "                \n",
    "                # Try cache first\n",
    "                if tier == ServiceTier.CACHE:\n",
    "                    cached = self.cache.get(messages)\n",
    "                    if cached:\n",
    "                        self.metrics['cache_hits'] += 1\n",
    "                        print(f\"   ✅ Cache hit!\")\n",
    "                        return {\n",
    "                            'content': cached,\n",
    "                            'tier': tier.name,\n",
    "                            'cached': True,\n",
    "                            'latency': time.time() - start_time\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"   ❌ Cache miss, trying next tier...\")\n",
    "                        continue  # Try next tier\n",
    "                \n",
    "                # Try static response\n",
    "                elif tier == ServiceTier.STATIC:\n",
    "                    query_type = self.classify_query(messages)\n",
    "                    response = self.static_responses.get(\n",
    "                        query_type, \n",
    "                        self.static_responses['error']\n",
    "                    )\n",
    "                    print(f\"   ✅ Using static response for '{query_type}' query\")\n",
    "                    return {\n",
    "                        'content': response,\n",
    "                        'tier': tier.name,\n",
    "                        'static': True,\n",
    "                        'latency': time.time() - start_time\n",
    "                    }\n",
    "                \n",
    "                # Try API call\n",
    "                else:\n",
    "                    config = self.tier_configs[tier]\n",
    "                    print(f\"   📡 Calling {config['model']} API...\")\n",
    "                    \n",
    "                    response = self.client.chat.completions.create(\n",
    "                        messages=messages,\n",
    "                        **config\n",
    "                    )\n",
    "                    \n",
    "                    content = response.choices[0].message.content\n",
    "                    \n",
    "                    # Cache successful responses\n",
    "                    self.cache.set(messages, content)\n",
    "                    \n",
    "                    self.metrics[f'success_{tier.name}'] += 1\n",
    "                    self.tier_history.append(tier)\n",
    "                    \n",
    "                    print(f\"   ✅ Success with {tier.name}!\")\n",
    "                    \n",
    "                    return {\n",
    "                        'content': content,\n",
    "                        'tier': tier.name,\n",
    "                        'cached': False,\n",
    "                        'latency': time.time() - start_time,\n",
    "                        'model': config['model']\n",
    "                    }\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️ Tier {tier.name} failed: {str(e)[:50]}\")\n",
    "                self.metrics[f'failure_{tier.name}'] += 1\n",
    "                continue\n",
    "        \n",
    "        # Ultimate fallback\n",
    "        print(f\"   ❌ All tiers failed!\")\n",
    "        return {\n",
    "            'content': \"Service temporarily unavailable. Please try again.\",\n",
    "            'tier': 'FALLBACK',\n",
    "            'error': True,\n",
    "            'latency': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def get_service_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get detailed service metrics\"\"\"\n",
    "        total_requests = sum(v for k, v in self.metrics.items() if k.startswith('attempt_'))\n",
    "        \n",
    "        if total_requests == 0:\n",
    "            return {}\n",
    "        \n",
    "        tier_distribution = {}\n",
    "        for tier in ServiceTier:\n",
    "            successes = self.metrics.get(f'success_{tier.name}', 0)\n",
    "            attempts = self.metrics.get(f'attempt_{tier.name}', 0)\n",
    "            failures = self.metrics.get(f'failure_{tier.name}', 0)\n",
    "            tier_distribution[tier.name] = {\n",
    "                'attempts': attempts,\n",
    "                'successes': successes,\n",
    "                'failures': failures,\n",
    "                'success_rate': successes / attempts if attempts > 0 else 0\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'total_requests': total_requests,\n",
    "            'cache_hits': self.metrics.get('cache_hits', 0),\n",
    "            'cache_hit_rate': self.metrics.get('cache_hits', 0) / total_requests if total_requests > 0 else 0,\n",
    "            'tier_distribution': tier_distribution\n",
    "        }\n",
    "\n",
    "# Initialize the service\n",
    "degradation_service = GracefulDegradationService()\n",
    "\n",
    "print(\"🏰 Graceful Degradation Service Initialized!\")\n",
    "print(\"\\n📊 Service Tiers:\")\n",
    "for tier in ServiceTier:\n",
    "    if tier in degradation_service.tier_configs:\n",
    "        config = degradation_service.tier_configs[tier]\n",
    "        print(f\"  • {tier.name}: {config['model']} ({config['max_tokens']} tokens)\")\n",
    "    else:\n",
    "        print(f\"  • {tier.name}: Special handling\")\n",
    "\n",
    "print(\"\\n✨ Features:\")\n",
    "print(\"  • Automatic tier fallback\")\n",
    "print(\"  • Response caching\")\n",
    "print(\"  • Static fallbacks\")\n",
    "print(\"  • Detailed metrics\")\n",
    "print(\"\\n✅ Ready for testing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎮 **Interactive Degradation Tester**\n",
    "#### Watch your system gracefully handle failures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Graceful Degradation Testing Dashboard\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348403b3c95049c690288530b4d12c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🏰 Test Graceful Degradation</h3>'), HTML(value=\"<p style='color: #666;'>Enter a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 How to Test:\n",
      "  1. Run a query normally first (no failures)\n",
      "  2. Run the same query again to see cache hit\n",
      "  3. Enable failure checkboxes to force fallbacks\n",
      "  4. Try starting from different tiers\n",
      "  5. Watch how the system degrades gracefully!\n",
      "\n",
      "⚡ Pro tip: Check multiple failure boxes to see cascading fallbacks!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 5: Live Degradation Testing Dashboard\n",
    "# See graceful degradation in action with real API calls!\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🎮 Graceful Degradation Testing Dashboard\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create interactive controls\n",
    "query_input = widgets.Textarea(\n",
    "    value='What is the meaning of life?',\n",
    "    placeholder='Enter your query...',\n",
    "    description='Query:',\n",
    "    layout=widgets.Layout(width='500px', height='80px')\n",
    ")\n",
    "\n",
    "tier_selector = widgets.RadioButtons(\n",
    "    options=['PREMIUM', 'STANDARD', 'BASIC', 'CACHE', 'STATIC'],\n",
    "    value='PREMIUM',\n",
    "    description='Start Tier:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Failure simulation checkboxes\n",
    "failure_premium = widgets.Checkbox(value=False, description='Fail PREMIUM')\n",
    "failure_standard = widgets.Checkbox(value=False, description='Fail STANDARD')\n",
    "failure_basic = widgets.Checkbox(value=False, description='Fail BASIC')\n",
    "\n",
    "test_button = widgets.Button(\n",
    "    description='🚀 Test Degradation',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "clear_cache_button = widgets.Button(\n",
    "    description='🗑️ Clear Cache',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "metrics_output = widgets.Output()\n",
    "\n",
    "# Test history for visualization\n",
    "test_history = []\n",
    "\n",
    "def run_degradation_test(b):\n",
    "    \"\"\"Run the degradation test - SYNCHRONOUS\"\"\"\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        \n",
    "        print(\"🔄 Testing degradation system...\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Keep responses brief.\"},\n",
    "            {\"role\": \"user\", \"content\": query_input.value}\n",
    "        ]\n",
    "        \n",
    "        # Get starting tier\n",
    "        start_tier = ServiceTier[tier_selector.value]\n",
    "        \n",
    "        # Configure failure simulation\n",
    "        degradation_service.simulate_failures = any([\n",
    "            failure_premium.value,\n",
    "            failure_standard.value,\n",
    "            failure_basic.value\n",
    "        ])\n",
    "        \n",
    "        degradation_service.failure_tiers = []\n",
    "        if failure_premium.value:\n",
    "            degradation_service.failure_tiers.append(ServiceTier.PREMIUM)\n",
    "        if failure_standard.value:\n",
    "            degradation_service.failure_tiers.append(ServiceTier.STANDARD)\n",
    "        if failure_basic.value:\n",
    "            degradation_service.failure_tiers.append(ServiceTier.BASIC)\n",
    "        \n",
    "        if degradation_service.failure_tiers:\n",
    "            print(f\"⚠️ Simulating failures for: {[t.name for t in degradation_service.failure_tiers]}\")\n",
    "        \n",
    "        try:\n",
    "            # Execute with fallback - SYNCHRONOUS call\n",
    "            result = degradation_service.execute_with_fallback(\n",
    "                messages, \n",
    "                preferred_tier=start_tier\n",
    "            )\n",
    "            \n",
    "            # Display result\n",
    "            print(f\"\\n✅ Response received!\")\n",
    "            print(\"=\"*40)\n",
    "            print(f\"\\n📊 Result Details:\")\n",
    "            print(f\"  • Final Tier: {result['tier']}\")\n",
    "            print(f\"  • Latency: {result['latency']:.2f}s\")\n",
    "            print(f\"  • From Cache: {result.get('cached', False)}\")\n",
    "            print(f\"  • Static Response: {result.get('static', False)}\")\n",
    "            if 'model' in result:\n",
    "                print(f\"  • Model Used: {result['model']}\")\n",
    "            \n",
    "            print(f\"\\n💬 Response:\")\n",
    "            print(\"-\"*40)\n",
    "            # Display full response or truncated version\n",
    "            content = result['content']\n",
    "            if len(content) > 300:\n",
    "                print(f\"{content[:300]}...\")\n",
    "                print(f\"\\n[Response truncated - {len(content)} total characters]\")\n",
    "            else:\n",
    "                print(content)\n",
    "            \n",
    "            # Store in history\n",
    "            test_history.append({\n",
    "                'timestamp': datetime.now(),\n",
    "                'query': query_input.value[:30],\n",
    "                'tier': result['tier'],\n",
    "                'latency': result['latency'],\n",
    "                'cached': result.get('cached', False)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Unexpected error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        \n",
    "        finally:\n",
    "            # Reset failure simulation\n",
    "            degradation_service.simulate_failures = False\n",
    "            degradation_service.failure_tiers = []\n",
    "    \n",
    "    # Update metrics display\n",
    "    update_metrics()\n",
    "\n",
    "def clear_cache(b):\n",
    "    \"\"\"Clear the response cache\"\"\"\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        cache_size = len(degradation_service.cache.cache)\n",
    "        degradation_service.cache.cache.clear()\n",
    "        degradation_service.cache.access_times.clear()\n",
    "        print(f\"🗑️ Cleared {cache_size} cached responses\")\n",
    "    update_metrics()\n",
    "\n",
    "def update_metrics():\n",
    "    \"\"\"Update metrics display\"\"\"\n",
    "    with metrics_output:\n",
    "        clear_output()\n",
    "        \n",
    "        metrics = degradation_service.get_service_metrics()\n",
    "        if metrics:\n",
    "            print(\"\\n📈 Service Metrics\")\n",
    "            print(\"=\"*40)\n",
    "            print(f\"Total Requests: {metrics.get('total_requests', 0)}\")\n",
    "            print(f\"Cache Hits: {metrics.get('cache_hits', 0)}\")\n",
    "            print(f\"Cache Hit Rate: {metrics.get('cache_hit_rate', 0):.1%}\")\n",
    "            print(f\"Current Cache Size: {len(degradation_service.cache.cache)}\")\n",
    "            \n",
    "            print(\"\\n📊 Tier Performance:\")\n",
    "            for tier_name, stats in metrics.get('tier_distribution', {}).items():\n",
    "                if stats['attempts'] > 0:\n",
    "                    print(f\"\\n  {tier_name}:\")\n",
    "                    print(f\"    Attempts: {stats['attempts']}\")\n",
    "                    print(f\"    Successes: {stats['successes']}\")\n",
    "                    print(f\"    Failures: {stats['failures']}\")\n",
    "                    print(f\"    Success Rate: {stats['success_rate']:.1%}\")\n",
    "        \n",
    "        if test_history:\n",
    "            print(\"\\n📝 Recent Tests:\")\n",
    "            print(\"-\"*40)\n",
    "            for test in test_history[-3:]:  # Show last 3 tests\n",
    "                print(f\"  • {test['timestamp'].strftime('%H:%M:%S')}: \"\n",
    "                      f\"{test['query']}... → {test['tier']} \"\n",
    "                      f\"({'cached' if test['cached'] else test['latency']:.1f}s)\")\n",
    "\n",
    "# Connect buttons\n",
    "test_button.on_click(run_degradation_test)\n",
    "clear_cache_button.on_click(clear_cache)\n",
    "\n",
    "# Create the dashboard\n",
    "dashboard = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🏰 Test Graceful Degradation</h3>\"),\n",
    "    widgets.HTML(\"<p style='color: #666;'>Enter a query and select options to test the fallback system</p>\"),\n",
    "    query_input,\n",
    "    widgets.HBox([\n",
    "        tier_selector, \n",
    "        widgets.VBox([\n",
    "            widgets.HTML(\"<b>Simulate Failures:</b>\"),\n",
    "            failure_premium,\n",
    "            failure_standard,\n",
    "            failure_basic\n",
    "        ])\n",
    "    ]),\n",
    "    widgets.HBox([test_button, clear_cache_button]),\n",
    "    widgets.HBox([output_area, metrics_output])\n",
    "])\n",
    "\n",
    "display(dashboard)\n",
    "\n",
    "print(\"\\n💡 How to Test:\")\n",
    "print(\"  1. Run a query normally first (no failures)\")\n",
    "print(\"  2. Run the same query again to see cache hit\")\n",
    "print(\"  3. Enable failure checkboxes to force fallbacks\")\n",
    "print(\"  4. Try starting from different tiers\")\n",
    "print(\"  5. Watch how the system degrades gracefully!\")\n",
    "print(\"\\n⚡ Pro tip: Check multiple failure boxes to see cascading fallbacks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚦 **Module 3: Rate Limiting & Quota Management**\n",
    "### Master the Art of API Traffic Control\n",
    "\n",
    "```python\n",
    "# The Rate Limit Dance 💃\n",
    "while tokens_remain:\n",
    "    if rate_limit_allows():\n",
    "        make_request()\n",
    "    else:\n",
    "        graceful_wait()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Advanced Smart Rate Limiter Initialized!\n",
      "\n",
      "📊 Rate Limits per Model:\n",
      "\n",
      "  gpt-4o:\n",
      "    • 500 requests/minute\n",
      "    • 30,000 tokens/minute\n",
      "    • 10,000 requests/day\n",
      "\n",
      "  gpt-4o-mini:\n",
      "    • 500 requests/minute\n",
      "    • 200,000 tokens/minute\n",
      "    • 50,000 requests/day\n",
      "\n",
      "  gpt-3.5-turbo:\n",
      "    • 500 requests/minute\n",
      "    • 200,000 tokens/minute\n",
      "    • 100,000 requests/day\n",
      "\n",
      "🎯 Advanced Features:\n",
      "  • Token bucket with burst capacity\n",
      "  • Sliding window with distribution tracking\n",
      "  • Adaptive rate limiting based on performance\n",
      "  • Circuit breaker pattern for fault tolerance\n",
      "  • Predictive analytics for usage patterns\n",
      "  • Comprehensive health scoring\n",
      "  • Real-time performance monitoring\n",
      "\n",
      "✅ System ready for production use!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 6: Advanced Rate Limiting System\n",
    "# Build a production-grade rate limiter with multiple algorithms!\n",
    "\n",
    "from enum import Enum, auto\n",
    "from collections import deque, defaultdict\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class RateLimitAlgorithm(Enum):\n",
    "    \"\"\"Rate limiting algorithms\"\"\"\n",
    "    TOKEN_BUCKET = auto()\n",
    "    SLIDING_WINDOW = auto()\n",
    "    FIXED_WINDOW = auto()\n",
    "    LEAKY_BUCKET = auto()\n",
    "    ADAPTIVE = auto()  # New: Adaptive rate limiting\n",
    "\n",
    "class TokenBucket:\n",
    "    \"\"\"Token bucket rate limiter with burst support\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int, refill_rate: float, burst_multiplier: float = 1.5):\n",
    "        self.capacity = capacity\n",
    "        self.refill_rate = refill_rate  # tokens per second\n",
    "        self.tokens = capacity\n",
    "        self.last_refill = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "        self.burst_capacity = capacity * burst_multiplier\n",
    "        self.burst_tokens = 0\n",
    "        \n",
    "        # Analytics\n",
    "        self.consumed_tokens = 0\n",
    "        self.rejected_tokens = 0\n",
    "        self.total_requests = 0\n",
    "    \n",
    "    def _refill(self):\n",
    "        \"\"\"Refill tokens based on elapsed time\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        \n",
    "        new_tokens = elapsed * self.refill_rate\n",
    "        self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
    "        \n",
    "        # Slowly refill burst tokens\n",
    "        if self.burst_tokens < self.burst_capacity - self.capacity:\n",
    "            self.burst_tokens = min(self.burst_capacity - self.capacity, \n",
    "                                   self.burst_tokens + new_tokens * 0.1)\n",
    "        \n",
    "        self.last_refill = now\n",
    "    \n",
    "    def consume(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"Try to consume tokens with burst support\"\"\"\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            self.total_requests += 1\n",
    "            \n",
    "            # Try regular tokens first\n",
    "            if self.tokens >= tokens:\n",
    "                self.tokens -= tokens\n",
    "                self.consumed_tokens += tokens\n",
    "                return True\n",
    "            \n",
    "            # Try burst tokens\n",
    "            total_available = self.tokens + self.burst_tokens\n",
    "            if total_available >= tokens:\n",
    "                needed_from_burst = tokens - self.tokens\n",
    "                self.burst_tokens -= needed_from_burst\n",
    "                self.tokens = 0\n",
    "                self.consumed_tokens += tokens\n",
    "                return True\n",
    "            \n",
    "            self.rejected_tokens += tokens\n",
    "            return False\n",
    "    \n",
    "    def wait_time(self, tokens: int = 1) -> float:\n",
    "        \"\"\"Calculate wait time for tokens\"\"\"\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            \n",
    "            total_available = self.tokens + self.burst_tokens\n",
    "            if total_available >= tokens:\n",
    "                return 0\n",
    "            \n",
    "            needed = tokens - total_available\n",
    "            return needed / self.refill_rate\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get current statistics\"\"\"\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            return {\n",
    "                'tokens': self.tokens,\n",
    "                'burst_tokens': self.burst_tokens,\n",
    "                'capacity': self.capacity,\n",
    "                'burst_capacity': self.burst_capacity,\n",
    "                'consumed': self.consumed_tokens,\n",
    "                'rejected': self.rejected_tokens,\n",
    "                'total_requests': self.total_requests,\n",
    "                'acceptance_rate': (self.consumed_tokens / \n",
    "                                   (self.consumed_tokens + self.rejected_tokens) \n",
    "                                   if (self.consumed_tokens + self.rejected_tokens) > 0 else 1.0)\n",
    "            }\n",
    "\n",
    "class SlidingWindowRateLimiter:\n",
    "    \"\"\"Enhanced sliding window with request distribution tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, max_requests: int, window_seconds: int):\n",
    "        self.max_requests = max_requests\n",
    "        self.window_seconds = window_seconds\n",
    "        self.requests = deque()\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Analytics\n",
    "        self.accepted = 0\n",
    "        self.rejected = 0\n",
    "        self.request_distribution = defaultdict(int)\n",
    "    \n",
    "    def _clean_old_requests(self):\n",
    "        \"\"\"Remove requests outside the window\"\"\"\n",
    "        cutoff = time.time() - self.window_seconds\n",
    "        removed = 0\n",
    "        while self.requests and self.requests[0] < cutoff:\n",
    "            self.requests.popleft()\n",
    "            removed += 1\n",
    "        return removed\n",
    "    \n",
    "    def allow_request(self) -> bool:\n",
    "        \"\"\"Check if request is allowed\"\"\"\n",
    "        with self.lock:\n",
    "            self._clean_old_requests()\n",
    "            \n",
    "            current_second = int(time.time())\n",
    "            \n",
    "            if len(self.requests) < self.max_requests:\n",
    "                self.requests.append(time.time())\n",
    "                self.accepted += 1\n",
    "                self.request_distribution[current_second] += 1\n",
    "                return True\n",
    "            \n",
    "            self.rejected += 1\n",
    "            return False\n",
    "    \n",
    "    def wait_time(self) -> float:\n",
    "        \"\"\"Calculate wait time until next slot\"\"\"\n",
    "        with self.lock:\n",
    "            self._clean_old_requests()\n",
    "            \n",
    "            if len(self.requests) < self.max_requests:\n",
    "                return 0\n",
    "            \n",
    "            oldest = self.requests[0]\n",
    "            wait = self.window_seconds - (time.time() - oldest)\n",
    "            return max(0, wait)\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get current statistics\"\"\"\n",
    "        with self.lock:\n",
    "            self._clean_old_requests()\n",
    "            return {\n",
    "                'current_requests': len(self.requests),\n",
    "                'max_requests': self.max_requests,\n",
    "                'utilization': len(self.requests) / self.max_requests * 100,\n",
    "                'accepted': self.accepted,\n",
    "                'rejected': self.rejected,\n",
    "                'acceptance_rate': self.accepted / (self.accepted + self.rejected) \n",
    "                                  if (self.accepted + self.rejected) > 0 else 1.0\n",
    "            }\n",
    "\n",
    "class AdaptiveRateLimiter:\n",
    "    \"\"\"Self-adjusting rate limiter based on error rates\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_rate: int, min_rate: int, max_rate: int):\n",
    "        self.current_rate = initial_rate\n",
    "        self.min_rate = min_rate\n",
    "        self.max_rate = max_rate\n",
    "        self.error_window = deque(maxlen=100)\n",
    "        self.success_window = deque(maxlen=100)\n",
    "        self.adjustment_interval = 10  # seconds\n",
    "        self.last_adjustment = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Underlying limiter\n",
    "        self.limiter = SlidingWindowRateLimiter(initial_rate, 60)\n",
    "    \n",
    "    def record_result(self, success: bool):\n",
    "        \"\"\"Record request result for adaptation\"\"\"\n",
    "        with self.lock:\n",
    "            if success:\n",
    "                self.success_window.append(time.time())\n",
    "            else:\n",
    "                self.error_window.append(time.time())\n",
    "            \n",
    "            # Check if we should adjust\n",
    "            if time.time() - self.last_adjustment > self.adjustment_interval:\n",
    "                self._adjust_rate()\n",
    "    \n",
    "    def _adjust_rate(self):\n",
    "        \"\"\"Adjust rate based on error rate\"\"\"\n",
    "        total = len(self.error_window) + len(self.success_window)\n",
    "        if total == 0:\n",
    "            return\n",
    "        \n",
    "        error_rate = len(self.error_window) / total\n",
    "        \n",
    "        # Adjust based on error rate\n",
    "        if error_rate > 0.1:  # More than 10% errors\n",
    "            # Decrease rate\n",
    "            self.current_rate = max(self.min_rate, int(self.current_rate * 0.9))\n",
    "        elif error_rate < 0.01:  # Less than 1% errors\n",
    "            # Increase rate\n",
    "            self.current_rate = min(self.max_rate, int(self.current_rate * 1.1))\n",
    "        \n",
    "        # Update underlying limiter\n",
    "        self.limiter = SlidingWindowRateLimiter(self.current_rate, 60)\n",
    "        self.last_adjustment = time.time()\n",
    "    \n",
    "    def allow_request(self) -> bool:\n",
    "        \"\"\"Check if request is allowed\"\"\"\n",
    "        return self.limiter.allow_request()\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get current statistics\"\"\"\n",
    "        with self.lock:\n",
    "            total = len(self.error_window) + len(self.success_window)\n",
    "            error_rate = len(self.error_window) / total if total > 0 else 0\n",
    "            \n",
    "            return {\n",
    "                'current_rate': self.current_rate,\n",
    "                'min_rate': self.min_rate,\n",
    "                'max_rate': self.max_rate,\n",
    "                'error_rate': error_rate,\n",
    "                'success_rate': 1 - error_rate,\n",
    "                **self.limiter.get_stats()\n",
    "            }\n",
    "\n",
    "class SmartRateLimiter:\n",
    "    \"\"\"Intelligent multi-algorithm rate limiter with predictive analytics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # OpenAI-like rate limits with realistic values\n",
    "        self.limits = {\n",
    "            'gpt-4o': {\n",
    "                'rpm': 500,      # Requests per minute\n",
    "                'tpm': 30000,    # Tokens per minute\n",
    "                'rpd': 10000     # Requests per day\n",
    "            },\n",
    "            'gpt-4o-mini': {\n",
    "                'rpm': 500,\n",
    "                'tpm': 200000,   \n",
    "                'rpd': 50000\n",
    "            },\n",
    "            'gpt-3.5-turbo': {\n",
    "                'rpm': 500,\n",
    "                'tpm': 200000,\n",
    "                'rpd': 100000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize rate limiters for each model\n",
    "        self.request_limiters = {}\n",
    "        self.token_limiters = {}\n",
    "        self.adaptive_limiters = {}\n",
    "        \n",
    "        for model, limits in self.limits.items():\n",
    "            # Request limiter (sliding window)\n",
    "            self.request_limiters[model] = SlidingWindowRateLimiter(\n",
    "                max_requests=limits['rpm'],\n",
    "                window_seconds=60\n",
    "            )\n",
    "            \n",
    "            # Token limiter (token bucket with burst)\n",
    "            self.token_limiters[model] = TokenBucket(\n",
    "                capacity=limits['tpm'],\n",
    "                refill_rate=limits['tpm'] / 60,\n",
    "                burst_multiplier=1.5\n",
    "            )\n",
    "            \n",
    "            # Adaptive limiter\n",
    "            self.adaptive_limiters[model] = AdaptiveRateLimiter(\n",
    "                initial_rate=limits['rpm'],\n",
    "                min_rate=limits['rpm'] // 10,\n",
    "                max_rate=limits['rpm'] * 2\n",
    "            )\n",
    "        \n",
    "        # Comprehensive metrics\n",
    "        self.metrics = defaultdict(lambda: defaultdict(int))\n",
    "        self.request_history = deque(maxlen=10000)\n",
    "        self.prediction_data = defaultdict(list)\n",
    "        \n",
    "        # Circuit breaker\n",
    "        self.circuit_breakers = defaultdict(lambda: {'state': 'closed', 'failures': 0, 'last_failure': None})\n",
    "    \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"More accurate token estimation\"\"\"\n",
    "        # Better approximation based on OpenAI's tokenizer\n",
    "        words = len(text.split())\n",
    "        chars = len(text)\n",
    "        \n",
    "        # Average of word-based and char-based estimation\n",
    "        word_estimate = words * 1.3\n",
    "        char_estimate = chars / 4\n",
    "        \n",
    "        return int((word_estimate + char_estimate) / 2)\n",
    "    \n",
    "    def check_limits(self, model: str, estimated_tokens: int) -> Dict[str, Any]:\n",
    "        \"\"\"Enhanced limit checking with circuit breaker\"\"\"\n",
    "        result = {\n",
    "            'allowed': False,\n",
    "            'wait_time': 0,\n",
    "            'reason': None,\n",
    "            'suggestions': []\n",
    "        }\n",
    "        \n",
    "        if model not in self.request_limiters:\n",
    "            result['allowed'] = True\n",
    "            return result\n",
    "        \n",
    "        # Check circuit breaker\n",
    "        breaker = self.circuit_breakers[model]\n",
    "        if breaker['state'] == 'open':\n",
    "            if breaker['last_failure'] and time.time() - breaker['last_failure'] > 30:\n",
    "                # Try to close circuit\n",
    "                breaker['state'] = 'half-open'\n",
    "            else:\n",
    "                result['reason'] = 'circuit_breaker_open'\n",
    "                result['wait_time'] = 30 - (time.time() - breaker['last_failure'])\n",
    "                result['suggestions'].append(\"Service temporarily disabled due to errors\")\n",
    "                return result\n",
    "        \n",
    "        # Check request limit\n",
    "        request_limiter = self.request_limiters[model]\n",
    "        if not request_limiter.allow_request():\n",
    "            result['wait_time'] = request_limiter.wait_time()\n",
    "            result['reason'] = 'request_limit'\n",
    "            result['suggestions'].append(f\"Reduce request rate or wait {result['wait_time']:.1f}s\")\n",
    "            return result\n",
    "        \n",
    "        # Check token limit\n",
    "        token_limiter = self.token_limiters[model]\n",
    "        if not token_limiter.consume(estimated_tokens):\n",
    "            result['wait_time'] = token_limiter.wait_time(estimated_tokens)\n",
    "            result['reason'] = 'token_limit'\n",
    "            result['suggestions'].append(f\"Reduce prompt size or use a different model\")\n",
    "            # Roll back request count\n",
    "            request_limiter.requests.pop()\n",
    "            request_limiter.accepted -= 1\n",
    "            return result\n",
    "        \n",
    "        # Check adaptive limit\n",
    "        if not self.adaptive_limiters[model].allow_request():\n",
    "            result['wait_time'] = 1.0\n",
    "            result['reason'] = 'adaptive_limit'\n",
    "            result['suggestions'].append(\"System is auto-adjusting rates based on performance\")\n",
    "            # Roll back other limits\n",
    "            request_limiter.requests.pop()\n",
    "            request_limiter.accepted -= 1\n",
    "            token_limiter.tokens += estimated_tokens\n",
    "            token_limiter.consumed_tokens -= estimated_tokens\n",
    "            return result\n",
    "        \n",
    "        result['allowed'] = True\n",
    "        return result\n",
    "    \n",
    "    def record_usage(self, model: str, tokens_used: int, latency: float, success: bool = True):\n",
    "        \"\"\"Record comprehensive usage metrics\"\"\"\n",
    "        self.metrics[model]['requests'] += 1\n",
    "        self.metrics[model]['tokens'] += tokens_used\n",
    "        self.metrics[model]['total_latency'] += latency\n",
    "        \n",
    "        if success:\n",
    "            self.metrics[model]['successes'] += 1\n",
    "            self.circuit_breakers[model]['failures'] = 0\n",
    "            if self.circuit_breakers[model]['state'] == 'half-open':\n",
    "                self.circuit_breakers[model]['state'] = 'closed'\n",
    "        else:\n",
    "            self.metrics[model]['failures'] += 1\n",
    "            self.circuit_breakers[model]['failures'] += 1\n",
    "            self.circuit_breakers[model]['last_failure'] = time.time()\n",
    "            \n",
    "            if self.circuit_breakers[model]['failures'] >= 5:\n",
    "                self.circuit_breakers[model]['state'] = 'open'\n",
    "        \n",
    "        self.adaptive_limiters[model].record_result(success)\n",
    "        \n",
    "        self.request_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'model': model,\n",
    "            'tokens': tokens_used,\n",
    "            'latency': latency,\n",
    "            'success': success\n",
    "        })\n",
    "        \n",
    "        # Update prediction data\n",
    "        hour = datetime.now().hour\n",
    "        self.prediction_data[model].append({\n",
    "            'hour': hour,\n",
    "            'tokens': tokens_used,\n",
    "            'latency': latency\n",
    "        })\n",
    "    \n",
    "    def predict_usage(self, model: str, hours_ahead: int = 1) -> dict:\n",
    "        \"\"\"Predict future usage based on patterns\"\"\"\n",
    "        if model not in self.prediction_data or len(self.prediction_data[model]) < 10:\n",
    "            return {'predicted_load': 'insufficient_data'}\n",
    "        \n",
    "        # Simple prediction based on recent patterns\n",
    "        recent = self.prediction_data[model][-100:]\n",
    "        avg_tokens = sum(r['tokens'] for r in recent) / len(recent)\n",
    "        avg_latency = sum(r['latency'] for r in recent) / len(recent)\n",
    "        \n",
    "        current_hour = datetime.now().hour\n",
    "        target_hour = (current_hour + hours_ahead) % 24\n",
    "        \n",
    "        # Hour-based patterns\n",
    "        hour_data = [r for r in recent if r['hour'] == target_hour]\n",
    "        if hour_data:\n",
    "            predicted_tokens = sum(r['tokens'] for r in hour_data) / len(hour_data)\n",
    "            predicted_load = 'high' if predicted_tokens > avg_tokens * 1.2 else 'normal'\n",
    "        else:\n",
    "            predicted_load = 'normal'\n",
    "        \n",
    "        return {\n",
    "            'predicted_load': predicted_load,\n",
    "            'predicted_tokens': predicted_tokens if hour_data else avg_tokens,\n",
    "            'predicted_latency': avg_latency,\n",
    "            'confidence': min(len(recent) / 100, 1.0)\n",
    "        }\n",
    "    \n",
    "    def get_comprehensive_stats(self) -> dict:\n",
    "        \"\"\"Get detailed statistics for all models\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        for model in self.limits.keys():\n",
    "            request_stats = self.request_limiters[model].get_stats()\n",
    "            token_stats = self.token_limiters[model].get_stats()\n",
    "            adaptive_stats = self.adaptive_limiters[model].get_stats()\n",
    "            \n",
    "            # Calculate overall health score\n",
    "            health_score = (\n",
    "                request_stats['acceptance_rate'] * 0.3 +\n",
    "                token_stats['acceptance_rate'] * 0.3 +\n",
    "                adaptive_stats['success_rate'] * 0.2 +\n",
    "                (1 - request_stats['utilization'] / 100) * 0.2\n",
    "            ) * 100\n",
    "            \n",
    "            stats[model] = {\n",
    "                'request_limiter': request_stats,\n",
    "                'token_limiter': token_stats,\n",
    "                'adaptive_limiter': adaptive_stats,\n",
    "                'circuit_breaker': self.circuit_breakers[model]['state'],\n",
    "                'health_score': health_score,\n",
    "                'total_requests': self.metrics[model]['requests'],\n",
    "                'total_tokens': self.metrics[model]['tokens'],\n",
    "                'avg_latency': (self.metrics[model]['total_latency'] / \n",
    "                               self.metrics[model]['requests'] \n",
    "                               if self.metrics[model]['requests'] > 0 else 0),\n",
    "                'success_rate': (self.metrics[model]['successes'] / \n",
    "                                self.metrics[model]['requests'] \n",
    "                                if self.metrics[model]['requests'] > 0 else 1.0)\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Initialize the advanced rate limiter\n",
    "rate_limiter = SmartRateLimiter()\n",
    "\n",
    "print(\"🚀 Advanced Smart Rate Limiter Initialized!\")\n",
    "print(\"\\n📊 Rate Limits per Model:\")\n",
    "for model, limits in rate_limiter.limits.items():\n",
    "    print(f\"\\n  {model}:\")\n",
    "    print(f\"    • {limits['rpm']:,} requests/minute\")\n",
    "    print(f\"    • {limits['tpm']:,} tokens/minute\")\n",
    "    print(f\"    • {limits['rpd']:,} requests/day\")\n",
    "\n",
    "print(\"\\n🎯 Advanced Features:\")\n",
    "print(\"  • Token bucket with burst capacity\")\n",
    "print(\"  • Sliding window with distribution tracking\")\n",
    "print(\"  • Adaptive rate limiting based on performance\")\n",
    "print(\"  • Circuit breaker pattern for fault tolerance\")\n",
    "print(\"  • Predictive analytics for usage patterns\")\n",
    "print(\"  • Comprehensive health scoring\")\n",
    "print(\"  • Real-time performance monitoring\")\n",
    "\n",
    "print(\"\\n✅ System ready for production use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 **Live Rate Limit Monitor**\n",
    "#### Real-time visualization of your API usage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Ultimate Rate Limit Control Center\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c467cdf32d84614b7fe9128e778ffdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>🎛️ Control Panel</h2>'), HBox(children=(VBox(children=(Dropdown(description='Mo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c156844a1027431e87f9c301247e7002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Pro Tips:\n",
      "  🎯 Start with 'Steady' to baseline performance\n",
      "  💥 Try 'Burst' to see adaptive rate limiting in action\n",
      "  🌊 'Wave' pattern shows how the system handles oscillating load\n",
      "  🎲 'Chaos' tests unpredictable patterns\n",
      "  ☠️ 'DDoS' triggers circuit breakers and protection mechanisms\n",
      "\n",
      "🔄 Refresh the dashboard during tests to see live updates!\n",
      "📈 Watch how different algorithms work together to maintain stability!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 7: Ultimate Interactive Rate Limit Dashboard\n",
    "# Monitor and test rate limiting with advanced visualizations!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class UltimateRateLimitDashboard:\n",
    "    \"\"\"Advanced real-time rate limit monitoring and testing dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, rate_limiter: SmartRateLimiter):\n",
    "        self.rate_limiter = rate_limiter\n",
    "        self.test_running = False\n",
    "        self.test_thread = None\n",
    "        self.test_history = []\n",
    "        self.live_metrics = defaultdict(list)\n",
    "        \n",
    "        # Color schemes for beautiful visualizations\n",
    "        self.colors = {\n",
    "            'success': '#00d084',\n",
    "            'warning': '#ff9800',\n",
    "            'danger': '#ff3860',\n",
    "            'info': '#3273dc',\n",
    "            'dark': '#363636',\n",
    "            'light': '#f5f5f5'\n",
    "        }\n",
    "    \n",
    "    def create_live_dashboard(self):\n",
    "        \"\"\"Create comprehensive real-time dashboard\"\"\"\n",
    "        stats = self.rate_limiter.get_comprehensive_stats()\n",
    "        \n",
    "        # Create subplots with different chart types\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=3,\n",
    "            subplot_titles=(\n",
    "                '🎯 Health Scores', '📊 Request Limits', '💰 Token Limits',\n",
    "                '🔄 Adaptive Rates', '⚡ Circuit Breakers', '📈 Success Rates',\n",
    "                '⏱️ Latency Distribution', '🔮 Usage Prediction', '🏆 Performance Leaderboard'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{'type': 'bar'}, {'type': 'indicator'}, {'type': 'indicator'}],\n",
    "                [{'type': 'scatter'}, {'type': 'pie'}, {'type': 'bar'}],\n",
    "                [{'type': 'box'}, {'type': 'scatter'}, {'type': 'table'}]\n",
    "            ],\n",
    "            vertical_spacing=0.12,\n",
    "            horizontal_spacing=0.15\n",
    "        )\n",
    "        \n",
    "        models = list(self.rate_limiter.limits.keys())\n",
    "        \n",
    "        # 1. Health Scores - Beautiful bar chart\n",
    "        health_scores = [stats[m]['health_score'] for m in models]\n",
    "        colors = [self.colors['success'] if s > 75 else \n",
    "                 self.colors['warning'] if s > 50 else \n",
    "                 self.colors['danger'] for s in health_scores]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=models,\n",
    "                y=health_scores,\n",
    "                marker_color=colors,\n",
    "                text=[f'{s:.1f}%' for s in health_scores],\n",
    "                textposition='outside',\n",
    "                name='Health'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Request Limits - Gauge\n",
    "        model = models[0]\n",
    "        request_util = stats[model]['request_limiter']['utilization']\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number+delta\",\n",
    "                value=request_util,\n",
    "                title={'text': f\"{model} Requests\"},\n",
    "                delta={'reference': 50},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': self._get_gauge_color(request_util)},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 50], 'color': self.colors['light']},\n",
    "                        {'range': [50, 80], 'color': '#ffeb3b'},\n",
    "                        {'range': [80, 100], 'color': '#ffcdd2'}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 4},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 90\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Token Limits - Gauge with animation\n",
    "        token_util = (1 - stats[model]['token_limiter']['tokens'] / \n",
    "                     stats[model]['token_limiter']['capacity']) * 100\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=token_util,\n",
    "                title={'text': f\"{model} Tokens\"},\n",
    "                number={'suffix': \"% used\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': self._get_gauge_color(token_util)},\n",
    "                    'bgcolor': \"white\",\n",
    "                    'borderwidth': 2,\n",
    "                    'bordercolor': \"gray\",\n",
    "                    'steps': [\n",
    "                        {'range': [0, 50], 'color': 'lightgray'},\n",
    "                        {'range': [50, 80], 'color': 'gray'}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 4},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 95\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # 4. Adaptive Rate Changes Over Time\n",
    "        if self.live_metrics['adaptive_rates']:\n",
    "            times = list(range(len(self.live_metrics['adaptive_rates'])))\n",
    "            for m in models:\n",
    "                rates = [r.get(m, 0) for r in self.live_metrics['adaptive_rates']]\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=times[-50:],  # Last 50 points\n",
    "                        y=rates[-50:],\n",
    "                        mode='lines+markers',\n",
    "                        name=m,\n",
    "                        line=dict(width=2)\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "        \n",
    "        # 5. Circuit Breaker States - Pie chart\n",
    "        breaker_states = {'Open': 0, 'Closed': 0, 'Half-Open': 0}\n",
    "        for m in models:\n",
    "            state = stats[m]['circuit_breaker']\n",
    "            breaker_states[state.title()] = breaker_states.get(state.title(), 0) + 1\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=list(breaker_states.keys()),\n",
    "                values=list(breaker_states.values()),\n",
    "                hole=.4,\n",
    "                marker_colors=[self.colors['danger'], self.colors['success'], self.colors['warning']],\n",
    "                textinfo='label+percent'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # 6. Success Rates Comparison\n",
    "        success_rates = [stats[m]['success_rate'] * 100 for m in models]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=models,\n",
    "                y=success_rates,\n",
    "                marker_color=[self.colors['success'] if r > 95 else \n",
    "                             self.colors['warning'] if r > 90 else \n",
    "                             self.colors['danger'] for r in success_rates],\n",
    "                text=[f'{r:.1f}%' for r in success_rates],\n",
    "                textposition='outside',\n",
    "                name='Success Rate'\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "        \n",
    "        # 7. Latency Distribution\n",
    "        if self.rate_limiter.request_history:\n",
    "            recent_requests = list(self.rate_limiter.request_history)[-100:]\n",
    "            for m in models:\n",
    "                model_latencies = [r['latency'] for r in recent_requests if r['model'] == m]\n",
    "                if model_latencies:\n",
    "                    fig.add_trace(\n",
    "                        go.Box(\n",
    "                            y=model_latencies,\n",
    "                            name=m,\n",
    "                            boxmean='sd',\n",
    "                            marker_color=self.colors['info']\n",
    "                        ),\n",
    "                        row=3, col=1\n",
    "                    )\n",
    "        \n",
    "        # 8. Usage Prediction\n",
    "        for i, m in enumerate(models):\n",
    "            prediction = self.rate_limiter.predict_usage(m, hours_ahead=1)\n",
    "            if prediction['predicted_load'] != 'insufficient_data':\n",
    "                x = list(range(24))\n",
    "                y = [random.uniform(100, 500) for _ in range(24)]  # Simulated hourly pattern\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        mode='lines',\n",
    "                        name=m,\n",
    "                        fill='tozeroy',\n",
    "                        line=dict(width=2)\n",
    "                    ),\n",
    "                    row=3, col=2\n",
    "                )\n",
    "        \n",
    "        # 9. Performance Leaderboard\n",
    "        leaderboard_data = []\n",
    "        for m in models:\n",
    "            leaderboard_data.append([\n",
    "                m,\n",
    "                f\"{stats[m]['health_score']:.1f}%\",\n",
    "                f\"{stats[m]['success_rate']*100:.1f}%\",\n",
    "                f\"{stats[m]['avg_latency']:.2f}s\",\n",
    "                f\"{stats[m]['total_requests']:,}\"\n",
    "            ])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Model', 'Health', 'Success', 'Latency', 'Requests'],\n",
    "                    fill_color=self.colors['dark'],\n",
    "                    font_color='white',\n",
    "                    align='center'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=list(zip(*leaderboard_data)),\n",
    "                    fill_color='lavender',\n",
    "                    align='center'\n",
    "                )\n",
    "            ),\n",
    "            row=3, col=3\n",
    "        )\n",
    "        \n",
    "        # Update layout for beautiful appearance\n",
    "        fig.update_layout(\n",
    "            height=900,\n",
    "            showlegend=False,\n",
    "            title_text=\"<b>🚀 Ultimate Rate Limit Monitor</b>\",\n",
    "            title_font_size=24,\n",
    "            title_x=0.5,\n",
    "            title_xanchor='center',\n",
    "            paper_bgcolor=self.colors['light'],\n",
    "            plot_bgcolor='white',\n",
    "            font=dict(size=11)\n",
    "        )\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_gauge_color(self, value):\n",
    "        \"\"\"Get color based on value\"\"\"\n",
    "        if value < 50:\n",
    "            return self.colors['success']\n",
    "        elif value < 80:\n",
    "            return self.colors['warning']\n",
    "        else:\n",
    "            return self.colors['danger']\n",
    "    \n",
    "    def run_stress_test(self, model: str, scenario: str, duration: int):\n",
    "        \"\"\"Run different stress test scenarios\"\"\"\n",
    "        \n",
    "        scenarios = {\n",
    "            'steady': {'rate': 10, 'pattern': 'constant', 'tokens': (50, 100)},\n",
    "            'burst': {'rate': 50, 'pattern': 'burst', 'tokens': (100, 500)},\n",
    "            'wave': {'rate': 20, 'pattern': 'wave', 'tokens': (50, 200)},\n",
    "            'chaos': {'rate': 30, 'pattern': 'random', 'tokens': (10, 1000)},\n",
    "            'ddos': {'rate': 100, 'pattern': 'attack', 'tokens': (10, 50)}\n",
    "        }\n",
    "        \n",
    "        config = scenarios.get(scenario, scenarios['steady'])\n",
    "        \n",
    "        print(f\"\\n🔥 Starting {scenario.upper()} stress test on {model}\")\n",
    "        print(f\"   Pattern: {config['pattern']}\")\n",
    "        print(f\"   Base Rate: {config['rate']} req/s\")\n",
    "        print(f\"   Duration: {duration}s\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        results = {\n",
    "            'accepted': 0,\n",
    "            'rejected': 0,\n",
    "            'by_reason': defaultdict(int)\n",
    "        }\n",
    "        \n",
    "        self.test_running = True\n",
    "        last_print_time = time.time()\n",
    "        \n",
    "        # Progress bar for better visibility\n",
    "        progress_interval = duration / 20  # Update 20 times during test\n",
    "        next_progress = progress_interval\n",
    "        \n",
    "        while time.time() - start_time < duration and self.test_running:\n",
    "            # Calculate current rate based on pattern\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if config['pattern'] == 'burst':\n",
    "                # Burst every 5 seconds\n",
    "                current_rate = config['rate'] * 3 if int(elapsed) % 5 == 0 else config['rate'] / 2\n",
    "            elif config['pattern'] == 'wave':\n",
    "                # Sine wave pattern\n",
    "                import math\n",
    "                current_rate = config['rate'] * (1 + 0.5 * math.sin(elapsed))\n",
    "            elif config['pattern'] == 'random':\n",
    "                current_rate = random.uniform(1, config['rate'] * 2)\n",
    "            else:\n",
    "                current_rate = config['rate']\n",
    "            \n",
    "            # Generate request\n",
    "            tokens = random.randint(*config['tokens'])\n",
    "            check = self.rate_limiter.check_limits(model, tokens)\n",
    "            \n",
    "            if check['allowed']:\n",
    "                results['accepted'] += 1\n",
    "                # Simulate processing\n",
    "                latency = random.uniform(0.1, 1.0)\n",
    "                success = random.random() > 0.05  # 95% success rate\n",
    "                self.rate_limiter.record_usage(model, tokens, latency, success)\n",
    "            else:\n",
    "                results['rejected'] += 1\n",
    "                results['by_reason'][check['reason']] += 1\n",
    "            \n",
    "            # Update live metrics\n",
    "            stats = self.rate_limiter.get_comprehensive_stats()\n",
    "            self.live_metrics['adaptive_rates'].append({\n",
    "                m: stats[m]['adaptive_limiter']['current_rate'] for m in stats\n",
    "            })\n",
    "            \n",
    "            # Print progress updates (better for Jupyter)\n",
    "            if elapsed >= next_progress or time.time() - last_print_time > 1:\n",
    "                progress = (elapsed / duration) * 100\n",
    "                total = results['accepted'] + results['rejected']\n",
    "                accept_rate = (results['accepted'] / total * 100) if total > 0 else 0\n",
    "                \n",
    "                # Create progress bar\n",
    "                bar_length = 30\n",
    "                filled = int(bar_length * progress / 100)\n",
    "                bar = '█' * filled + '░' * (bar_length - filled)\n",
    "                \n",
    "                print(f\"Progress: {bar} {progress:.0f}% | \"\n",
    "                      f\"✅ {results['accepted']} | ❌ {results['rejected']} | \"\n",
    "                      f\"Rate: {accept_rate:.1f}% accepted\")\n",
    "                \n",
    "                next_progress += progress_interval\n",
    "                last_print_time = time.time()\n",
    "            \n",
    "            # Sleep to control rate\n",
    "            sleep_time = 1 / current_rate\n",
    "            time.sleep(max(0.01, sleep_time))\n",
    "        \n",
    "        self.test_running = False\n",
    "        \n",
    "        # Print results\n",
    "        total = results['accepted'] + results['rejected']\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"📊 STRESS TEST COMPLETE - {scenario.upper()}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Total Requests: {total:,}\")\n",
    "        print(f\"Accepted: {results['accepted']:,} ({results['accepted']/total*100:.1f}%)\")\n",
    "        print(f\"Rejected: {results['rejected']:,} ({results['rejected']/total*100:.1f}%)\")\n",
    "        \n",
    "        if results['by_reason']:\n",
    "            print(f\"\\nRejection Breakdown:\")\n",
    "            for reason, count in results['by_reason'].items():\n",
    "                percentage = count/results['rejected']*100 if results['rejected'] > 0 else 0\n",
    "                print(f\"  • {reason}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show final stats\n",
    "        final_stats = self.rate_limiter.get_comprehensive_stats()[model]\n",
    "        print(f\"\\nFinal Model Stats:\")\n",
    "        print(f\"  • Health Score: {final_stats['health_score']:.1f}%\")\n",
    "        print(f\"  • Circuit Breaker: {final_stats['circuit_breaker']}\")\n",
    "        print(f\"  • Adaptive Rate: {final_stats['adaptive_limiter']['current_rate']}\")\n",
    "        print(f\"  • Token Utilization: {final_stats['token_limiter']['tokens']}/{final_stats['token_limiter']['capacity']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create the ultimate dashboard\n",
    "dashboard = UltimateRateLimitDashboard(rate_limiter)\n",
    "\n",
    "# Create interactive controls\n",
    "print(\"🎮 Ultimate Rate Limit Control Center\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Model selector\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['gpt-4o', 'gpt-4o-mini', 'gpt-3.5-turbo'],\n",
    "    value='gpt-4o-mini',\n",
    "    description='Model:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Scenario selector with descriptions\n",
    "scenario_selector = widgets.RadioButtons(\n",
    "    options=[\n",
    "        ('💚 Steady - Constant load', 'steady'),\n",
    "        ('💥 Burst - Sudden spikes', 'burst'),\n",
    "        ('🌊 Wave - Oscillating pattern', 'wave'),\n",
    "        ('🎲 Chaos - Random mayhem', 'chaos'),\n",
    "        ('☠️ DDoS - Attack simulation', 'ddos')\n",
    "    ],\n",
    "    value='steady',\n",
    "    description='Scenario:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Duration slider\n",
    "duration_slider = widgets.IntSlider(\n",
    "    value=15,\n",
    "    min=5,\n",
    "    max=60,\n",
    "    step=5,\n",
    "    description='Duration (s):',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Control buttons\n",
    "start_button = widgets.Button(\n",
    "    description='🚀 Start Test',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='🛑 Stop Test',\n",
    "    button_style='danger',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description='🔄 Refresh Dashboard',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "reset_button = widgets.Button(\n",
    "    description='🗑️ Reset All',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "test_output = widgets.Output()\n",
    "dashboard_output = widgets.Output()\n",
    "\n",
    "# Button handlers\n",
    "def start_test(b):\n",
    "    with test_output:\n",
    "        clear_output()\n",
    "        if not dashboard.test_running:\n",
    "            # Run synchronously for better visibility\n",
    "            dashboard.run_stress_test(\n",
    "                model_selector.value, \n",
    "                scenario_selector.value, \n",
    "                duration_slider.value\n",
    "            )\n",
    "\n",
    "def stop_test(b):\n",
    "    dashboard.test_running = False\n",
    "    with test_output:\n",
    "        print(\"\\n⏹️ Stopping test...\")\n",
    "        time.sleep(0.5)\n",
    "        print(\"Test stopped by user\")\n",
    "\n",
    "def refresh_dashboard(b):\n",
    "    with dashboard_output:\n",
    "        clear_output()\n",
    "        fig = dashboard.create_live_dashboard()\n",
    "        fig.show()\n",
    "\n",
    "def reset_all(b):\n",
    "    with test_output:\n",
    "        clear_output()\n",
    "        # Reset metrics\n",
    "        rate_limiter.metrics.clear()\n",
    "        rate_limiter.request_history.clear()\n",
    "        dashboard.live_metrics.clear()\n",
    "        print(\"♻️ All metrics reset!\")\n",
    "    refresh_dashboard(b)\n",
    "\n",
    "# Connect handlers\n",
    "start_button.on_click(start_test)\n",
    "stop_button.on_click(stop_test)\n",
    "refresh_button.on_click(refresh_dashboard)\n",
    "reset_button.on_click(reset_all)\n",
    "\n",
    "# Quick Test Function for immediate feedback\n",
    "def quick_test():\n",
    "    \"\"\"Run a quick test to show the system working\"\"\"\n",
    "    print(\"🚀 Running Quick Rate Limit Test...\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    model = 'gpt-4o-mini'\n",
    "    test_requests = 20\n",
    "    \n",
    "    for i in range(test_requests):\n",
    "        tokens = random.randint(100, 500)\n",
    "        check = rate_limiter.check_limits(model, tokens)\n",
    "        \n",
    "        if check['allowed']:\n",
    "            print(f\"✅ Request {i+1}: ALLOWED ({tokens} tokens)\")\n",
    "            rate_limiter.record_usage(model, tokens, random.uniform(0.1, 0.5), True)\n",
    "        else:\n",
    "            print(f\"❌ Request {i+1}: BLOCKED - {check['reason']} (wait {check['wait_time']:.1f}s)\")\n",
    "        \n",
    "        time.sleep(0.1)  # Small delay between requests\n",
    "    \n",
    "    # Show stats\n",
    "    stats = rate_limiter.get_comprehensive_stats()[model]\n",
    "    print(\"\\n📊 Quick Test Results:\")\n",
    "    print(f\"  Health Score: {stats['health_score']:.1f}%\")\n",
    "    print(f\"  Token Usage: {stats['token_limiter']['tokens']:.0f}/{stats['token_limiter']['capacity']}\")\n",
    "    print(f\"  Request Count: {stats['request_limiter']['current_requests']}/{stats['request_limiter']['max_requests']}\")\n",
    "\n",
    "# Add quick test button\n",
    "quick_test_button = widgets.Button(\n",
    "    description='⚡ Quick Test',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "def run_quick_test(b):\n",
    "    with test_output:\n",
    "        clear_output()\n",
    "        quick_test()\n",
    "\n",
    "quick_test_button.on_click(run_quick_test)\n",
    "\n",
    "# Create enhanced layout\n",
    "control_panel = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>🎛️ Control Panel</h2>\"),\n",
    "    widgets.HBox([\n",
    "        widgets.VBox([model_selector, duration_slider]),\n",
    "        scenario_selector\n",
    "    ]),\n",
    "    widgets.HBox([start_button, stop_button, refresh_button, reset_button, quick_test_button]),\n",
    "    test_output\n",
    "])\n",
    "\n",
    "# Display everything\n",
    "display(control_panel)\n",
    "display(dashboard_output)\n",
    "\n",
    "# Show initial dashboard\n",
    "with dashboard_output:\n",
    "    fig = dashboard.create_live_dashboard()\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n💡 Pro Tips:\")\n",
    "print(\"  🎯 Start with 'Steady' to baseline performance\")\n",
    "print(\"  💥 Try 'Burst' to see adaptive rate limiting in action\")\n",
    "print(\"  🌊 'Wave' pattern shows how the system handles oscillating load\")\n",
    "print(\"  🎲 'Chaos' tests unpredictable patterns\")\n",
    "print(\"  ☠️ 'DDoS' triggers circuit breakers and protection mechanisms\")\n",
    "print(\"\\n🔄 Refresh the dashboard during tests to see live updates!\")\n",
    "print(\"📈 Watch how different algorithms work together to maintain stability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔌 **Module 4: Circuit Breakers & Self-Protection**\n",
    "### Build Systems That Protect Themselves!\n",
    "\n",
    "```\n",
    "Circuit Breaker States:\n",
    "┌──────┐  Success   ┌────────┐  Timeout  ┌──────┐\n",
    "│CLOSED│ ────────► │HALF-OPEN│ ◄──────── │ OPEN │\n",
    "└──────┘  Failure   └────────┘  Success  └──────┘\n",
    "     │                                        ▲\n",
    "     └────────────Threshold Exceeded──────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resilient_system' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 530\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfailure_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# Create the demo system\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m demo \u001b[38;5;241m=\u001b[39m CircuitBreakerLiveDemo(\u001b[43mresilient_system\u001b[49m)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Create interactive controls\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎮 Circuit Breaker Interactive Demo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resilient_system' is not defined"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 9: Interactive Circuit Breaker Live Demo & Visualization\n",
    "# Experience circuit breakers in action with real-time visual feedback!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import queue\n",
    "\n",
    "class CircuitBreakerLiveDemo:\n",
    "    \"\"\"Interactive circuit breaker demonstration with live visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, resilient_system):\n",
    "        self.system = resilient_system\n",
    "        self.demo_running = False\n",
    "        self.demo_thread = None\n",
    "        self.event_queue = queue.Queue(maxsize=1000)\n",
    "        self.visualization_thread = None\n",
    "        self.visualization_running = False\n",
    "        \n",
    "        # Live data storage\n",
    "        self.live_data = {\n",
    "            'timestamps': deque(maxlen=100),\n",
    "            'states': {model: deque(maxlen=100) for model in self.system.breakers},\n",
    "            'success_rates': {model: deque(maxlen=100) for model in self.system.breakers},\n",
    "            'latencies': {model: deque(maxlen=100) for model in self.system.breakers},\n",
    "            'events': deque(maxlen=50)\n",
    "        }\n",
    "        \n",
    "        # State colors for visualization\n",
    "        self.state_colors = {\n",
    "            'closed': '#00d084',      # Green\n",
    "            'open': '#ff3860',        # Red\n",
    "            'half_open': '#ffdd57',   # Yellow\n",
    "            'degraded': '#ff9800',    # Orange\n",
    "            'forced_open': '#9c27b0'  # Purple\n",
    "        }\n",
    "        \n",
    "        # Demo scenarios\n",
    "        self.scenarios = {\n",
    "            'normal': {\n",
    "                'name': '😊 Normal Operation',\n",
    "                'description': 'Everything works perfectly',\n",
    "                'failure_rate': 0.0,\n",
    "                'latency_range': (0.5, 1.5),\n",
    "                'pattern': 'steady'\n",
    "            },\n",
    "            'degrading': {\n",
    "                'name': '📉 Gradual Degradation',\n",
    "                'description': 'Service slowly degrades over time',\n",
    "                'failure_rate': 0.1,\n",
    "                'latency_range': (1.0, 3.0),\n",
    "                'pattern': 'increasing'\n",
    "            },\n",
    "            'cascade': {\n",
    "                'name': '💥 Cascade Failure',\n",
    "                'description': 'One failure triggers others',\n",
    "                'failure_rate': 0.3,\n",
    "                'latency_range': (2.0, 5.0),\n",
    "                'pattern': 'cascade'\n",
    "            },\n",
    "            'recovery': {\n",
    "                'name': '🔄 Failure & Recovery',\n",
    "                'description': 'Service fails then recovers',\n",
    "                'failure_rate': 0.5,\n",
    "                'latency_range': (1.0, 10.0),\n",
    "                'pattern': 'wave'\n",
    "            },\n",
    "            'chaos': {\n",
    "                'name': '🎲 Chaos Monkey',\n",
    "                'description': 'Random failures everywhere!',\n",
    "                'failure_rate': 0.7,\n",
    "                'latency_range': (0.1, 15.0),\n",
    "                'pattern': 'random'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def create_live_visualization(self):\n",
    "        \"\"\"Create comprehensive live visualization dashboard\"\"\"\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=3,\n",
    "            subplot_titles=(\n",
    "                '🎭 Circuit States', '📈 Success Rates', '⏱️ Latency Trends',\n",
    "                '🎯 Current Status', '📊 Failure Breakdown', '🔄 State Transitions',\n",
    "                '📝 Live Event Log', '💪 System Health', '🏆 Model Performance'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "                [{'type': 'indicator'}, {'type': 'pie'}, {'type': 'bar'}],\n",
    "                [{'type': 'table', 'rowspan': 1}, {'type': 'indicator'}, {'type': 'bar'}]\n",
    "            ],\n",
    "            vertical_spacing=0.12,\n",
    "            horizontal_spacing=0.12,\n",
    "            row_heights=[0.35, 0.35, 0.3]\n",
    "        )\n",
    "        \n",
    "        models = list(self.system.breakers.keys())\n",
    "        current_time = datetime.now()\n",
    "        \n",
    "        # 1. Circuit States Timeline (Visual state representation)\n",
    "        for model in models:\n",
    "            if self.live_data['timestamps']:\n",
    "                states = self.live_data['states'].get(model, [])\n",
    "                if states:\n",
    "                    # Convert states to numeric values for plotting\n",
    "                    state_values = []\n",
    "                    colors = []\n",
    "                    for state in states:\n",
    "                        if state == 'closed':\n",
    "                            state_values.append(3)\n",
    "                            colors.append(self.state_colors['closed'])\n",
    "                        elif state == 'degraded':\n",
    "                            state_values.append(2)\n",
    "                            colors.append(self.state_colors['degraded'])\n",
    "                        elif state == 'half_open':\n",
    "                            state_values.append(1)\n",
    "                            colors.append(self.state_colors['half_open'])\n",
    "                        else:  # open or forced_open\n",
    "                            state_values.append(0)\n",
    "                            colors.append(self.state_colors['open'])\n",
    "                    \n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=list(self.live_data['timestamps']),\n",
    "                            y=state_values,\n",
    "                            mode='lines+markers',\n",
    "                            name=model,\n",
    "                            line=dict(width=3),\n",
    "                            marker=dict(size=8, color=colors[-1] if colors else 'gray'),\n",
    "                            hovertemplate='%{y}<extra></extra>'\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "        \n",
    "        # 2. Success Rates Over Time\n",
    "        for model in models:\n",
    "            if self.live_data['timestamps'] and model in self.live_data['success_rates']:\n",
    "                rates = list(self.live_data['success_rates'][model])\n",
    "                if rates:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=list(self.live_data['timestamps']),\n",
    "                            y=[r * 100 for r in rates],\n",
    "                            mode='lines',\n",
    "                            name=model,\n",
    "                            line=dict(width=2),\n",
    "                            fill='tozeroy',\n",
    "                            opacity=0.7\n",
    "                        ),\n",
    "                        row=1, col=2\n",
    "                    )\n",
    "        \n",
    "        # 3. Latency Trends\n",
    "        for model in models:\n",
    "            if self.live_data['timestamps'] and model in self.live_data['latencies']:\n",
    "                latencies = list(self.live_data['latencies'][model])\n",
    "                if latencies:\n",
    "                    fig.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=list(self.live_data['timestamps']),\n",
    "                            y=latencies,\n",
    "                            mode='lines+markers',\n",
    "                            name=model,\n",
    "                            line=dict(width=2)\n",
    "                        ),\n",
    "                        row=1, col=3\n",
    "                    )\n",
    "        \n",
    "        # 4. Current Status Indicator (for primary model)\n",
    "        primary_model = models[0]\n",
    "        breaker = self.system.breakers[primary_model]\n",
    "        status = breaker.get_detailed_status()\n",
    "        \n",
    "        # Create a compound indicator showing multiple metrics\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"number+gauge+delta\",\n",
    "                value=status['metrics']['total_calls'],\n",
    "                title={'text': f\"{primary_model}<br>Total Calls\"},\n",
    "                delta={'reference': 100, 'position': \"top\"},\n",
    "                gauge={\n",
    "                    'shape': \"bullet\",\n",
    "                    'axis': {'range': [None, 1000]},\n",
    "                    'bar': {'color': self.state_colors.get(status['state'], 'gray')},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 250], 'color': \"lightgray\"},\n",
    "                        {'range': [250, 750], 'color': \"gray\"}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 2},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 900\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 5. Failure Breakdown Pie Chart\n",
    "        failure_data = {}\n",
    "        for model in models:\n",
    "            breaker = self.system.breakers[model]\n",
    "            for failure_type, count in breaker.metrics.failures_by_type.items():\n",
    "                failure_data[failure_type.name] = failure_data.get(failure_type.name, 0) + count\n",
    "        \n",
    "        if failure_data:\n",
    "            fig.add_trace(\n",
    "                go.Pie(\n",
    "                    labels=list(failure_data.keys()),\n",
    "                    values=list(failure_data.values()),\n",
    "                    hole=0.4,\n",
    "                    marker=dict(colors=['#ff6b6b', '#4ecdc4', '#45b7d1', '#fdcb6e', '#6c5ce7']),\n",
    "                    textinfo='label+percent'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # 6. State Transition History\n",
    "        transition_data = {'From': [], 'To': [], 'Count': []}\n",
    "        for model in models:\n",
    "            breaker = self.system.breakers[model]\n",
    "            transitions = {}\n",
    "            for i in range(1, len(breaker.state_history)):\n",
    "                from_state = breaker.state_history[i-1]['from']\n",
    "                to_state = breaker.state_history[i]['to']\n",
    "                key = f\"{from_state}→{to_state}\"\n",
    "                transitions[key] = transitions.get(key, 0) + 1\n",
    "            \n",
    "            for key, count in transitions.items():\n",
    "                states = key.split('→')\n",
    "                if len(states) == 2:\n",
    "                    transition_data['From'].append(states[0])\n",
    "                    transition_data['To'].append(states[1])\n",
    "                    transition_data['Count'].append(count)\n",
    "        \n",
    "        if transition_data['Count']:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[f\"{f}→{t}\" for f, t in zip(transition_data['From'], transition_data['To'])],\n",
    "                    y=transition_data['Count'],\n",
    "                    marker_color='#3498db'\n",
    "                ),\n",
    "                row=2, col=3\n",
    "            )\n",
    "        \n",
    "        # 7. Live Event Log\n",
    "        events_data = []\n",
    "        for event in list(self.live_data['events'])[-10:]:  # Last 10 events\n",
    "            events_data.append([\n",
    "                event.get('time', '').strftime('%H:%M:%S') if isinstance(event.get('time'), datetime) else '',\n",
    "                event.get('model', ''),\n",
    "                event.get('event', ''),\n",
    "                event.get('details', '')\n",
    "            ])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Time', 'Model', 'Event', 'Details'],\n",
    "                    fill_color='#34495e',\n",
    "                    font_color='white',\n",
    "                    align='left',\n",
    "                    height=25\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=list(zip(*events_data)) if events_data else [[], [], [], []],\n",
    "                    fill_color='#ecf0f1',\n",
    "                    align='left',\n",
    "                    height=20,\n",
    "                    font_size=10\n",
    "                )\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # 8. System Health Score\n",
    "        health_scores = []\n",
    "        for model in models:\n",
    "            breaker = self.system.breakers[model]\n",
    "            metrics = breaker.metrics\n",
    "            health = (metrics.success_rate * 0.5 + \n",
    "                     (1 - metrics.slow_call_rate) * 0.3 +\n",
    "                     (1 - len(breaker.sliding_window) / breaker.config.sliding_window_size) * 0.2)\n",
    "            health_scores.append(health * 100)\n",
    "        \n",
    "        avg_health = sum(health_scores) / len(health_scores) if health_scores else 0\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=avg_health,\n",
    "                title={'text': \"System Health\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': self._get_health_color(avg_health)},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 50], 'color': \"#ffebee\"},\n",
    "                        {'range': [50, 80], 'color': \"#fff3e0\"},\n",
    "                        {'range': [80, 100], 'color': \"#e8f5e9\"}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 4},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 30\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=3, col=2\n",
    "        )\n",
    "        \n",
    "        # 9. Model Performance Comparison\n",
    "        perf_data = []\n",
    "        for model in models:\n",
    "            breaker = self.system.breakers[model]\n",
    "            perf_data.append({\n",
    "                'model': model,\n",
    "                'success': breaker.metrics.success_rate * 100,\n",
    "                'speed': 100 - min(breaker.metrics.average_duration * 10, 100)  # Convert to speed score\n",
    "            })\n",
    "        \n",
    "        if perf_data:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[d['model'] for d in perf_data],\n",
    "                    y=[d['success'] for d in perf_data],\n",
    "                    name='Success Rate',\n",
    "                    marker_color='#2ecc71'\n",
    "                ),\n",
    "                row=3, col=3\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=[d['model'] for d in perf_data],\n",
    "                    y=[d['speed'] for d in perf_data],\n",
    "                    name='Speed Score',\n",
    "                    marker_color='#3498db'\n",
    "                ),\n",
    "                row=3, col=3\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=900,\n",
    "            showlegend=True,\n",
    "            title_text=\"<b>🚀 Circuit Breaker Live Monitor</b>\",\n",
    "            title_font_size=24,\n",
    "            title_x=0.5,\n",
    "            title_xanchor='center',\n",
    "            plot_bgcolor='white',\n",
    "            paper_bgcolor='#f8f9fa',\n",
    "            font=dict(size=10),\n",
    "            margin=dict(l=50, r=50, t=80, b=50)\n",
    "        )\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_xaxes(showgrid=True, gridwidth=0.5, gridcolor='#e0e0e0')\n",
    "        fig.update_yaxes(showgrid=True, gridwidth=0.5, gridcolor='#e0e0e0')\n",
    "        \n",
    "        # Custom y-axis for state chart\n",
    "        fig.update_yaxes(\n",
    "            ticktext=['Open', 'Half-Open', 'Degraded', 'Closed'],\n",
    "            tickvals=[0, 1, 2, 3],\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_health_color(self, health):\n",
    "        \"\"\"Get color based on health score\"\"\"\n",
    "        if health > 80:\n",
    "            return '#2ecc71'\n",
    "        elif health > 60:\n",
    "            return '#f39c12'\n",
    "        elif health > 40:\n",
    "            return '#e67e22'\n",
    "        else:\n",
    "            return '#e74c3c'\n",
    "    \n",
    "    def simulate_scenario(self, scenario_key, duration=30):\n",
    "        \"\"\"Run a demo scenario\"\"\"\n",
    "        scenario = self.scenarios[scenario_key]\n",
    "        self.demo_running = True\n",
    "        \n",
    "        print(f\"\\n🎬 Starting Demo: {scenario['name']}\")\n",
    "        print(f\"   {scenario['description']}\")\n",
    "        print(f\"   Duration: {duration} seconds\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        request_count = 0\n",
    "        \n",
    "        # Add initial event\n",
    "        self.add_event('all', 'Demo Started', f\"Running {scenario['name']}\")\n",
    "        \n",
    "        while time.time() - start_time < duration and self.demo_running:\n",
    "            elapsed = time.time() - start_time\n",
    "            progress = elapsed / duration\n",
    "            \n",
    "            # Select model based on scenario pattern\n",
    "            if scenario['pattern'] == 'cascade':\n",
    "                # Start with one model, then cascade to others\n",
    "                if progress < 0.3:\n",
    "                    model = 'gpt-4o'\n",
    "                elif progress < 0.6:\n",
    "                    model = random.choice(['gpt-4o', 'gpt-4o-mini'])\n",
    "                else:\n",
    "                    model = random.choice(list(self.system.breakers.keys()))\n",
    "            else:\n",
    "                model = random.choice(list(self.system.breakers.keys()))\n",
    "            \n",
    "            # Adjust failure rate based on pattern\n",
    "            if scenario['pattern'] == 'increasing':\n",
    "                current_failure_rate = scenario['failure_rate'] * progress\n",
    "            elif scenario['pattern'] == 'wave':\n",
    "                import math\n",
    "                current_failure_rate = scenario['failure_rate'] * (0.5 + 0.5 * math.sin(elapsed))\n",
    "            else:\n",
    "                current_failure_rate = scenario['failure_rate']\n",
    "            \n",
    "            # Simulate request\n",
    "            request_count += 1\n",
    "            should_fail = random.random() < current_failure_rate\n",
    "            latency = random.uniform(*scenario['latency_range'])\n",
    "            \n",
    "            if should_fail:\n",
    "                latency = latency * 2  # Failed requests take longer\n",
    "            \n",
    "            # Simulate the call through the circuit breaker\n",
    "            breaker = self.system.breakers[model]\n",
    "            \n",
    "            try:\n",
    "                if should_fail:\n",
    "                    # Inject a failure\n",
    "                    def failing_call():\n",
    "                        time.sleep(latency)\n",
    "                        raise Exception(\"Simulated failure\")\n",
    "                    \n",
    "                    breaker.call(failing_call)\n",
    "                else:\n",
    "                    # Successful call\n",
    "                    def successful_call():\n",
    "                        time.sleep(latency)\n",
    "                        return \"Success\"\n",
    "                    \n",
    "                    breaker.call(successful_call)\n",
    "                    \n",
    "                # Record success\n",
    "                self.add_event(model, 'Success', f\"Latency: {latency:.2f}s\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Record failure\n",
    "                failure_type = breaker._classify_failure(e)\n",
    "                self.add_event(model, 'Failed', f\"{failure_type.name}\")\n",
    "            \n",
    "            # Update live data\n",
    "            self.update_live_data()\n",
    "            \n",
    "            # Print status\n",
    "            if request_count % 10 == 0:\n",
    "                print(f\"📊 Requests: {request_count} | Time: {elapsed:.1f}s\", end='\\r')\n",
    "            \n",
    "            # Control request rate\n",
    "            time.sleep(random.uniform(0.1, 0.5))\n",
    "        \n",
    "        self.demo_running = False\n",
    "        \n",
    "        print(f\"\\n\\n✅ Demo Complete!\")\n",
    "        print(f\"   Total Requests: {request_count}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Final status\n",
    "        self.print_final_status()\n",
    "    \n",
    "    def add_event(self, model, event_type, details):\n",
    "        \"\"\"Add event to the event log\"\"\"\n",
    "        event = {\n",
    "            'time': datetime.now(),\n",
    "            'model': model,\n",
    "            'event': event_type,\n",
    "            'details': details\n",
    "        }\n",
    "        self.live_data['events'].append(event)\n",
    "        \n",
    "        # Also add to queue for processing\n",
    "        try:\n",
    "            self.event_queue.put_nowait(event)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def update_live_data(self):\n",
    "        \"\"\"Update live data for visualization\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        self.live_data['timestamps'].append(timestamp)\n",
    "        \n",
    "        for model, breaker in self.system.breakers.items():\n",
    "            # State\n",
    "            self.live_data['states'][model].append(breaker.state.value)\n",
    "            \n",
    "            # Success rate\n",
    "            self.live_data['success_rates'][model].append(breaker.metrics.success_rate)\n",
    "            \n",
    "            # Average latency\n",
    "            self.live_data['latencies'][model].append(breaker.metrics.average_duration)\n",
    "    \n",
    "    def print_final_status(self):\n",
    "        \"\"\"Print detailed final status\"\"\"\n",
    "        print(\"\\n📊 Final Circuit Breaker Status:\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for model, breaker in self.system.breakers.items():\n",
    "            status = breaker.get_detailed_status()\n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  State: {status['state'].upper()}\")\n",
    "            print(f\"  Total Calls: {status['metrics']['total_calls']}\")\n",
    "            print(f\"  Success Rate: {status['metrics']['success_rate']}\")\n",
    "            print(f\"  Avg Duration: {status['metrics']['avg_duration']}\")\n",
    "            \n",
    "            if status['failures_by_type']:\n",
    "                print(f\"  Failures:\")\n",
    "                for failure_type, count in status['failures_by_type'].items():\n",
    "                    print(f\"    - {failure_type}: {count}\")\n",
    "\n",
    "# Create the demo system\n",
    "demo = CircuitBreakerLiveDemo(resilient_system)\n",
    "\n",
    "# Create interactive controls\n",
    "print(\"🎮 Circuit Breaker Interactive Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario selector\n",
    "scenario_cards = []\n",
    "for key, scenario in demo.scenarios.items():\n",
    "    card_html = f\"\"\"\n",
    "    <div style='border: 2px solid #ddd; padding: 10px; margin: 5px; border-radius: 8px;'>\n",
    "        <h4>{scenario['name']}</h4>\n",
    "        <p style='color: #666;'>{scenario['description']}</p>\n",
    "        <small>Failure Rate: {scenario['failure_rate']*100:.0f}%</small>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    scenario_cards.append((card_html, key))\n",
    "\n",
    "scenario_selector = widgets.RadioButtons(\n",
    "    options=[(s[1], s[1]) for s in scenario_cards],\n",
    "    value='normal',\n",
    "    description='',\n",
    "    layout=widgets.Layout(width='600px')\n",
    ")\n",
    "\n",
    "# Duration slider\n",
    "duration_slider = widgets.IntSlider(\n",
    "    value=20,\n",
    "    min=10,\n",
    "    max=60,\n",
    "    step=5,\n",
    "    description='Duration:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Control buttons\n",
    "start_demo_button = widgets.Button(\n",
    "    description='🎬 Start Demo',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "stop_demo_button = widgets.Button(\n",
    "    description='⏹️ Stop Demo',\n",
    "    button_style='danger',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "refresh_viz_button = widgets.Button(\n",
    "    description='🔄 Refresh Viz',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "reset_system_button = widgets.Button(\n",
    "    description='♻️ Reset System',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='150px')\n",
    ")\n",
    "\n",
    "# Manual controls\n",
    "manual_controls = widgets.HBox([\n",
    "    widgets.Dropdown(\n",
    "        options=list(resilient_system.breakers.keys()),\n",
    "        description='Model:',\n",
    "        value='gpt-4o'\n",
    "    ),\n",
    "    widgets.Button(description='🔴 Force Open', button_style='danger', layout=widgets.Layout(width='120px')),\n",
    "    widgets.Button(description='🟢 Force Close', button_style='success', layout=widgets.Layout(width='120px')),\n",
    "    widgets.Button(description='🏥 Health Check', button_style='primary', layout=widgets.Layout(width='120px'))\n",
    "])\n",
    "\n",
    "# Output areas\n",
    "demo_output = widgets.Output()\n",
    "viz_output = widgets.Output()\n",
    "\n",
    "# Button handlers\n",
    "def start_demo(b):\n",
    "    if not demo.demo_running:\n",
    "        with demo_output:\n",
    "            clear_output()\n",
    "            demo.demo_thread = threading.Thread(\n",
    "                target=demo.simulate_scenario,\n",
    "                args=(scenario_selector.value, duration_slider.value)\n",
    "            )\n",
    "            demo.demo_thread.start()\n",
    "\n",
    "def stop_demo(b):\n",
    "    demo.demo_running = False\n",
    "    with demo_output:\n",
    "        print(\"\\n⏹️ Demo stopped by user\")\n",
    "\n",
    "def refresh_viz(b):\n",
    "    with viz_output:\n",
    "        clear_output()\n",
    "        fig = demo.create_live_visualization()\n",
    "        fig.show()\n",
    "\n",
    "def reset_system(b):\n",
    "    with demo_output:\n",
    "        clear_output()\n",
    "        for breaker in resilient_system.breakers.values():\n",
    "            breaker.reset()\n",
    "        demo.live_data = {\n",
    "            'timestamps': deque(maxlen=100),\n",
    "            'states': {model: deque(maxlen=100) for model in resilient_system.breakers},\n",
    "            'success_rates': {model: deque(maxlen=100) for model in resilient_system.breakers},\n",
    "            'latencies': {model: deque(maxlen=100) for model in resilient_system.breakers},\n",
    "            'events': deque(maxlen=50)\n",
    "        }\n",
    "        print(\"♻️ System reset complete!\")\n",
    "    refresh_viz(b)\n",
    "\n",
    "def force_open(b):\n",
    "    model = manual_controls.children[0].value\n",
    "    resilient_system.breakers[model].force_open(\"Manual test\")\n",
    "    demo.add_event(model, 'Forced Open', 'Manual intervention')\n",
    "    demo.update_live_data()\n",
    "    refresh_viz(b)\n",
    "\n",
    "def force_close(b):\n",
    "    model = manual_controls.children[0].value\n",
    "    resilient_system.breakers[model].force_close()\n",
    "    demo.add_event(model, 'Forced Close', 'Manual intervention')\n",
    "    demo.update_live_data()\n",
    "    refresh_viz(b)\n",
    "\n",
    "def health_check(b):\n",
    "    with demo_output:\n",
    "        clear_output()\n",
    "        resilient_system.run_health_check()\n",
    "\n",
    "# Connect handlers\n",
    "start_demo_button.on_click(start_demo)\n",
    "stop_demo_button.on_click(stop_demo)\n",
    "refresh_viz_button.on_click(refresh_viz)\n",
    "reset_system_button.on_click(reset_system)\n",
    "manual_controls.children[1].on_click(force_open)\n",
    "manual_controls.children[2].on_click(force_close)\n",
    "manual_controls.children[3].on_click(health_check)\n",
    "\n",
    "# Create the layout\n",
    "scenario_display = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>📚 Select a Scenario:</h3>\"),\n",
    "    widgets.HTML(f\"\"\"\n",
    "    <div style='display: flex; flex-wrap: wrap;'>\n",
    "        {''.join([s[0] for s in scenario_cards])}\n",
    "    </div>\n",
    "    \"\"\"),\n",
    "    scenario_selector,\n",
    "    duration_slider\n",
    "])\n",
    "\n",
    "control_panel = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>🎛️ Circuit Breaker Control Center</h2>\"),\n",
    "    scenario_display,\n",
    "    widgets.HBox([start_demo_button, stop_demo_button, refresh_viz_button, reset_system_button]),\n",
    "    widgets.HTML(\"<h3>🔧 Manual Controls:</h3>\"),\n",
    "    manual_controls,\n",
    "    demo_output\n",
    "])\n",
    "\n",
    "# Display everything\n",
    "display(control_panel)\n",
    "display(viz_output)\n",
    "\n",
    "# Show initial visualization\n",
    "with viz_output:\n",
    "    fig = demo.create_live_visualization()\n",
    "    fig.show()\n",
    "\n",
    "# Auto-refresh visualization during demos\n",
    "def auto_refresh_viz():\n",
    "    while True:\n",
    "        if demo.demo_running:\n",
    "            time.sleep(1)  # Update every second\n",
    "            with viz_output:\n",
    "                clear_output(wait=True)\n",
    "                fig = demo.create_live_visualization()\n",
    "                fig.show()\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "# Start auto-refresh in background\n",
    "auto_refresh_thread = threading.Thread(target=auto_refresh_viz, daemon=True)\n",
    "auto_refresh_thread.start()\n",
    "\n",
    "print(\"\\n🎯 How to Use:\")\n",
    "print(\"1. Select a scenario to see different failure patterns\")\n",
    "print(\"2. Click 'Start Demo' to begin the simulation\")\n",
    "print(\"3. Watch the live visualization update in real-time!\")\n",
    "print(\"4. Try manual controls to force state changes\")\n",
    "print(\"5. Use 'Health Check' to test all models\")\n",
    "print(\"\\n💡 Watch for:\")\n",
    "print(\"• State transitions (green → orange → red)\")\n",
    "print(\"• Success rates dropping and recovering\")\n",
    "print(\"• Failure patterns in the pie chart\")\n",
    "print(\"• Events appearing in the live log\")\n",
    "print(\"• System health score changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🤖 **Module 5: Building Self-Healing AI Systems**\n",
    "### The Ultimate Goal: Systems That Fix Themselves!\n",
    "\n",
    "```python\n",
    "# The Self-Healing Loop\n",
    "while system.is_running():\n",
    "    health = monitor.check_health()\n",
    "    if not health.is_healthy:\n",
    "        diagnosis = analyzer.diagnose(health)\n",
    "        remedy = healer.prescribe(diagnosis)\n",
    "        executor.apply_remedy(remedy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 Self-Healing System Control Panel\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fcaee833574a2884b92a56973c64a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Control Panel</h3>'), Dropdown(description='Scenario:', options=(('Normal Opera…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54ebdbc3fa84102897c29f2dedceee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📖 Instructions:\n",
      "1. Select a scenario\n",
      "2. Click 'Start' to begin monitoring\n",
      "3. Click 'Refresh' periodically to update the dashboard\n",
      "4. Watch the system heal itself!\n",
      "5. Try injecting failures to test healing\n",
      "\n",
      "Note: Manual refresh prevents flickering\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 10: Fixed Self-Healing AI System with Stable Demo\n",
    "# Corrected version with proper visualization and no flickering\n",
    "\n",
    "from enum import Enum, auto\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, deque\n",
    "from typing import Optional, Dict, Any, List, Callable\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import json\n",
    "import psutil\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    \"\"\"System health states with emoji indicators\"\"\"\n",
    "    HEALTHY = (\"healthy\", \"💚\", \"#00d084\")\n",
    "    DEGRADED = (\"degraded\", \"💛\", \"#ff9800\")\n",
    "    CRITICAL = (\"critical\", \"🔴\", \"#ff3860\")\n",
    "    RECOVERING = (\"recovering\", \"🔄\", \"#3273dc\")\n",
    "    MAINTENANCE = (\"maintenance\", \"🔧\", \"#9c27b0\")\n",
    "\n",
    "class HealingAction(Enum):\n",
    "    \"\"\"Types of healing actions the system can take\"\"\"\n",
    "    CIRCUIT_BREAKER_ADJUST = auto()\n",
    "    CACHE_EXPANSION = auto()\n",
    "    LOAD_SHEDDING = auto()\n",
    "    FALLBACK_ACTIVATION = auto()\n",
    "    RATE_LIMIT_ADJUST = auto()\n",
    "    TIMEOUT_ADJUST = auto()\n",
    "    RETRY_POLICY_CHANGE = auto()\n",
    "    RESOURCE_SCALING = auto()\n",
    "    GARBAGE_COLLECTION = auto()\n",
    "    DIAGNOSTIC_MODE = auto()\n",
    "\n",
    "@dataclass\n",
    "class HealthMetrics:\n",
    "    \"\"\"Comprehensive system health metrics\"\"\"\n",
    "    timestamp: datetime\n",
    "    error_rate: float\n",
    "    latency_p50: float\n",
    "    latency_p95: float\n",
    "    latency_p99: float\n",
    "    success_rate: float\n",
    "    throughput: float\n",
    "    queue_depth: int\n",
    "    active_connections: int\n",
    "    memory_usage: float\n",
    "    cpu_usage: float\n",
    "    cache_hit_rate: float\n",
    "    circuit_breaker_trips: int\n",
    "    healing_actions_taken: int\n",
    "    status: HealthStatus\n",
    "    \n",
    "    @property\n",
    "    def health_score(self) -> float:\n",
    "        \"\"\"Calculate overall health score (0-100)\"\"\"\n",
    "        weights = {\n",
    "            'error_rate': 0.3,\n",
    "            'latency': 0.2,\n",
    "            'throughput': 0.2,\n",
    "            'resources': 0.15,\n",
    "            'stability': 0.15\n",
    "        }\n",
    "        \n",
    "        # Error rate component\n",
    "        error_score = (1 - min(self.error_rate * 2, 1)) * 100 * weights['error_rate']\n",
    "        \n",
    "        # Latency component\n",
    "        latency_score = 100\n",
    "        if self.latency_p95 > 5:\n",
    "            latency_score -= min((self.latency_p95 - 5) * 10, 50)\n",
    "        latency_score = max(0, latency_score) * weights['latency']\n",
    "        \n",
    "        # Throughput component\n",
    "        throughput_score = min(self.throughput / 10, 1) * 100 * weights['throughput']\n",
    "        \n",
    "        # Resource usage component\n",
    "        resource_score = 100\n",
    "        if self.memory_usage > 80:\n",
    "            resource_score -= min((self.memory_usage - 80), 30)\n",
    "        if self.cpu_usage > 80:\n",
    "            resource_score -= min((self.cpu_usage - 80), 30)\n",
    "        resource_score = max(0, resource_score) * weights['resources']\n",
    "        \n",
    "        # Stability component\n",
    "        stability_score = max(0, 100 - self.circuit_breaker_trips * 10) * weights['stability']\n",
    "        \n",
    "        total = error_score + latency_score + throughput_score + resource_score + stability_score\n",
    "        return min(100, max(0, total))\n",
    "    \n",
    "    def get_diagnosis(self) -> List[str]:\n",
    "        \"\"\"Get list of current issues\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if self.error_rate > 0.1:\n",
    "            issues.append(f\"High error rate: {self.error_rate:.1%}\")\n",
    "        if self.latency_p95 > 3:\n",
    "            issues.append(f\"High latency: {self.latency_p95:.1f}s\")\n",
    "        if self.memory_usage > 80:\n",
    "            issues.append(f\"High memory: {self.memory_usage:.0f}%\")\n",
    "        if self.queue_depth > 50:\n",
    "            issues.append(f\"Queue backup: {self.queue_depth} items\")\n",
    "        if self.circuit_breaker_trips > 2:\n",
    "            issues.append(f\"Circuit instability: {self.circuit_breaker_trips} trips\")\n",
    "        \n",
    "        return issues\n",
    "\n",
    "class SelfHealingSystem:\n",
    "    \"\"\"Self-healing AI system with monitoring and auto-recovery\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Health monitoring\n",
    "        self.health_history = deque(maxlen=100)\n",
    "        self.healing_log = deque(maxlen=50)\n",
    "        self.event_stream = deque(maxlen=50)\n",
    "        \n",
    "        # Monitoring state\n",
    "        self.is_monitoring = False\n",
    "        self.monitor_thread = None\n",
    "        self.healer_thread = None\n",
    "        \n",
    "        # Configuration\n",
    "        self.config = {\n",
    "            'monitor_interval': 1,\n",
    "            'healing_interval': 3,\n",
    "            'health_threshold': 70,\n",
    "            'auto_heal': True,\n",
    "            'aggressive_healing': False\n",
    "        }\n",
    "        \n",
    "        # Metrics storage\n",
    "        self.metrics = {\n",
    "            'requests': deque(maxlen=100),\n",
    "            'errors': deque(maxlen=50),\n",
    "            'healings': deque(maxlen=20)\n",
    "        }\n",
    "        \n",
    "        # Simulation\n",
    "        self.simulation_active = False\n",
    "        self.simulation_scenario = 'normal'\n",
    "        self.simulation_time = 0\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Start the self-healing system\"\"\"\n",
    "        if self.is_monitoring:\n",
    "            print(\"System already running!\")\n",
    "            return\n",
    "            \n",
    "        self.is_monitoring = True\n",
    "        self.simulation_time = 0\n",
    "        \n",
    "        # Start monitor thread\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "        \n",
    "        # Start healer thread\n",
    "        self.healer_thread = threading.Thread(target=self._healer_loop, daemon=True)\n",
    "        self.healer_thread.start()\n",
    "        \n",
    "        self._log_event(\"System\", \"Started\", \"Self-healing activated\")\n",
    "        \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Stop the monitoring system\"\"\"\n",
    "        self.is_monitoring = False\n",
    "        self._log_event(\"System\", \"Stopped\", \"Monitoring stopped\")\n",
    "        \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Main monitoring loop\"\"\"\n",
    "        while self.is_monitoring:\n",
    "            try:\n",
    "                metrics = self._collect_metrics()\n",
    "                self.health_history.append(metrics)\n",
    "                \n",
    "                if metrics.health_score < self.config['health_threshold']:\n",
    "                    self._log_event(\"Monitor\", \"Alert\", f\"Health: {metrics.health_score:.1f}\")\n",
    "                \n",
    "                self.simulation_time += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Monitor error: {e}\")\n",
    "            \n",
    "            time.sleep(self.config['monitor_interval'])\n",
    "    \n",
    "    def _healer_loop(self):\n",
    "        \"\"\"Automatic healing loop\"\"\"\n",
    "        while self.is_monitoring:\n",
    "            try:\n",
    "                if self.config['auto_heal'] and len(self.health_history) > 0:\n",
    "                    latest = self.health_history[-1]\n",
    "                    \n",
    "                    if latest.status != HealthStatus.HEALTHY:\n",
    "                        self._perform_healing(latest)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Healer error: {e}\")\n",
    "            \n",
    "            time.sleep(self.config['healing_interval'])\n",
    "    \n",
    "    def _collect_metrics(self) -> HealthMetrics:\n",
    "        \"\"\"Collect metrics based on simulation scenario\"\"\"\n",
    "        \n",
    "        if self.simulation_scenario == 'degradation':\n",
    "            # Gradual degradation\n",
    "            progress = min(self.simulation_time / 30, 1)\n",
    "            error_rate = min(0.4, progress * 0.5)\n",
    "            latency_p95 = 1 + progress * 8\n",
    "            \n",
    "        elif self.simulation_scenario == 'spike':\n",
    "            # Traffic spike pattern\n",
    "            if self.simulation_time % 20 < 8:\n",
    "                error_rate = 0.3\n",
    "                latency_p95 = 6\n",
    "            else:\n",
    "                error_rate = 0.05\n",
    "                latency_p95 = 2\n",
    "                \n",
    "        elif self.simulation_scenario == 'recovery':\n",
    "            # Failure and recovery\n",
    "            cycle = self.simulation_time % 40\n",
    "            if cycle < 10:\n",
    "                error_rate = 0.35\n",
    "                latency_p95 = 7\n",
    "            elif cycle < 25:\n",
    "                error_rate = max(0.02, 0.35 - (cycle - 10) * 0.02)\n",
    "                latency_p95 = max(1.5, 7 - (cycle - 10) * 0.3)\n",
    "            else:\n",
    "                error_rate = 0.02\n",
    "                latency_p95 = 1.5\n",
    "                \n",
    "        elif self.simulation_scenario == 'chaos':\n",
    "            # Random chaos\n",
    "            error_rate = random.uniform(0, 0.45)\n",
    "            latency_p95 = random.uniform(1, 10)\n",
    "            \n",
    "        else:  # normal\n",
    "            error_rate = random.uniform(0, 0.05)\n",
    "            latency_p95 = random.uniform(0.8, 2.5)\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        latency_p50 = latency_p95 * 0.6\n",
    "        latency_p99 = latency_p95 * 1.3\n",
    "        success_rate = 1 - error_rate\n",
    "        throughput = max(1, 20 * success_rate * random.uniform(0.8, 1.2))\n",
    "        \n",
    "        # Determine status\n",
    "        if error_rate > 0.25 or latency_p95 > 5:\n",
    "            status = HealthStatus.CRITICAL\n",
    "        elif error_rate > 0.1 or latency_p95 > 3:\n",
    "            status = HealthStatus.DEGRADED\n",
    "        elif error_rate > 0.05:\n",
    "            status = HealthStatus.RECOVERING\n",
    "        else:\n",
    "            status = HealthStatus.HEALTHY\n",
    "        \n",
    "        return HealthMetrics(\n",
    "            timestamp=datetime.now(),\n",
    "            error_rate=error_rate,\n",
    "            latency_p50=latency_p50,\n",
    "            latency_p95=latency_p95,\n",
    "            latency_p99=latency_p99,\n",
    "            success_rate=success_rate,\n",
    "            throughput=throughput,\n",
    "            queue_depth=random.randint(0, 100),\n",
    "            active_connections=random.randint(10, 100),\n",
    "            memory_usage=random.uniform(30, 70),\n",
    "            cpu_usage=random.uniform(20, 60),\n",
    "            cache_hit_rate=random.uniform(0.6, 0.95),\n",
    "            circuit_breaker_trips=random.randint(0, 5) if error_rate > 0.2 else 0,\n",
    "            healing_actions_taken=len(self.healing_log),\n",
    "            status=status\n",
    "        )\n",
    "    \n",
    "    def _perform_healing(self, metrics: HealthMetrics):\n",
    "        \"\"\"Perform automatic healing\"\"\"\n",
    "        issues = metrics.get_diagnosis()\n",
    "        \n",
    "        if not issues:\n",
    "            return\n",
    "        \n",
    "        # Apply appropriate healing\n",
    "        healing_actions = []\n",
    "        \n",
    "        if metrics.error_rate > 0.2:\n",
    "            healing_actions.append(\"Circuit breakers tightened\")\n",
    "            healing_actions.append(\"Fallback activated\")\n",
    "            \n",
    "        if metrics.latency_p95 > 5:\n",
    "            healing_actions.append(\"Timeout reduced\")\n",
    "            healing_actions.append(\"Cache expanded\")\n",
    "            \n",
    "        if metrics.memory_usage > 80:\n",
    "            healing_actions.append(\"Garbage collection forced\")\n",
    "            \n",
    "        if healing_actions:\n",
    "            self._apply_healing(healing_actions)\n",
    "    \n",
    "    def _apply_healing(self, actions: List[str]):\n",
    "        \"\"\"Apply and log healing actions\"\"\"\n",
    "        healing_record = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'actions': actions\n",
    "        }\n",
    "        \n",
    "        self.healing_log.append(healing_record)\n",
    "        \n",
    "        for action in actions:\n",
    "            self._log_event(\"Healer\", \"Applied\", action)\n",
    "    \n",
    "    def _log_event(self, component: str, action: str, details: str):\n",
    "        \"\"\"Log system events\"\"\"\n",
    "        self.event_stream.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'component': component,\n",
    "            'action': action,\n",
    "            'details': details\n",
    "        })\n",
    "    \n",
    "    def inject_failure(self, failure_type: str):\n",
    "        \"\"\"Inject a failure for testing\"\"\"\n",
    "        # Add some bad metrics\n",
    "        for _ in range(10):\n",
    "            self.metrics['requests'].append({\n",
    "                'success': False,\n",
    "                'latency': 10,\n",
    "                'timestamp': datetime.now()\n",
    "            })\n",
    "        \n",
    "        self._log_event(\"Test\", \"Injected\", failure_type)\n",
    "\n",
    "# Create the system\n",
    "system = SelfHealingSystem()\n",
    "\n",
    "# Dashboard class\n",
    "class Dashboard:\n",
    "    \"\"\"Dashboard for visualization\"\"\"\n",
    "    \n",
    "    def __init__(self, system):\n",
    "        self.system = system\n",
    "    \n",
    "    def create_visualization(self):\n",
    "        \"\"\"Create the dashboard\"\"\"\n",
    "        if len(self.system.health_history) == 0:\n",
    "            return go.Figure().add_annotation(text=\"No data yet - Start the system first!\")\n",
    "        \n",
    "        # Create figure\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=3,\n",
    "            subplot_titles=(\n",
    "                'Health Score', 'Error Rate %', 'Latency (seconds)',\n",
    "                'System Status', 'Recent Events', 'Healing Actions'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{'type': 'indicator'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "                [{'type': 'indicator'}, {'type': 'table'}, {'type': 'bar'}]\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Get data\n",
    "        history = list(self.system.health_history)\n",
    "        latest = history[-1]\n",
    "        \n",
    "        # 1. Health Score Gauge\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=latest.health_score,\n",
    "                title={'text': \"Health\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': latest.status.value[2]},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 50], 'color': \"#ffcdd2\"},\n",
    "                        {'range': [50, 70], 'color': \"#fff9c4\"},\n",
    "                        {'range': [70, 100], 'color': \"#c8e6c9\"}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 2},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 30\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Error Rate\n",
    "        timestamps = [h.timestamp for h in history[-50:]]\n",
    "        error_rates = [h.error_rate * 100 for h in history[-50:]]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=error_rates,\n",
    "                mode='lines',\n",
    "                name='Error Rate',\n",
    "                line=dict(color='red', width=2),\n",
    "                fill='tozeroy'\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Latency\n",
    "        latencies = [h.latency_p95 for h in history[-50:]]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=latencies,\n",
    "                mode='lines',\n",
    "                name='P95 Latency',\n",
    "                line=dict(color='blue', width=2)\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # 4. Status Indicator\n",
    "        status_text = latest.status.value[0].upper()\n",
    "        status_emoji = latest.status.value[1]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"number\",\n",
    "                value=len(self.system.healing_log),\n",
    "                title={'text': f\"{status_emoji} {status_text}<br>Total Healings\"},\n",
    "                number={'font': {'size': 40}}\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 5. Event Log\n",
    "        events_data = []\n",
    "        for event in list(self.system.event_stream)[-8:]:\n",
    "            events_data.append([\n",
    "                event['timestamp'].strftime('%H:%M:%S'),\n",
    "                event['component'],\n",
    "                event['details'][:30]\n",
    "            ])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Time', 'Component', 'Event'],\n",
    "                    fill_color='paleturquoise',\n",
    "                    align='left'\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=list(zip(*events_data)) if events_data else [[], [], []],\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # 6. Healing Actions Count\n",
    "        healing_counts = {}\n",
    "        for record in self.system.healing_log:\n",
    "            for action in record['actions']:\n",
    "                key = action.split()[0]\n",
    "                healing_counts[key] = healing_counts.get(key, 0) + 1\n",
    "        \n",
    "        if healing_counts:\n",
    "            fig.add_trace(\n",
    "                go.Bar(\n",
    "                    x=list(healing_counts.keys()),\n",
    "                    y=list(healing_counts.values()),\n",
    "                    marker_color='green'\n",
    "                ),\n",
    "                row=2, col=3\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=600,\n",
    "            showlegend=False,\n",
    "            title_text=\"Self-Healing System Dashboard\"\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = Dashboard(system)\n",
    "\n",
    "# Interactive controls\n",
    "print(\"🎮 Self-Healing System Control Panel\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario selector\n",
    "scenario_selector = widgets.Dropdown(\n",
    "    options=[\n",
    "        ('Normal Operations', 'normal'),\n",
    "        ('Gradual Degradation', 'degradation'),\n",
    "        ('Traffic Spike', 'spike'),\n",
    "        ('Failure & Recovery', 'recovery'),\n",
    "        ('Chaos Mode', 'chaos')\n",
    "    ],\n",
    "    value='normal',\n",
    "    description='Scenario:'\n",
    ")\n",
    "\n",
    "# Control buttons\n",
    "start_button = widgets.Button(\n",
    "    description='▶️ Start',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='⏹️ Stop',\n",
    "    button_style='danger',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description='🔄 Refresh',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "inject_button = widgets.Button(\n",
    "    description='💥 Inject Failure',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "# Settings\n",
    "auto_heal_toggle = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Auto-Heal'\n",
    ")\n",
    "\n",
    "# Output\n",
    "output = widgets.Output()\n",
    "viz_output = widgets.Output()\n",
    "\n",
    "# Handlers\n",
    "def start_system(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        system.simulation_scenario = scenario_selector.value\n",
    "        system.simulation_active = True\n",
    "        system.start_monitoring()\n",
    "        print(f\"✅ Started with scenario: {scenario_selector.value}\")\n",
    "        print(\"Click 'Refresh' to update visualization\")\n",
    "\n",
    "def stop_system(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        system.stop_monitoring()\n",
    "        print(\"⏹️ System stopped\")\n",
    "\n",
    "def refresh_viz(b):\n",
    "    with viz_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            fig = dashboard.create_visualization()\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Visualization error: {e}\")\n",
    "\n",
    "def inject_failure(b):\n",
    "    system.inject_failure(\"manual\")\n",
    "    with output:\n",
    "        print(\"💥 Failure injected!\")\n",
    "\n",
    "def update_settings(change):\n",
    "    system.config['auto_heal'] = auto_heal_toggle.value\n",
    "\n",
    "# Connect handlers\n",
    "start_button.on_click(start_system)\n",
    "stop_button.on_click(stop_system)\n",
    "refresh_button.on_click(refresh_viz)\n",
    "inject_button.on_click(inject_failure)\n",
    "auto_heal_toggle.observe(update_settings, names='value')\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Control Panel</h3>\"),\n",
    "    scenario_selector,\n",
    "    widgets.HBox([start_button, stop_button, refresh_button]),\n",
    "    widgets.HBox([auto_heal_toggle, inject_button]),\n",
    "    output\n",
    "])\n",
    "\n",
    "display(controls)\n",
    "display(viz_output)\n",
    "\n",
    "# Initial viz\n",
    "with viz_output:\n",
    "    fig = dashboard.create_visualization()\n",
    "    fig.show()\n",
    "\n",
    "print(\"\\n📖 Instructions:\")\n",
    "print(\"1. Select a scenario\")\n",
    "print(\"2. Click 'Start' to begin monitoring\")\n",
    "print(\"3. Click 'Refresh' periodically to update the dashboard\")\n",
    "print(\"4. Watch the system heal itself!\")\n",
    "print(\"5. Try injecting failures to test healing\")\n",
    "print(\"\\nNote: Manual refresh prevents flickering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🎮 **Final Boss: Complete System Test**\n",
    "#### Put everything together and watch your resilient AI handle chaos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎮 ULTIMATE CHAOS ENGINEERING CONSOLE\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.widget-label { color: white !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2ec0ed584a45f4a5d463d28e94c3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"<h2 style='color: white;'>🎮 CHAOS CONTROL CENTER</h2>\"), HBox(children=(VBox(childr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d04f8f56654f94b68d6127c3e331cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 OBJECTIVES:\n",
      "• Maintain >80% success rate to earn achievements\n",
      "• Complete all scenarios to become Chaos Master\n",
      "• Reach Level 10 for ultimate bragging rights\n",
      "• Compete for high scores in each scenario\n",
      "\n",
      "💡 TIPS:\n",
      "• Start with lower intensity to learn patterns\n",
      "• Watch system health and adjust strategy\n",
      "• Healing triggers boost your score\n",
      "• Each scenario has unique challenges\n",
      "\n",
      "🏆 Ready to master chaos? Select a scenario and UNLEASH!\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 11: Ultimate Chaos Engineering System with Gamification\n",
    "# Advanced chaos testing with live visualizations, achievements, and interactive controls!\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "import threading\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque, defaultdict\n",
    "from enum import Enum, auto\n",
    "import json\n",
    "\n",
    "class ChaosType(Enum):\n",
    "    \"\"\"Types of chaos that can be injected\"\"\"\n",
    "    API_FLOOD = auto()\n",
    "    SERVICE_OUTAGE = auto()\n",
    "    LATENCY_SPIKE = auto()\n",
    "    RANDOM_ERRORS = auto()\n",
    "    MEMORY_PRESSURE = auto()\n",
    "    NETWORK_PARTITION = auto()\n",
    "    CASCADE_FAILURE = auto()\n",
    "    BYZANTINE_FAULT = auto()\n",
    "    TOTAL_CHAOS = auto()\n",
    "\n",
    "class Achievement:\n",
    "    \"\"\"Achievement system for chaos engineering\"\"\"\n",
    "    def __init__(self, name, description, icon, condition):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.icon = icon\n",
    "        self.condition = condition\n",
    "        self.unlocked = False\n",
    "        self.unlock_time = None\n",
    "\n",
    "class ChaosEngineeringSystem:\n",
    "    \"\"\"Advanced chaos engineering system with gamification\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Chaos scenarios with detailed configurations\n",
    "        self.chaos_scenarios = {\n",
    "            '🌊 API Flood': {\n",
    "                'type': ChaosType.API_FLOOD,\n",
    "                'description': 'Massive request surge (100-500 req/s)',\n",
    "                'difficulty': 3,\n",
    "                'duration': 15,\n",
    "                'params': {'rate_multiplier': 10, 'burst_size': 100},\n",
    "                'points': 300\n",
    "            },\n",
    "            '💥 Service Outage': {\n",
    "                'type': ChaosType.SERVICE_OUTAGE,\n",
    "                'description': 'Complete service failure for primary models',\n",
    "                'difficulty': 4,\n",
    "                'duration': 20,\n",
    "                'params': {'failure_rate': 0.95, 'affected_services': ['primary']},\n",
    "                'points': 400\n",
    "            },\n",
    "            '🐌 Latency Spike': {\n",
    "                'type': ChaosType.LATENCY_SPIKE,\n",
    "                'description': 'Extreme latency (10-30 second delays)',\n",
    "                'difficulty': 2,\n",
    "                'duration': 15,\n",
    "                'params': {'latency_range': (10, 30), 'spike_probability': 0.7},\n",
    "                'points': 200\n",
    "            },\n",
    "            '🎲 Random Errors': {\n",
    "                'type': ChaosType.RANDOM_ERRORS,\n",
    "                'description': 'Unpredictable 50-80% failure rate',\n",
    "                'difficulty': 3,\n",
    "                'duration': 20,\n",
    "                'params': {'error_rate_range': (0.5, 0.8), 'error_types': ['timeout', '500', '429']},\n",
    "                'points': 350\n",
    "            },\n",
    "            '💾 Memory Pressure': {\n",
    "                'type': ChaosType.MEMORY_PRESSURE,\n",
    "                'description': 'Simulate memory leak and resource exhaustion',\n",
    "                'difficulty': 4,\n",
    "                'duration': 25,\n",
    "                'params': {'memory_growth_rate': 0.05, 'gc_failure_rate': 0.3},\n",
    "                'points': 450\n",
    "            },\n",
    "            '🌐 Network Partition': {\n",
    "                'type': ChaosType.NETWORK_PARTITION,\n",
    "                'description': 'Network splits and connectivity issues',\n",
    "                'difficulty': 5,\n",
    "                'duration': 20,\n",
    "                'params': {'partition_probability': 0.4, 'recovery_time': 5},\n",
    "                'points': 500\n",
    "            },\n",
    "            '🔥 Cascade Failure': {\n",
    "                'type': ChaosType.CASCADE_FAILURE,\n",
    "                'description': 'One failure triggers chain reaction',\n",
    "                'difficulty': 5,\n",
    "                'duration': 30,\n",
    "                'params': {'initial_failure': 'primary', 'cascade_probability': 0.8},\n",
    "                'points': 600\n",
    "            },\n",
    "            '👹 Byzantine Fault': {\n",
    "                'type': ChaosType.BYZANTINE_FAULT,\n",
    "                'description': 'Services give wrong responses',\n",
    "                'difficulty': 6,\n",
    "                'duration': 25,\n",
    "                'params': {'corruption_rate': 0.3, 'inconsistency_rate': 0.5},\n",
    "                'points': 700\n",
    "            },\n",
    "            '🌪️ TOTAL CHAOS': {\n",
    "                'type': ChaosType.TOTAL_CHAOS,\n",
    "                'description': 'ALL problems simultaneously!',\n",
    "                'difficulty': 10,\n",
    "                'duration': 60,\n",
    "                'params': {'all_chaos': True, 'intensity': 'maximum'},\n",
    "                'points': 1000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Initialize achievements\n",
    "        self.achievements = self._init_achievements()\n",
    "        \n",
    "        # Chaos state\n",
    "        self.chaos_active = False\n",
    "        self.current_scenario = None\n",
    "        self.chaos_thread = None\n",
    "        self.chaos_start_time = None\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.metrics = {\n",
    "            'total_chaos_runs': 0,\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'fallback_uses': 0,\n",
    "            'healing_triggers': 0,\n",
    "            'circuit_breaks': 0,\n",
    "            'avg_response_time': 0,\n",
    "            'uptime_percentage': 100\n",
    "        }\n",
    "        \n",
    "        # Live data for visualization\n",
    "        self.live_data = {\n",
    "            'timestamps': deque(maxlen=100),\n",
    "            'success_rate': deque(maxlen=100),\n",
    "            'latency': deque(maxlen=100),\n",
    "            'health_score': deque(maxlen=100),\n",
    "            'chaos_intensity': deque(maxlen=100),\n",
    "            'events': deque(maxlen=50)\n",
    "        }\n",
    "        \n",
    "        # Player stats\n",
    "        self.player_stats = {\n",
    "            'level': 1,\n",
    "            'experience': 0,\n",
    "            'total_score': 0,\n",
    "            'chaos_mastery': 0,\n",
    "            'resilience_rating': 100,\n",
    "            'scenarios_completed': [],\n",
    "            'best_scores': {}\n",
    "        }\n",
    "        \n",
    "        # System reference (would connect to actual self-healing system)\n",
    "        self.system_health = 100\n",
    "        self.system_status = \"READY\"\n",
    "        \n",
    "    def _init_achievements(self):\n",
    "        \"\"\"Initialize achievement system\"\"\"\n",
    "        return [\n",
    "            Achievement(\n",
    "                \"First Blood\", \n",
    "                \"Complete your first chaos scenario\",\n",
    "                \"🩸\",\n",
    "                lambda s: s['total_chaos_runs'] >= 1\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Survivor\",\n",
    "                \"Maintain 80% success rate during chaos\",\n",
    "                \"🛡️\",\n",
    "                lambda s: s['successful_requests'] / max(s['total_requests'], 1) > 0.8\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Chaos Master\",\n",
    "                \"Complete all chaos scenarios\",\n",
    "                \"👑\",\n",
    "                lambda s: len(s.get('scenarios_completed', [])) >= 9\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Unbreakable\",\n",
    "                \"Achieve 95% uptime during Total Chaos\",\n",
    "                \"💎\",\n",
    "                lambda s: s.get('total_chaos_uptime', 0) > 0.95\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Speed Demon\",\n",
    "                \"Complete a scenario with <2s avg response time\",\n",
    "                \"⚡\",\n",
    "                lambda s: s['avg_response_time'] < 2 and s['total_chaos_runs'] > 0\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Healer\",\n",
    "                \"Trigger 100+ healing actions\",\n",
    "                \"💊\",\n",
    "                lambda s: s['healing_triggers'] >= 100\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Circuit Breaker\",\n",
    "                \"Trip 50+ circuit breakers\",\n",
    "                \"⚡\",\n",
    "                lambda s: s['circuit_breaks'] >= 50\n",
    "            ),\n",
    "            Achievement(\n",
    "                \"Resilience God\",\n",
    "                \"Reach level 10\",\n",
    "                \"🌟\",\n",
    "                lambda s: s.get('level', 1) >= 10\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def start_chaos(self, scenario_name, intensity=1.0):\n",
    "        \"\"\"Start a chaos scenario\"\"\"\n",
    "        if self.chaos_active:\n",
    "            return \"Chaos already active!\"\n",
    "        \n",
    "        self.current_scenario = self.chaos_scenarios[scenario_name]\n",
    "        self.chaos_active = True\n",
    "        self.chaos_start_time = time.time()\n",
    "        self.metrics['total_chaos_runs'] += 1\n",
    "        \n",
    "        # Start chaos thread\n",
    "        self.chaos_thread = threading.Thread(\n",
    "            target=self._run_chaos,\n",
    "            args=(scenario_name, intensity),\n",
    "            daemon=True\n",
    "        )\n",
    "        self.chaos_thread.start()\n",
    "        \n",
    "        # Log event\n",
    "        self._log_event(\"CHAOS\", \"Started\", f\"{scenario_name} (Intensity: {intensity:.1f})\")\n",
    "        \n",
    "        return f\"Chaos scenario '{scenario_name}' started!\"\n",
    "    \n",
    "    def _run_chaos(self, scenario_name, intensity):\n",
    "        \"\"\"Run the chaos scenario\"\"\"\n",
    "        scenario = self.current_scenario\n",
    "        duration = scenario['duration']\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Track scenario-specific metrics\n",
    "        scenario_metrics = {\n",
    "            'requests': 0,\n",
    "            'successes': 0,\n",
    "            'failures': 0,\n",
    "            'total_latency': 0\n",
    "        }\n",
    "        \n",
    "        while time.time() - start_time < duration and self.chaos_active:\n",
    "            # Simulate chaos effects\n",
    "            chaos_intensity = self._calculate_chaos_intensity(\n",
    "                scenario['type'], \n",
    "                (time.time() - start_time) / duration,\n",
    "                intensity\n",
    "            )\n",
    "            \n",
    "            # Simulate request\n",
    "            success, latency = self._simulate_request(scenario['type'], chaos_intensity)\n",
    "            \n",
    "            # Update metrics\n",
    "            scenario_metrics['requests'] += 1\n",
    "            if success:\n",
    "                scenario_metrics['successes'] += 1\n",
    "                self.metrics['successful_requests'] += 1\n",
    "            else:\n",
    "                scenario_metrics['failures'] += 1\n",
    "                self.metrics['failed_requests'] += 1\n",
    "                \n",
    "            scenario_metrics['total_latency'] += latency\n",
    "            self.metrics['total_requests'] += 1\n",
    "            \n",
    "            # Update live data\n",
    "            self._update_live_data(success, latency, chaos_intensity)\n",
    "            \n",
    "            # Simulate system response\n",
    "            self._simulate_system_response(success, chaos_intensity)\n",
    "            \n",
    "            # Small delay\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # Calculate final score\n",
    "        success_rate = scenario_metrics['successes'] / max(scenario_metrics['requests'], 1)\n",
    "        avg_latency = scenario_metrics['total_latency'] / max(scenario_metrics['requests'], 1)\n",
    "        \n",
    "        score = self._calculate_score(scenario, success_rate, avg_latency)\n",
    "        \n",
    "        # Update player stats\n",
    "        self._update_player_stats(scenario_name, score, success_rate)\n",
    "        \n",
    "        # Check achievements\n",
    "        self._check_achievements()\n",
    "        \n",
    "        self.chaos_active = False\n",
    "        self._log_event(\"CHAOS\", \"Completed\", f\"Score: {score}\")\n",
    "        \n",
    "    def _calculate_chaos_intensity(self, chaos_type, progress, base_intensity):\n",
    "        \"\"\"Calculate current chaos intensity based on scenario and progress\"\"\"\n",
    "        if chaos_type == ChaosType.CASCADE_FAILURE:\n",
    "            # Exponential growth for cascade\n",
    "            return min(1.0, base_intensity * (1.5 ** (progress * 3)))\n",
    "        elif chaos_type == ChaosType.API_FLOOD:\n",
    "            # Wave pattern for flood\n",
    "            import math\n",
    "            return base_intensity * (1 + 0.5 * math.sin(progress * 10))\n",
    "        elif chaos_type == ChaosType.TOTAL_CHAOS:\n",
    "            # Random spikes for total chaos\n",
    "            return base_intensity * random.uniform(0.5, 1.5)\n",
    "        else:\n",
    "            # Default linear progression\n",
    "            return base_intensity * (0.5 + progress * 0.5)\n",
    "    \n",
    "    def _simulate_request(self, chaos_type, intensity):\n",
    "        \"\"\"Simulate a request under chaos conditions\"\"\"\n",
    "        # Base success rate and latency\n",
    "        base_success_rate = 0.95\n",
    "        base_latency = 1.0\n",
    "        \n",
    "        # Apply chaos effects\n",
    "        if chaos_type == ChaosType.SERVICE_OUTAGE:\n",
    "            success_rate = 1 - intensity * 0.9\n",
    "            latency = base_latency * (1 + intensity * 10)\n",
    "        elif chaos_type == ChaosType.LATENCY_SPIKE:\n",
    "            success_rate = base_success_rate\n",
    "            latency = base_latency + intensity * 20\n",
    "        elif chaos_type == ChaosType.RANDOM_ERRORS:\n",
    "            success_rate = 1 - random.uniform(0, intensity)\n",
    "            latency = base_latency * random.uniform(1, 5)\n",
    "        elif chaos_type == ChaosType.TOTAL_CHAOS:\n",
    "            success_rate = random.uniform(0.1, 0.9)\n",
    "            latency = random.uniform(0.5, 30)\n",
    "        else:\n",
    "            success_rate = base_success_rate - intensity * 0.3\n",
    "            latency = base_latency * (1 + intensity * 3)\n",
    "        \n",
    "        success = random.random() < success_rate\n",
    "        \n",
    "        return success, latency\n",
    "    \n",
    "    def _simulate_system_response(self, request_success, chaos_intensity):\n",
    "        \"\"\"Simulate system's self-healing response\"\"\"\n",
    "        # Simulate healing triggers\n",
    "        if not request_success and random.random() < 0.5:\n",
    "            self.metrics['healing_triggers'] += 1\n",
    "            self._log_event(\"HEAL\", \"Triggered\", \"Auto-healing activated\")\n",
    "        \n",
    "        # Simulate circuit breaker\n",
    "        if chaos_intensity > 0.7 and random.random() < 0.3:\n",
    "            self.metrics['circuit_breaks'] += 1\n",
    "            self._log_event(\"CIRCUIT\", \"Tripped\", \"Circuit breaker activated\")\n",
    "        \n",
    "        # Update system health\n",
    "        if request_success:\n",
    "            self.system_health = min(100, self.system_health + 0.5)\n",
    "        else:\n",
    "            self.system_health = max(0, self.system_health - 2)\n",
    "        \n",
    "        # Update system status\n",
    "        if self.system_health > 80:\n",
    "            self.system_status = \"HEALTHY\"\n",
    "        elif self.system_health > 50:\n",
    "            self.system_status = \"DEGRADED\"\n",
    "        elif self.system_health > 20:\n",
    "            self.system_status = \"CRITICAL\"\n",
    "        else:\n",
    "            self.system_status = \"FAILING\"\n",
    "    \n",
    "    def _update_live_data(self, success, latency, chaos_intensity):\n",
    "        \"\"\"Update live visualization data\"\"\"\n",
    "        timestamp = datetime.now()\n",
    "        \n",
    "        self.live_data['timestamps'].append(timestamp)\n",
    "        \n",
    "        # Calculate rolling success rate\n",
    "        recent_successes = sum(1 for _ in range(min(10, len(self.live_data['success_rate']))))\n",
    "        success_rate = (recent_successes + (1 if success else 0)) / 11\n",
    "        self.live_data['success_rate'].append(success_rate * 100)\n",
    "        \n",
    "        self.live_data['latency'].append(latency)\n",
    "        self.live_data['health_score'].append(self.system_health)\n",
    "        self.live_data['chaos_intensity'].append(chaos_intensity * 100)\n",
    "    \n",
    "    def _calculate_score(self, scenario, success_rate, avg_latency):\n",
    "        \"\"\"Calculate score for chaos run\"\"\"\n",
    "        base_score = scenario['points']\n",
    "        \n",
    "        # Success rate multiplier (0.0 - 1.5)\n",
    "        success_multiplier = min(1.5, success_rate * 1.5)\n",
    "        \n",
    "        # Latency bonus (lower is better)\n",
    "        latency_bonus = max(0, 100 - avg_latency * 10)\n",
    "        \n",
    "        # Difficulty multiplier\n",
    "        difficulty_multiplier = 1 + (scenario['difficulty'] - 1) * 0.2\n",
    "        \n",
    "        final_score = int(\n",
    "            (base_score * success_multiplier + latency_bonus) * difficulty_multiplier\n",
    "        )\n",
    "        \n",
    "        return final_score\n",
    "    \n",
    "    def _update_player_stats(self, scenario_name, score, success_rate):\n",
    "        \"\"\"Update player statistics\"\"\"\n",
    "        self.player_stats['total_score'] += score\n",
    "        \n",
    "        # Update experience and level\n",
    "        self.player_stats['experience'] += score // 10\n",
    "        new_level = 1 + self.player_stats['experience'] // 1000\n",
    "        if new_level > self.player_stats['level']:\n",
    "            self.player_stats['level'] = new_level\n",
    "            self._log_event(\"LEVEL\", \"Level Up!\", f\"Reached level {new_level}\")\n",
    "        \n",
    "        # Track completed scenarios\n",
    "        if scenario_name not in self.player_stats['scenarios_completed']:\n",
    "            self.player_stats['scenarios_completed'].append(scenario_name)\n",
    "        \n",
    "        # Update best scores\n",
    "        if scenario_name not in self.player_stats['best_scores'] or \\\n",
    "           score > self.player_stats['best_scores'][scenario_name]:\n",
    "            self.player_stats['best_scores'][scenario_name] = score\n",
    "        \n",
    "        # Update chaos mastery\n",
    "        self.player_stats['chaos_mastery'] = min(100, \n",
    "            self.player_stats['chaos_mastery'] + success_rate * 10)\n",
    "        \n",
    "        # Update resilience rating\n",
    "        self.player_stats['resilience_rating'] = int(\n",
    "            (self.system_health + success_rate * 100) / 2\n",
    "        )\n",
    "    \n",
    "    def _check_achievements(self):\n",
    "        \"\"\"Check and unlock achievements\"\"\"\n",
    "        stats = {\n",
    "            **self.metrics,\n",
    "            **self.player_stats\n",
    "        }\n",
    "        \n",
    "        for achievement in self.achievements:\n",
    "            if not achievement.unlocked and achievement.condition(stats):\n",
    "                achievement.unlocked = True\n",
    "                achievement.unlock_time = datetime.now()\n",
    "                self._log_event(\"ACHIEVEMENT\", achievement.name, achievement.description)\n",
    "    \n",
    "    def _log_event(self, category, action, details):\n",
    "        \"\"\"Log an event\"\"\"\n",
    "        event = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'category': category,\n",
    "            'action': action,\n",
    "            'details': details\n",
    "        }\n",
    "        self.live_data['events'].append(event)\n",
    "    \n",
    "    def stop_chaos(self):\n",
    "        \"\"\"Stop the current chaos scenario\"\"\"\n",
    "        self.chaos_active = False\n",
    "        self._log_event(\"CHAOS\", \"Stopped\", \"Manually terminated\")\n",
    "    \n",
    "    def get_dashboard_data(self):\n",
    "        \"\"\"Get data for dashboard visualization\"\"\"\n",
    "        return {\n",
    "            'metrics': self.metrics,\n",
    "            'live_data': self.live_data,\n",
    "            'player_stats': self.player_stats,\n",
    "            'system_health': self.system_health,\n",
    "            'system_status': self.system_status,\n",
    "            'achievements': self.achievements,\n",
    "            'chaos_active': self.chaos_active,\n",
    "            'current_scenario': self.current_scenario\n",
    "        }\n",
    "\n",
    "# Create chaos engineering system\n",
    "chaos_system = ChaosEngineeringSystem()\n",
    "\n",
    "# Dashboard class\n",
    "class ChaosDashboard:\n",
    "    \"\"\"Interactive dashboard for chaos engineering\"\"\"\n",
    "    \n",
    "    def __init__(self, chaos_system):\n",
    "        self.system = chaos_system\n",
    "    \n",
    "    def create_visualization(self):\n",
    "        \"\"\"Create comprehensive chaos dashboard\"\"\"\n",
    "        data = self.system.get_dashboard_data()\n",
    "        \n",
    "        # Create figure with subplots\n",
    "        fig = make_subplots(\n",
    "            rows=3, cols=4,\n",
    "            subplot_titles=(\n",
    "                'System Health', 'Chaos Intensity', 'Success Rate', 'Response Latency',\n",
    "                'Player Level', 'Score', 'Achievements', 'Resilience Rating',\n",
    "                'Event Log', 'Live Metrics', 'Chaos Progress', 'Leaderboard'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{'type': 'indicator'}, {'type': 'scatter'}, {'type': 'scatter'}, {'type': 'scatter'}],\n",
    "                [{'type': 'indicator'}, {'type': 'indicator'}, {'type': 'bar'}, {'type': 'indicator'}],\n",
    "                [{'type': 'table'}, {'type': 'scatter'}, {'type': 'indicator'}, {'type': 'table'}]\n",
    "            ],\n",
    "            vertical_spacing=0.12,\n",
    "            horizontal_spacing=0.1\n",
    "        )\n",
    "        \n",
    "        live_data = data['live_data']\n",
    "        \n",
    "        # 1. System Health Gauge\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=data['system_health'],\n",
    "                title={'text': \"Health\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': self._get_health_color(data['system_health'])},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 30], 'color': \"#ffebee\"},\n",
    "                        {'range': [30, 70], 'color': \"#fff3e0\"},\n",
    "                        {'range': [70, 100], 'color': \"#e8f5e9\"}\n",
    "                    ],\n",
    "                    'threshold': {\n",
    "                        'line': {'color': \"red\", 'width': 4},\n",
    "                        'thickness': 0.75,\n",
    "                        'value': 20\n",
    "                    }\n",
    "                }\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Chaos Intensity Timeline\n",
    "        if len(live_data['timestamps']) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(live_data['timestamps']),\n",
    "                    y=list(live_data['chaos_intensity']),\n",
    "                    mode='lines',\n",
    "                    name='Chaos',\n",
    "                    line=dict(color='red', width=2),\n",
    "                    fill='tozeroy',\n",
    "                    fillcolor='rgba(255,0,0,0.1)'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # 3. Success Rate Timeline\n",
    "        if len(live_data['timestamps']) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(live_data['timestamps']),\n",
    "                    y=list(live_data['success_rate']),\n",
    "                    mode='lines',\n",
    "                    name='Success',\n",
    "                    line=dict(color='green', width=2),\n",
    "                    fill='tozeroy',\n",
    "                    fillcolor='rgba(0,255,0,0.1)'\n",
    "                ),\n",
    "                row=1, col=3\n",
    "            )\n",
    "        \n",
    "        # 4. Latency Timeline\n",
    "        if len(live_data['timestamps']) > 0:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(live_data['timestamps']),\n",
    "                    y=list(live_data['latency']),\n",
    "                    mode='lines+markers',\n",
    "                    name='Latency',\n",
    "                    line=dict(color='blue', width=2)\n",
    "                ),\n",
    "                row=1, col=4\n",
    "            )\n",
    "        \n",
    "        # 5. Player Level\n",
    "        player_stats = data['player_stats']\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"number+delta\",\n",
    "                value=player_stats['level'],\n",
    "                title={'text': f\"Level {player_stats['level']}\"},\n",
    "                delta={'reference': player_stats['level'] - 1},\n",
    "                number={'font': {'size': 40}}\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 6. Total Score\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"number\",\n",
    "                value=player_stats['total_score'],\n",
    "                title={'text': \"Total Score\"},\n",
    "                number={'font': {'size': 35, 'color': '#FFD700'}}\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # 7. Achievements Progress\n",
    "        unlocked = sum(1 for a in data['achievements'] if a.unlocked)\n",
    "        total = len(data['achievements'])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=['Unlocked', 'Locked'],\n",
    "                y=[unlocked, total - unlocked],\n",
    "                marker=dict(color=['#4CAF50', '#9E9E9E']),\n",
    "                text=[f'{unlocked}', f'{total - unlocked}'],\n",
    "                textposition='auto'\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "        \n",
    "        # 8. Resilience Rating\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=player_stats['resilience_rating'],\n",
    "                title={'text': \"Resilience\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': '#3498db'},\n",
    "                    'bgcolor': \"white\"\n",
    "                }\n",
    "            ),\n",
    "            row=2, col=4\n",
    "        )\n",
    "        \n",
    "        # 9. Event Log\n",
    "        events_data = []\n",
    "        for event in list(live_data['events'])[-8:]:\n",
    "            emoji = {\n",
    "                'CHAOS': '🌪️',\n",
    "                'HEAL': '💊',\n",
    "                'CIRCUIT': '⚡',\n",
    "                'ACHIEVEMENT': '🏆',\n",
    "                'LEVEL': '⬆️'\n",
    "            }.get(event['category'], '📝')\n",
    "            \n",
    "            events_data.append([\n",
    "                event['timestamp'].strftime('%H:%M:%S'),\n",
    "                emoji,\n",
    "                event['action'][:15],\n",
    "                event['details'][:25]\n",
    "            ])\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                header=dict(\n",
    "                    values=['Time', '', 'Action', 'Details'],\n",
    "                    fill_color='#34495e',\n",
    "                    font_color='white',\n",
    "                    align='left',\n",
    "                    height=20\n",
    "                ),\n",
    "                cells=dict(\n",
    "                    values=list(zip(*events_data)) if events_data else [[], [], [], []],\n",
    "                    fill_color='#ecf0f1',\n",
    "                    align='left',\n",
    "                    height=18,\n",
    "                    font_size=9\n",
    "                )\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        # 10. Live Metrics Summary\n",
    "        if len(live_data['timestamps']) > 0:\n",
    "            # Combined metrics view\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=list(live_data['timestamps'])[-30:],\n",
    "                    y=list(live_data['health_score'])[-30:],\n",
    "                    mode='lines',\n",
    "                    name='Health',\n",
    "                    line=dict(color='green', width=3)\n",
    "                ),\n",
    "                row=3, col=2\n",
    "            )\n",
    "        \n",
    "        # 11. Chaos Progress\n",
    "        if data['chaos_active'] and data['current_scenario']:\n",
    "            elapsed = time.time() - self.system.chaos_start_time\n",
    "            total = data['current_scenario']['duration']\n",
    "            progress = min(100, (elapsed / total) * 100)\n",
    "        else:\n",
    "            progress = 0\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"number+gauge\",\n",
    "                value=progress,\n",
    "                title={'text': \"Progress %\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': '#ff9800'}\n",
    "                }\n",
    "            ),\n",
    "            row=3, col=3\n",
    "        )\n",
    "        \n",
    "        # 12. Best Scores Leaderboard\n",
    "        best_scores = player_stats.get('best_scores', {})\n",
    "        if best_scores:\n",
    "            scenarios = list(best_scores.keys())[:5]\n",
    "            scores = [best_scores[s] for s in scenarios]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Table(\n",
    "                    header=dict(\n",
    "                        values=['Scenario', 'Best Score'],\n",
    "                        fill_color='#2c3e50',\n",
    "                        font_color='white',\n",
    "                        align='center'\n",
    "                    ),\n",
    "                    cells=dict(\n",
    "                        values=[scenarios, scores],\n",
    "                        fill_color='#34495e',\n",
    "                        font_color='white',\n",
    "                        align='center'\n",
    "                    )\n",
    "                ),\n",
    "                row=3, col=4\n",
    "            )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            showlegend=False,\n",
    "            title_text=\"<b>🌪️ CHAOS ENGINEERING COMMAND CENTER</b>\",\n",
    "            title_font_size=20,\n",
    "            title_x=0.5,\n",
    "            paper_bgcolor='#1e1e1e',\n",
    "            plot_bgcolor='#2d2d2d',\n",
    "            font=dict(color='white', size=10)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def _get_health_color(self, health):\n",
    "        \"\"\"Get color based on health value\"\"\"\n",
    "        if health > 80:\n",
    "            return '#4CAF50'\n",
    "        elif health > 50:\n",
    "            return '#FFC107'\n",
    "        elif health > 20:\n",
    "            return '#FF5722'\n",
    "        else:\n",
    "            return '#F44336'\n",
    "\n",
    "# Create dashboard\n",
    "dashboard = ChaosDashboard(chaos_system)\n",
    "\n",
    "# Interactive controls\n",
    "print(\"🎮 ULTIMATE CHAOS ENGINEERING CONSOLE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Scenario selector with cards\n",
    "scenario_selector = widgets.RadioButtons(\n",
    "    options=[(name, name) for name in chaos_system.chaos_scenarios.keys()],\n",
    "    value='🌊 API Flood',\n",
    "    layout=widgets.Layout(width='300px')\n",
    ")\n",
    "\n",
    "# Intensity slider\n",
    "intensity_slider = widgets.FloatSlider(\n",
    "    value=1.0,\n",
    "    min=0.5,\n",
    "    max=2.0,\n",
    "    step=0.1,\n",
    "    description='Intensity:',\n",
    "    style={'description_width': 'initial'},\n",
    "    readout_format='.1f'\n",
    ")\n",
    "\n",
    "# Control buttons\n",
    "start_button = widgets.Button(\n",
    "    description='🔥 UNLEASH CHAOS',\n",
    "    button_style='danger',\n",
    "    layout=widgets.Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "stop_button = widgets.Button(\n",
    "    description='⏹️ STOP',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description='🔄 REFRESH',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='100px')\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "status_output = widgets.Output()\n",
    "viz_output = widgets.Output()\n",
    "achievement_output = widgets.Output()\n",
    "\n",
    "# Handlers\n",
    "def start_chaos(b):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        scenario = scenario_selector.value\n",
    "        intensity = intensity_slider.value\n",
    "        \n",
    "        print(f\"🌪️ INITIATING CHAOS: {scenario}\")\n",
    "        print(f\"⚡ Intensity: {intensity:.1f}x\")\n",
    "        print(f\"⏱️ Duration: {chaos_system.chaos_scenarios[scenario]['duration']}s\")\n",
    "        print(f\"🏆 Potential Points: {chaos_system.chaos_scenarios[scenario]['points']}\")\n",
    "        \n",
    "        result = chaos_system.start_chaos(scenario, intensity)\n",
    "        print(f\"\\n{result}\")\n",
    "\n",
    "def stop_chaos(b):\n",
    "    chaos_system.stop_chaos()\n",
    "    with status_output:\n",
    "        print(\"\\n⏹️ Chaos terminated\")\n",
    "\n",
    "def refresh_viz(b):\n",
    "    with viz_output:\n",
    "        clear_output(wait=True)\n",
    "        fig = dashboard.create_visualization()\n",
    "        fig.show()\n",
    "    \n",
    "    # Update achievements display\n",
    "    with achievement_output:\n",
    "        clear_output()\n",
    "        print(\"🏆 ACHIEVEMENTS\")\n",
    "        print(\"-\"*30)\n",
    "        for ach in chaos_system.achievements:\n",
    "            if ach.unlocked:\n",
    "                print(f\"{ach.icon} {ach.name} ✅\")\n",
    "            else:\n",
    "                print(f\"🔒 {ach.name} - {ach.description}\")\n",
    "\n",
    "# Connect handlers\n",
    "start_button.on_click(start_chaos)\n",
    "stop_button.on_click(stop_chaos)\n",
    "refresh_button.on_click(refresh_viz)\n",
    "\n",
    "# Create scenario info display\n",
    "def update_scenario_info(change):\n",
    "    with status_output:\n",
    "        clear_output()\n",
    "        scenario = chaos_system.chaos_scenarios[scenario_selector.value]\n",
    "        print(f\"📋 SCENARIO: {scenario_selector.value}\")\n",
    "        print(f\"📝 {scenario['description']}\")\n",
    "        print(f\"⚡ Difficulty: {'⭐' * scenario['difficulty']}\")\n",
    "        print(f\"⏱️ Duration: {scenario['duration']}s\")\n",
    "        print(f\"🏆 Points: {scenario['points']}\")\n",
    "\n",
    "scenario_selector.observe(update_scenario_info, names='value')\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h2 style='color: white;'>🎮 CHAOS CONTROL CENTER</h2>\"),\n",
    "    widgets.HBox([\n",
    "        widgets.VBox([\n",
    "            widgets.HTML(\"<b style='color: white;'>Select Chaos Type:</b>\"),\n",
    "            scenario_selector,\n",
    "            intensity_slider\n",
    "        ]),\n",
    "        widgets.VBox([\n",
    "            status_output,\n",
    "            widgets.HBox([start_button, stop_button, refresh_button])\n",
    "        ])\n",
    "    ]),\n",
    "    achievement_output\n",
    "], layout=widgets.Layout(\n",
    "    padding='20px',\n",
    "    border='2px solid #333',\n",
    "    background_color='#1e1e1e'\n",
    "))\n",
    "\n",
    "display(HTML(\"<style>.widget-label { color: white !important; }</style>\"))\n",
    "display(controls)\n",
    "display(viz_output)\n",
    "\n",
    "# Initial display\n",
    "update_scenario_info(None)\n",
    "with viz_output:\n",
    "    fig = dashboard.create_visualization()\n",
    "    fig.show()\n",
    "\n",
    "# Auto-refresh\n",
    "def auto_refresh():\n",
    "    while True:\n",
    "        if chaos_system.chaos_active:\n",
    "            time.sleep(1)\n",
    "            with viz_output:\n",
    "                clear_output(wait=True)\n",
    "                fig = dashboard.create_visualization()\n",
    "                fig.show()\n",
    "        else:\n",
    "            time.sleep(2)\n",
    "\n",
    "refresh_thread = threading.Thread(target=auto_refresh, daemon=True)\n",
    "refresh_thread.start()\n",
    "\n",
    "print(\"\\n🎯 OBJECTIVES:\")\n",
    "print(\"• Maintain >80% success rate to earn achievements\")\n",
    "print(\"• Complete all scenarios to become Chaos Master\")\n",
    "print(\"• Reach Level 10 for ultimate bragging rights\")\n",
    "print(\"• Compete for high scores in each scenario\")\n",
    "print(\"\\n💡 TIPS:\")\n",
    "print(\"• Start with lower intensity to learn patterns\")\n",
    "print(\"• Watch system health and adjust strategy\")\n",
    "print(\"• Healing triggers boost your score\")\n",
    "print(\"• Each scenario has unique challenges\")\n",
    "print(\"\\n🏆 Ready to master chaos? Select a scenario and UNLEASH!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎓 **Workshop Summary & Production Checklist**\n",
    "### You've Built an Unbreakable AI System! 🏆\n",
    "\n",
    "### ✅ **What You've Mastered**\n",
    "\n",
    "1. **🛡️ Error Handling**\n",
    "   - Complete error taxonomy\n",
    "   - Smart retry strategies\n",
    "   - Context-aware recovery\n",
    "\n",
    "2. **🎭 Graceful Degradation**\n",
    "   - Multi-tier fallback systems\n",
    "   - Intelligent caching\n",
    "   - Service quality management\n",
    "\n",
    "3. **🚦 Rate Limiting**\n",
    "   - Token bucket algorithm\n",
    "   - Sliding window implementation\n",
    "   - Smart quota management\n",
    "\n",
    "4. **🔌 Circuit Breakers**\n",
    "   - State management\n",
    "   - Automatic recovery\n",
    "   - Component protection\n",
    "\n",
    "5. **🤖 Self-Healing Systems**\n",
    "   - Health monitoring\n",
    "   - Auto-diagnosis\n",
    "   - Autonomous recovery\n",
    "\n",
    "### 📋 **Production Deployment Checklist**\n",
    "\n",
    "```python\n",
    "production_ready = {\n",
    "    '✅ Error Handling': True,\n",
    "    '✅ Rate Limiting': True,\n",
    "    '✅ Circuit Breakers': True,\n",
    "    '✅ Monitoring': True,\n",
    "    '✅ Self-Healing': True,\n",
    "    '✅ Chaos Tested': True\n",
    "}\n",
    "```\n",
    "\n",
    "### 🚀 **Your Next Steps**\n",
    "\n",
    "1. **Deploy to Production**\n",
    "   - Use environment variables for API keys\n",
    "   - Set up proper logging infrastructure\n",
    "   - Configure alerts and monitoring\n",
    "\n",
    "2. **Scale Your System**\n",
    "   - Implement distributed rate limiting\n",
    "   - Add database-backed circuit breakers\n",
    "   - Create multi-region failover\n",
    "\n",
    "3. **Advanced Features**\n",
    "   - Predictive failure detection\n",
    "   - ML-based anomaly detection\n",
    "   - Automated capacity planning\n",
    "\n",
    "### 🎁 **Bonus: Your Production Toolkit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Initializing Production Resilient AI System Demo...\n",
      "✅ Production Resilient AI System initialized\n",
      "\n",
      "📊 Interactive Control Panel\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82ca566369d472b804a03747cc6a882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>🎛️ Test Controls</h3>'), Textarea(value='What is the meaning of life?', descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff66085c91947a6842e0a157c9ed828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ System Ready!\n",
      "\n",
      "📚 Features Demonstrated:\n",
      "  ✅ Multi-model fallback chain\n",
      "  ✅ Circuit breaker protection\n",
      "  ✅ Rate limiting\n",
      "  ✅ LRU caching\n",
      "  ✅ Health monitoring\n",
      "  ✅ Self-healing\n",
      "  ✅ Real-time metrics\n",
      "\n",
      "🎯 Try:\n",
      "  1. Send single requests\n",
      "  2. Run load tests\n",
      "  3. Enable failure simulation\n",
      "  4. Watch circuit breakers trip\n",
      "  5. See cache performance\n",
      "  6. Monitor health score\n"
     ]
    }
   ],
   "source": [
    "# 🎯 Cell 12: Complete Production Resilient AI System - Live Demo\n",
    "# Run this cell to see the enterprise system in action!\n",
    "\n",
    "import time\n",
    "import threading\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from collections import deque, defaultdict\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum, auto\n",
    "from typing import Dict, List, Optional, Any\n",
    "import hashlib\n",
    "import json\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "\n",
    "# ============================================================================\n",
    "# CORE SYSTEM COMPONENTS\n",
    "# ============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for AI models\"\"\"\n",
    "    name: str\n",
    "    max_tokens: int = 1000\n",
    "    rate_limit: int = 60  # requests per minute\n",
    "    cost_per_token: float = 0.00002\n",
    "    priority: int = 1\n",
    "\n",
    "@dataclass  \n",
    "class SystemConfig:\n",
    "    \"\"\"System configuration\"\"\"\n",
    "    cache_enabled: bool = True\n",
    "    cache_ttl: int = 300\n",
    "    max_cache_size: int = 100\n",
    "    circuit_breaker_threshold: int = 5\n",
    "    circuit_breaker_timeout: int = 30\n",
    "    health_check_interval: int = 5\n",
    "    auto_heal: bool = True\n",
    "\n",
    "class HealthStatus(Enum):\n",
    "    \"\"\"System health states\"\"\"\n",
    "    HEALTHY = (\"healthy\", \"🟢\", \"#4CAF50\")\n",
    "    DEGRADED = (\"degraded\", \"🟡\", \"#FFC107\")\n",
    "    CRITICAL = (\"critical\", \"🔴\", \"#F44336\")\n",
    "\n",
    "class CircuitBreakerState(Enum):\n",
    "    \"\"\"Circuit breaker states\"\"\"\n",
    "    CLOSED = \"closed\"\n",
    "    OPEN = \"open\"\n",
    "    HALF_OPEN = \"half_open\"\n",
    "\n",
    "# ============================================================================\n",
    "# CIRCUIT BREAKER\n",
    "# ============================================================================\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, threshold: int = 5, timeout: int = 30):\n",
    "        self.name = name\n",
    "        self.threshold = threshold\n",
    "        self.timeout = timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitBreakerState.CLOSED\n",
    "        self.success_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.lock = threading.RLock()\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Execute function with circuit breaker protection\"\"\"\n",
    "        with self.lock:\n",
    "            self.total_calls += 1\n",
    "            \n",
    "            # Check if circuit is open\n",
    "            if self.state == CircuitBreakerState.OPEN:\n",
    "                if self.last_failure_time and time.time() - self.last_failure_time > self.timeout:\n",
    "                    self.state = CircuitBreakerState.HALF_OPEN\n",
    "                    self.success_count = 0\n",
    "                else:\n",
    "                    raise Exception(f\"Circuit breaker {self.name} is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            # Simulate API call\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            with self.lock:\n",
    "                if self.state == CircuitBreakerState.HALF_OPEN:\n",
    "                    self.success_count += 1\n",
    "                    if self.success_count >= 3:\n",
    "                        self.state = CircuitBreakerState.CLOSED\n",
    "                        self.failure_count = 0\n",
    "                elif self.state == CircuitBreakerState.CLOSED:\n",
    "                    self.failure_count = max(0, self.failure_count - 1)\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            with self.lock:\n",
    "                self.failure_count += 1\n",
    "                self.last_failure_time = time.time()\n",
    "                \n",
    "                if self.failure_count >= self.threshold:\n",
    "                    self.state = CircuitBreakerState.OPEN\n",
    "                    self.success_count = 0\n",
    "            raise\n",
    "\n",
    "# ============================================================================\n",
    "# RATE LIMITER\n",
    "# ============================================================================\n",
    "\n",
    "class TokenBucketRateLimiter:\n",
    "    \"\"\"Token bucket rate limiter\"\"\"\n",
    "    \n",
    "    def __init__(self, rate: int):\n",
    "        self.rate = rate  # tokens per minute\n",
    "        self.capacity = rate\n",
    "        self.tokens = self.capacity\n",
    "        self.last_refill = time.time()\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def acquire(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"Try to acquire tokens\"\"\"\n",
    "        with self.lock:\n",
    "            self._refill()\n",
    "            if self.tokens >= tokens:\n",
    "                self.tokens -= tokens\n",
    "                return True\n",
    "            return False\n",
    "    \n",
    "    def _refill(self):\n",
    "        \"\"\"Refill tokens based on time elapsed\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_refill\n",
    "        new_tokens = elapsed * (self.rate / 60)\n",
    "        self.tokens = min(self.capacity, self.tokens + new_tokens)\n",
    "        self.last_refill = now\n",
    "\n",
    "# ============================================================================\n",
    "# LRU CACHE\n",
    "# ============================================================================\n",
    "\n",
    "class LRUCache:\n",
    "    \"\"\"Simple LRU cache\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int, ttl: int):\n",
    "        self.capacity = capacity\n",
    "        self.ttl = ttl\n",
    "        self.cache = {}\n",
    "        self.order = deque()\n",
    "        self.lock = threading.RLock()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"Get value from cache\"\"\"\n",
    "        with self.lock:\n",
    "            if key in self.cache:\n",
    "                value, timestamp = self.cache[key]\n",
    "                if time.time() - timestamp < self.ttl:\n",
    "                    self.order.remove(key)\n",
    "                    self.order.append(key)\n",
    "                    self.hits += 1\n",
    "                    return value\n",
    "                else:\n",
    "                    del self.cache[key]\n",
    "                    self.order.remove(key)\n",
    "            self.misses += 1\n",
    "            return None\n",
    "    \n",
    "    def put(self, key: str, value: Any):\n",
    "        \"\"\"Put value in cache\"\"\"\n",
    "        with self.lock:\n",
    "            if key in self.cache:\n",
    "                self.order.remove(key)\n",
    "            elif len(self.cache) >= self.capacity:\n",
    "                oldest = self.order.popleft()\n",
    "                del self.cache[oldest]\n",
    "            \n",
    "            self.cache[key] = (value, time.time())\n",
    "            self.order.append(key)\n",
    "\n",
    "# ============================================================================\n",
    "# PRODUCTION RESILIENT AI SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class ProductionResilientAI:\n",
    "    \"\"\"Production-ready resilient AI system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: SystemConfig = None):\n",
    "        self.config = config or SystemConfig()\n",
    "        \n",
    "        # Models\n",
    "        self.models = {\n",
    "            'primary': ModelConfig('gpt-4o', rate_limit=100),\n",
    "            'fallback': ModelConfig('gpt-4o-mini', rate_limit=200),\n",
    "            'emergency': ModelConfig('gpt-3.5-turbo', rate_limit=500)\n",
    "        }\n",
    "        \n",
    "        # Initialize components\n",
    "        self.circuit_breakers = {\n",
    "            name: CircuitBreaker(name, self.config.circuit_breaker_threshold)\n",
    "            for name in self.models.keys()\n",
    "        }\n",
    "        \n",
    "        self.rate_limiters = {\n",
    "            name: TokenBucketRateLimiter(model.rate_limit)\n",
    "            for name, model in self.models.items()\n",
    "        }\n",
    "        \n",
    "        self.cache = LRUCache(\n",
    "            self.config.max_cache_size,\n",
    "            self.config.cache_ttl\n",
    "        ) if self.config.cache_enabled else None\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0,\n",
    "            'total_latency': 0,\n",
    "            'request_history': deque(maxlen=100),\n",
    "            'error_history': deque(maxlen=50)\n",
    "        }\n",
    "        \n",
    "        # Health monitoring\n",
    "        self.health_status = HealthStatus.HEALTHY\n",
    "        self.health_score = 100\n",
    "        self.health_history = deque(maxlen=100)\n",
    "        \n",
    "        # Start monitoring\n",
    "        self.monitoring_active = True\n",
    "        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)\n",
    "        self.monitor_thread.start()\n",
    "        \n",
    "        print(\"✅ Production Resilient AI System initialized\")\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Background health monitoring\"\"\"\n",
    "        while self.monitoring_active:\n",
    "            try:\n",
    "                self._update_health()\n",
    "                if self.config.auto_heal and self.health_score < 70:\n",
    "                    self._trigger_healing()\n",
    "            except Exception as e:\n",
    "                print(f\"Monitor error: {e}\")\n",
    "            time.sleep(self.config.health_check_interval)\n",
    "    \n",
    "    def _update_health(self):\n",
    "        \"\"\"Update health score and status\"\"\"\n",
    "        # Calculate health score based on metrics\n",
    "        score = 100\n",
    "        \n",
    "        if self.metrics['total_requests'] > 0:\n",
    "            error_rate = self.metrics['failed_requests'] / self.metrics['total_requests']\n",
    "            score -= error_rate * 50\n",
    "            \n",
    "            avg_latency = self.metrics['total_latency'] / self.metrics['total_requests']\n",
    "            if avg_latency > 3:\n",
    "                score -= min((avg_latency - 3) * 10, 30)\n",
    "        \n",
    "        # Check circuit breakers\n",
    "        open_breakers = sum(1 for cb in self.circuit_breakers.values() \n",
    "                          if cb.state == CircuitBreakerState.OPEN)\n",
    "        score -= open_breakers * 20\n",
    "        \n",
    "        self.health_score = max(0, min(100, score))\n",
    "        \n",
    "        # Update status\n",
    "        if self.health_score >= 80:\n",
    "            self.health_status = HealthStatus.HEALTHY\n",
    "        elif self.health_score >= 50:\n",
    "            self.health_status = HealthStatus.DEGRADED\n",
    "        else:\n",
    "            self.health_status = HealthStatus.CRITICAL\n",
    "        \n",
    "        # Record history\n",
    "        self.health_history.append({\n",
    "            'timestamp': datetime.now(),\n",
    "            'score': self.health_score,\n",
    "            'status': self.health_status\n",
    "        })\n",
    "    \n",
    "    def _trigger_healing(self):\n",
    "        \"\"\"Trigger self-healing actions\"\"\"\n",
    "        print(f\"🏥 Triggering self-healing (health: {self.health_score:.1f})\")\n",
    "        \n",
    "        # Reset circuit breakers if needed\n",
    "        for cb in self.circuit_breakers.values():\n",
    "            if cb.state == CircuitBreakerState.OPEN and cb.failure_count < 10:\n",
    "                cb.state = CircuitBreakerState.HALF_OPEN\n",
    "                print(f\"  Reset circuit breaker: {cb.name}\")\n",
    "        \n",
    "        # Clear cache if needed\n",
    "        if self.cache and len(self.cache.cache) > self.config.max_cache_size * 0.8:\n",
    "            self.cache.cache.clear()\n",
    "            self.cache.order.clear()\n",
    "            print(\"  Cleared cache\")\n",
    "    \n",
    "    def make_request(self, message: str, simulate_failure: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Make a resilient API request\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        # Generate cache key\n",
    "        cache_key = hashlib.md5(message.encode()).hexdigest()\n",
    "        \n",
    "        # Check cache\n",
    "        if self.cache:\n",
    "            cached = self.cache.get(cache_key)\n",
    "            if cached:\n",
    "                self.metrics['cache_hits'] += 1\n",
    "                self.metrics['successful_requests'] += 1\n",
    "                return {\n",
    "                    'content': cached,\n",
    "                    'cached': True,\n",
    "                    'latency': time.time() - start_time,\n",
    "                    'model': 'cache'\n",
    "                }\n",
    "            self.metrics['cache_misses'] += 1\n",
    "        \n",
    "        # Try models in order\n",
    "        for model_name, model_config in self.models.items():\n",
    "            try:\n",
    "                # Check rate limit\n",
    "                if not self.rate_limiters[model_name].acquire():\n",
    "                    continue\n",
    "                \n",
    "                # Use circuit breaker\n",
    "                def api_call():\n",
    "                    # Simulate API call\n",
    "                    if simulate_failure and random.random() < 0.5:\n",
    "                        raise Exception(\"Simulated API failure\")\n",
    "                    \n",
    "                    # Simulate latency\n",
    "                    latency = random.uniform(0.5, 2.5)\n",
    "                    time.sleep(latency)\n",
    "                    \n",
    "                    return f\"Response from {model_name}: {message[:50]}...\"\n",
    "                \n",
    "                response = self.circuit_breakers[model_name].call(api_call)\n",
    "                \n",
    "                # Cache successful response\n",
    "                if self.cache:\n",
    "                    self.cache.put(cache_key, response)\n",
    "                \n",
    "                # Update metrics\n",
    "                latency = time.time() - start_time\n",
    "                self.metrics['successful_requests'] += 1\n",
    "                self.metrics['total_latency'] += latency\n",
    "                self.metrics['request_history'].append({\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'model': model_name,\n",
    "                    'latency': latency,\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "                return {\n",
    "                    'content': response,\n",
    "                    'cached': False,\n",
    "                    'latency': latency,\n",
    "                    'model': model_name\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.metrics['error_history'].append({\n",
    "                    'timestamp': datetime.now(),\n",
    "                    'model': model_name,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                continue\n",
    "        \n",
    "        # All models failed\n",
    "        self.metrics['failed_requests'] += 1\n",
    "        return {\n",
    "            'content': 'Service temporarily unavailable',\n",
    "            'error': True,\n",
    "            'latency': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def get_dashboard_data(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get data for dashboard\"\"\"\n",
    "        cache_hit_rate = 0\n",
    "        if self.cache and (self.metrics['cache_hits'] + self.metrics['cache_misses']) > 0:\n",
    "            cache_hit_rate = self.metrics['cache_hits'] / (\n",
    "                self.metrics['cache_hits'] + self.metrics['cache_misses']\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'health_score': self.health_score,\n",
    "            'health_status': self.health_status,\n",
    "            'metrics': self.metrics,\n",
    "            'circuit_breakers': {\n",
    "                name: {\n",
    "                    'state': cb.state.value,\n",
    "                    'failures': cb.failure_count,\n",
    "                    'total_calls': cb.total_calls\n",
    "                }\n",
    "                for name, cb in self.circuit_breakers.items()\n",
    "            },\n",
    "            'cache_hit_rate': cache_hit_rate,\n",
    "            'health_history': list(self.health_history)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# INTERACTIVE DASHBOARD\n",
    "# ============================================================================\n",
    "\n",
    "class InteractiveDashboard:\n",
    "    \"\"\"Interactive system dashboard\"\"\"\n",
    "    \n",
    "    def __init__(self, system: ProductionResilientAI):\n",
    "        self.system = system\n",
    "    \n",
    "    def create_visualization(self):\n",
    "        \"\"\"Create dashboard visualization\"\"\"\n",
    "        data = self.system.get_dashboard_data()\n",
    "        \n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=3,\n",
    "            subplot_titles=(\n",
    "                'Health Score', 'Request Success Rate', 'Circuit Breakers',\n",
    "                'Cache Performance', 'Latency Distribution', 'System Metrics'\n",
    "            ),\n",
    "            specs=[\n",
    "                [{'type': 'indicator'}, {'type': 'scatter'}, {'type': 'bar'}],\n",
    "                [{'type': 'indicator'}, {'type': 'scatter'}, {'type': 'table'}]\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Health Score Gauge\n",
    "        status = data['health_status']\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=data['health_score'],\n",
    "                title={'text': f\"{status.value[1]} System Health\"},\n",
    "                gauge={\n",
    "                    'axis': {'range': [0, 100]},\n",
    "                    'bar': {'color': status.value[2]},\n",
    "                    'steps': [\n",
    "                        {'range': [0, 50], 'color': \"#ffebee\"},\n",
    "                        {'range': [50, 80], 'color': \"#fff9c4\"},\n",
    "                        {'range': [80, 100], 'color': \"#c8e6c9\"}\n",
    "                    ]\n",
    "                }\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # Success Rate Timeline\n",
    "        if data['health_history']:\n",
    "            timestamps = [h['timestamp'] for h in data['health_history']]\n",
    "            scores = [h['score'] for h in data['health_history']]\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=timestamps,\n",
    "                    y=scores,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='green', width=2),\n",
    "                    fill='tozeroy'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "        \n",
    "        # Circuit Breakers Bar Chart\n",
    "        cb_names = list(data['circuit_breakers'].keys())\n",
    "        cb_states = [data['circuit_breakers'][n]['failures'] for n in cb_names]\n",
    "        colors = ['green' if data['circuit_breakers'][n]['state'] == 'closed' \n",
    "                 else 'red' for n in cb_names]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=cb_names,\n",
    "                y=cb_states,\n",
    "                marker_color=colors,\n",
    "                text=[data['circuit_breakers'][n]['state'] for n in cb_names],\n",
    "                textposition='auto'\n",
    "            ),\n",
    "            row=1, col=3\n",
    "        )\n",
    "        \n",
    "        # Cache Hit Rate Gauge\n",
    "        fig.add_trace(\n",
    "            go.Indicator(\n",
    "                mode=\"gauge+number\",\n",
    "                value=data['cache_hit_rate'] * 100,\n",
    "                title={'text': \"Cache Hit Rate %\"},\n",
    "                gauge={'axis': {'range': [0, 100]}}\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # Latency Distribution\n",
    "        if data['metrics']['request_history']:\n",
    "            latencies = [r['latency'] for r in data['metrics']['request_history']]\n",
    "            fig.add_trace(\n",
    "                go.Histogram(\n",
    "                    x=latencies,\n",
    "                    nbinsx=20,\n",
    "                    marker_color='blue'\n",
    "                ),\n",
    "                row=2, col=2\n",
    "            )\n",
    "        \n",
    "        # Metrics Table\n",
    "        metrics_data = [\n",
    "            ['Total Requests', data['metrics']['total_requests']],\n",
    "            ['Successful', data['metrics']['successful_requests']],\n",
    "            ['Failed', data['metrics']['failed_requests']],\n",
    "            ['Cache Hits', data['metrics']['cache_hits']],\n",
    "            ['Cache Misses', data['metrics']['cache_misses']]\n",
    "        ]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Table(\n",
    "                cells=dict(\n",
    "                    values=list(zip(*metrics_data)),\n",
    "                    fill_color='lavender',\n",
    "                    align='left'\n",
    "                )\n",
    "            ),\n",
    "            row=2, col=3\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(height=600, showlegend=False, title_text=\"Production AI System Dashboard\")\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "print(\"🚀 Initializing Production Resilient AI System Demo...\")\n",
    "system = ProductionResilientAI()\n",
    "dashboard = InteractiveDashboard(system)\n",
    "\n",
    "# ============================================================================\n",
    "# INTERACTIVE CONTROLS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n📊 Interactive Control Panel\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test controls\n",
    "test_message = widgets.Textarea(\n",
    "    value=\"What is the meaning of life?\",\n",
    "    placeholder=\"Enter test message\",\n",
    "    description='Message:',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "simulate_failure = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Simulate Failures'\n",
    ")\n",
    "\n",
    "num_requests = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=1,\n",
    "    max=50,\n",
    "    description='Requests:'\n",
    ")\n",
    "\n",
    "# Buttons\n",
    "single_test = widgets.Button(\n",
    "    description='Send Request',\n",
    "    button_style='primary',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "load_test = widgets.Button(\n",
    "    description='Run Load Test',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "refresh_dash = widgets.Button(\n",
    "    description='Refresh Dashboard',\n",
    "    button_style='info',\n",
    "    layout=widgets.Layout(width='140px')\n",
    ")\n",
    "\n",
    "reset_system = widgets.Button(\n",
    "    description='Reset System',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='120px')\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "output = widgets.Output()\n",
    "dashboard_output = widgets.Output()\n",
    "\n",
    "# Event handlers\n",
    "def send_single_request(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"📤 Sending request...\")\n",
    "        result = system.make_request(\n",
    "            test_message.value,\n",
    "            simulate_failure=simulate_failure.value\n",
    "        )\n",
    "        print(f\"✅ Response received:\")\n",
    "        print(f\"  Model: {result.get('model', 'unknown')}\")\n",
    "        print(f\"  Latency: {result.get('latency', 0):.2f}s\")\n",
    "        print(f\"  Cached: {result.get('cached', False)}\")\n",
    "        if not result.get('error'):\n",
    "            print(f\"  Content: {result['content'][:100]}...\")\n",
    "\n",
    "def run_load_test(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(f\"🔄 Running load test with {num_requests.value} requests...\")\n",
    "        \n",
    "        results = {'success': 0, 'failed': 0, 'latencies': []}\n",
    "        \n",
    "        for i in range(num_requests.value):\n",
    "            result = system.make_request(\n",
    "                f\"Test message {i}\",\n",
    "                simulate_failure=simulate_failure.value\n",
    "            )\n",
    "            \n",
    "            if result.get('error'):\n",
    "                results['failed'] += 1\n",
    "            else:\n",
    "                results['success'] += 1\n",
    "                results['latencies'].append(result['latency'])\n",
    "            \n",
    "            print(f\"  Progress: {i+1}/{num_requests.value}\", end='\\r')\n",
    "        \n",
    "        print(f\"\\n📊 Load Test Results:\")\n",
    "        print(f\"  Successful: {results['success']}\")\n",
    "        print(f\"  Failed: {results['failed']}\")\n",
    "        if results['latencies']:\n",
    "            print(f\"  Avg Latency: {np.mean(results['latencies']):.2f}s\")\n",
    "            print(f\"  P95 Latency: {np.percentile(results['latencies'], 95):.2f}s\")\n",
    "\n",
    "def refresh_dashboard(b):\n",
    "    with dashboard_output:\n",
    "        clear_output()\n",
    "        fig = dashboard.create_visualization()\n",
    "        fig.show()\n",
    "\n",
    "def reset_system_handler(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        # Reset metrics\n",
    "        system.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_requests': 0,\n",
    "            'failed_requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'cache_misses': 0,\n",
    "            'total_latency': 0,\n",
    "            'request_history': deque(maxlen=100),\n",
    "            'error_history': deque(maxlen=50)\n",
    "        }\n",
    "        # Reset circuit breakers\n",
    "        for cb in system.circuit_breakers.values():\n",
    "            cb.state = CircuitBreakerState.CLOSED\n",
    "            cb.failure_count = 0\n",
    "        print(\"♻️ System reset complete!\")\n",
    "\n",
    "# Connect handlers\n",
    "single_test.on_click(send_single_request)\n",
    "load_test.on_click(run_load_test)\n",
    "refresh_dash.on_click(refresh_dashboard)\n",
    "reset_system.on_click(reset_system_handler)\n",
    "\n",
    "# Layout\n",
    "controls = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>🎛️ Test Controls</h3>\"),\n",
    "    test_message,\n",
    "    widgets.HBox([simulate_failure, num_requests]),\n",
    "    widgets.HBox([single_test, load_test, refresh_dash, reset_system]),\n",
    "    output\n",
    "])\n",
    "\n",
    "display(controls)\n",
    "display(dashboard_output)\n",
    "\n",
    "# Initial dashboard\n",
    "with dashboard_output:\n",
    "    fig = dashboard.create_visualization()\n",
    "    fig.show()\n",
    "\n",
    "# Auto-refresh dashboard\n",
    "def auto_refresh():\n",
    "    while True:\n",
    "        time.sleep(5)\n",
    "        with dashboard_output:\n",
    "            clear_output(wait=True)\n",
    "            fig = dashboard.create_visualization()\n",
    "            fig.show()\n",
    "\n",
    "refresh_thread = threading.Thread(target=auto_refresh, daemon=True)\n",
    "refresh_thread.start()\n",
    "\n",
    "print(\"\\n✨ System Ready!\")\n",
    "print(\"\\n📚 Features Demonstrated:\")\n",
    "print(\"  ✅ Multi-model fallback chain\")\n",
    "print(\"  ✅ Circuit breaker protection\")\n",
    "print(\"  ✅ Rate limiting\")\n",
    "print(\"  ✅ LRU caching\")\n",
    "print(\"  ✅ Health monitoring\")\n",
    "print(\"  ✅ Self-healing\")\n",
    "print(\"  ✅ Real-time metrics\")\n",
    "print(\"\\n🎯 Try:\")\n",
    "print(\"  1. Send single requests\")\n",
    "print(\"  2. Run load tests\")\n",
    "print(\"  3. Enable failure simulation\")\n",
    "print(\"  4. Watch circuit breakers trip\")\n",
    "print(\"  5. See cache performance\")\n",
    "print(\"  6. Monitor health score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 **Congratulations! Workshop Complete!**\n",
    "### You've Built a Production-Grade Resilient AI System! 🏆\n",
    "\n",
    "### 📚 **What You've Accomplished**\n",
    "\n",
    "Over the past 6 hours, you've:\n",
    "\n",
    "1. **Mastered Error Handling** - Built comprehensive error recovery systems\n",
    "2. **Implemented Graceful Degradation** - Created multi-tier fallback mechanisms\n",
    "3. **Conquered Rate Limiting** - Developed intelligent quota management\n",
    "4. **Deployed Circuit Breakers** - Protected your system from cascading failures\n",
    "5. **Created Self-Healing AI** - Built autonomous recovery capabilities\n",
    "6. **Survived Chaos Engineering** - Tested your system under extreme conditions\n",
    "\n",
    "### 🚀 **Your Production Toolkit**\n",
    "\n",
    "You now have:\n",
    "- ✅ Complete resilient AI implementation\n",
    "- ✅ Production-ready error handlers\n",
    "- ✅ Real-time monitoring dashboard\n",
    "- ✅ Self-healing mechanisms\n",
    "- ✅ Chaos testing framework\n",
    "\n",
    "### 📈 **Next Steps**\n",
    "\n",
    "1. **Deploy to Production**\n",
    "   ```python\n",
    "   # Use the ProductionResilientAI class\n",
    "   ai_system = ProductionResilientAI(api_key=your_key)\n",
    "   ```\n",
    "\n",
    "2. **Customize for Your Use Case**\n",
    "   - Adjust rate limits\n",
    "   - Configure circuit breakers\n",
    "   - Add custom error handlers\n",
    "\n",
    "3. **Scale Your System**\n",
    "   - Add Redis for distributed caching\n",
    "   - Implement database persistence\n",
    "   - Deploy to Kubernetes for auto-scaling\n",
    "\n",
    "### 🎓 **Certificate of Completion**\n",
    "\n",
    "```\n",
    "╔═══════════════════════════════════════════════╗\n",
    "║                                               ║\n",
    "║          CERTIFICATE OF ACHIEVEMENT           ║\n",
    "║                                               ║\n",
    "║     AI Resilience & Self-Healing Systems     ║\n",
    "║                                               ║\n",
    "║            You have successfully:             ║\n",
    "║                                               ║\n",
    "║    ✓ Built production-grade error handling   ║\n",
    "║    ✓ Implemented graceful degradation        ║\n",
    "║    ✓ Mastered rate limiting strategies       ║\n",
    "║    ✓ Deployed circuit breaker protection     ║\n",
    "║    ✓ Created self-healing AI systems         ║\n",
    "║                                               ║\n",
    "║         Your systems are unbreakable!        ║\n",
    "║                                               ║\n",
    "╚═══════════════════════════════════════════════╝\n",
    "```\n",
    "\n",
    "### 🙏 **Thank You!**\n",
    "\n",
    "Thank you for joining this workshop! Your AI systems are now:\n",
    "- 💪 Resilient\n",
    "- 🛡️ Self-protecting\n",
    "- 🤖 Self-healing\n",
    "- 🚀 Production-ready\n",
    "\n",
    "### 📬 **Keep Learning**\n",
    "\n",
    "Continue your journey:\n",
    "- Experiment with different failure scenarios\n",
    "- Share your implementations\n",
    "- Build even more resilient systems\n",
    "\n",
    "**Remember: The best AI systems aren't those that never fail, but those that fail gracefully and recover automatically!**\n",
    "\n",
    "---\n",
    "\n",
    "🌟 **Happy Building! May your systems never go down!** 🌟"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
